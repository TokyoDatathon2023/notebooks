{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2chWXcuEuQN6"
      },
      "source": [
        "# Workshop 3: Let's get our model! / å®Ÿéš›ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚Šã¾ã—ã‚‡ã†ï¼"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ¨ ãƒãƒ¼ãƒ /Team __**\n",
        "\n",
        "`ä¸Šè¨˜ã®ä¸‹ç·šéƒ¨ã«ãƒãƒ¼ãƒ ç•ªå·ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„` \\\n",
        "`Add your team number above`"
      ],
      "metadata": {
        "id": "3iSlTBpwvWjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ‘¥ ãƒ¡ãƒ³ãƒãƒ¼/Authors**\n",
        "\n",
        "`50éŸ³é †ã§ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®æ°åã‚’å…¨ã¦è¨˜è¼‰ã—ã¦ãã ã•ã„`\\\n",
        "`Add your names here (alphabetically)`"
      ],
      "metadata": {
        "id": "P-SOUUOevhjs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOhnU575voaW"
      },
      "source": [
        "**ğŸ¯ ã“ã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã®ç›®æ¨™/Goals of this Workshop**\n",
        "1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é©å½“ã«åˆ†å‰²ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹\n",
        "2. ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ç”¨ã„ã‚‹æ€§èƒ½æŒ‡æ¨™(ãƒ¡ãƒˆãƒªã‚¯ã‚¹)ã‚’å®šç¾©ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹\n",
        "3. ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å‚™ãˆãŸæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹\n",
        "  - $SaO_2$ã®å€¤ã‚’äºˆæ¸¬ã™ã‚‹ (å›å¸°å•é¡Œ)\n",
        "  - $SaO_2$ã¨$SpO_2$ã®å·®ã‚’äºˆæ¸¬ã™ã‚‹ (å›å¸°å•é¡Œ)\n",
        "  - æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã‚’æ¤œçŸ¥ã™ã‚‹ (åˆ†é¡å•é¡Œ)\n",
        "\n",
        "  (ãƒ¢ãƒ‡ãƒ«ã¯ç·šå½¢ã§ã‚‚éç·šå½¢ã§ã‚‚ã‚ˆã„)\n",
        "\n",
        "4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–æ‰‹æ³•ã®ä¸€ã¤ã§ã‚ã‚‹ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹\n",
        "5. å›å¸°å•é¡Œã‚„åˆ†é¡å•é¡Œã«ã¤ã„ã¦æœ€ã‚‚å¯„ä¸ã™ã‚‹ç‰¹å¾´é‡ãŒãªã‚“ã§ã‚ã£ãŸã‹è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹\n",
        "\n",
        "<br/>\n",
        "\n",
        "1. Understand the best way to split the data into train and test\n",
        "2. Define the performance metrics that you are going to use in the evaluation of your model\n",
        "3. Develop a machine learning (ML) model to either:\n",
        "  - predict $SaO_2$ values -> regression\n",
        "  - predict the gap between $SaO_2$ and $SpO_2$ -> regression\n",
        "  - detect cases of Hidden Hypoxemia (HH) -> classification\n",
        "\n",
        "  The developed model can either be linear or non-linear.\n",
        "\n",
        "4. Implement grid-search to further optimize parameters.\n",
        "5. Assess what were the most relevant features for the regression/classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZTkWnmCzKvb"
      },
      "source": [
        "**âœï¸ æœ€çµ‚ç›®æ¨™ / Expected Deliverables**\n",
        " - è©•ä¾¡æŒ‡æ¨™ã‚„ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’æ­£ã—ãç†è§£ã—ãŸä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«ä½œæˆã™ã‚‹ã“ã¨\n",
        "\n",
        " - Developed models with the performance metrics and feature importance properly reported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeADt57czpeB"
      },
      "source": [
        "**â— æ³¨æ„äº‹é … / Highlighted Pitfall(s)**\n",
        "- ãƒªãƒ¼ã‚±ãƒ¼ã‚¸(Data Leakage)ã‚’èµ·ã“ã•ãªã„ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†\n",
        "- é©åˆ‡ã§ãªã„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®æŒ‡æ¨™(ãƒ¡ãƒˆãƒªã‚¯ã‚¹)ã‚’ç”¨ã„ãªã„ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†\n",
        "- ä¸ãˆã‚‰ã‚ŒãŸåŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦æ€§èƒ½ãŒå‘ä¸Šã—ãªã„ã‚±ãƒ¼ã‚¹ã‚„å­¦ç¿’ãŒé€²ã‚“ã§ã„ãªã„ã‚±ãƒ¼ã‚¹ã«æ³¨æ„ã—ã¾ã—ã‚‡ã†\n",
        "- ã‚ã¾ã‚Šã«ã‚‚è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã¯ä½œæˆã—ãªã„ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†\n",
        "\n",
        "- Outcome leakage\n",
        "- Suboptimal metrics for model evaluation\n",
        "- No improvement compared to the presented baseline / Models not learning\n",
        "- Overly complex models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. æº–å‚™ / Setup Environment"
      ],
      "metadata": {
        "id": "CUby9hdLkhY8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8of3shDt1WW-"
      },
      "source": [
        "ä½œæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€ã‚ãªãŸãŒã‚ˆã‚Šæ…£ã‚Œã¦ã„ã‚‹ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ ã—ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚\n",
        "\n",
        "You can add more libraries if you are familiar with them for your own model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKnijQhgy-os"
      },
      "outputs": [],
      "source": [
        "# If your datasets exist in Google Drive, you need to mount Google Drive to access files\n",
        "# If you upload your datasets directly to Google Colab, you don't need it.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "fLfLhYkUlLzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbnBPPKh7mnL"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXG-Ekog8m46"
      },
      "outputs": [],
      "source": [
        "!pip install yellowbrick"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for Japanese characters in matplotlib\n",
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "id": "SNPzaPuxylXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "aDGC2hIMlN9P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvY3nMXkEAmc"
      },
      "outputs": [],
      "source": [
        "# Data reading in Dataframe format and data preprocessing\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_columns\", 160)\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Dataset Creation\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Dataset Processing\n",
        "from sklearn import datasets, linear_model, metrics\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "\n",
        "# Model Development\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from yellowbrick.classifier import ClassificationReport, ClassPredictionError\n",
        "from yellowbrick.regressor import ResidualsPlot, PredictionError\n",
        "\n",
        "# Feature Importance\n",
        "import shap\n",
        "\n",
        "# Japanize Matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htgztOyb2Kt9"
      },
      "source": [
        "## 2. å‰å‡¦ç† / Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFlCvmfM4PKs"
      },
      "source": [
        "###### âœï¸ work_dirã‚’æ­£ã—ãä¿®æ­£ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã£ã¦ãã¾ã—ã‚‡ã†\n",
        "âœï¸ Set your path to the datasets, and fetch them"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set pathes\n",
        "train_csv = \"path_to_your_train.csv\"\n",
        "test_csv = \"path_to_your_test.csv\"\n",
        "\n",
        "# fetch dataset\n",
        "df_train = pd.read_csv(train_csv)\n",
        "df_test = pd.read_csv(test_csv)"
      ],
      "metadata": {
        "id": "AmDOk43T7wqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check dataset (Do not peep test data)\n",
        "df_train.head(2)"
      ],
      "metadata": {
        "id": "FBcj2VbAgPa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Variable Dictionary (GitHub Link)](https://github.com/TokyoDatathon2023/notebooks/blob/main/variable_dictionary.md): ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å¤‰æ•°ã®ä¸€è¦§ã¨ç°¡æ˜“çš„ãªèª¬æ˜"
      ],
      "metadata": {
        "id": "SafoHqgvsEln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ / Encoding\n",
        "\n",
        "Backgroundã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯ç§ãŸã¡ã®æŒã£ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ãã“ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚ã“ã®å‡¦ç†ã¯æ•°å€¤çš„ãªå€¤ã«ã—ã‹é©ç”¨ã§ãã¾ã›ã‚“ã€‚ã¤ã¾ã‚Šã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¯æ•°å€¤çš„ãªå€¤ã«å¤‰æ›ã—ãªãã¦ã¯ãªã‚‰ãªã„ã®ã§ã™ã€‚\n",
        "\n",
        "ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«ã¯æ¬¡ã®ä¸‰ç¨®é¡ãŒå­˜åœ¨ã—ã¾ã™ã€‚\n",
        "- ãƒã‚¤ãƒŠãƒªå¤‰æ•° (Binary Variables):\n",
        "> ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã¯1ã¨0ã®äºŒã¤ã®å€¤ã‚’è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ä¾‹ãˆã°ã€å¯¾è±¡ãŒç‰¹å®šã®ã‚°ãƒ«ãƒ¼ãƒ—ã«å¯¾ã—ã¦æ‰€å±(1)ã€éæ‰€å±(0)ã¨ã„ã£ãŸã“ã¨ã‚’è¡¨ç¾ã™ã‚‹ã®ã«ç”¨ã„ã‚‰ã‚Œã‚‹ã€‚\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "- é †åºå¤‰æ•° (Ordinal Variables):\n",
        "> é †åºå¤‰æ•°ã¯ã‚ã‚‹ç‰¹å®šã®é †ç•ªã‚’æŒã£ã¦ãŠã‚Šã€ãƒ©ãƒ™ãƒ«ã«ã‚ˆã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰(Label Encoding)ã‚’ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦æ•°å€¤çš„ãªå€¤ã«è¡¨ç¾ã—ãªãŠã™ã“ã¨ãŒã§ãã‚‹ã€‚ä¾‹ãˆã°ã€High, Medium, Lowã¨ã„ã£ãŸä¸‰ã¤ã®ãƒ‡ãƒ¼ã‚¿ã¯é †åºå¤‰æ•°ã§ã‚ã‚‹ã€‚ãªãœãªã‚‰ã€ã“ã‚Œã‚‰ã¯æ±ºã¾ã£ãŸé †åºã‚’æŒã¤ã®ã§ã€ãã‚Œãã‚Œ3, 2, 1ã¨ã„ã†æ•°å€¤çš„ãªå€¤ã«è¡¨ç¾ã—ãªãŠã™ã“ã¨ãŒã§ãã‚‹ã‹ã‚‰ã ã€‚\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "- åç¾©å¤‰æ•° (Nominal Variables):\n",
        "> åç¾©å¤‰æ•°ã¯æ±ºã¾ã£ãŸé †åºã‚’æŒãŸãªã„ã€‚å¾“ã£ã¦ã“ã®ç¨®ã®å¤‰æ•°ã«ã¯ãƒ©ãƒ™ãƒ«ã«ã‚ˆã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒé©ã—ã¦ã„ãªã„ã€‚ä»£ã‚ã‚Šã«ã€ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’è¡Œã†ã®ãŒè‰¯ã„ã€‚\n",
        "\n",
        "<br />\n",
        "\n",
        "Since the machine learning model in the background models and finds patterns in our data. It only supports numeric values. For this reason, categorical variables must be coded to numeric values.\n",
        "\n",
        "Categorical variables can be of 3 types:\n",
        "- Binary variables: Binary variables can be represented with two values, 1 and 0. Examples are whether or not the variable belongs to a group.\n",
        "- Ordinal variables: Ordinal variables are a type of variables that have a specific order and can be represented with numeric variables through a label encoder. An example is High, Medium, and Low which can be represented as 3, 2, 1.\n",
        "- Nominal variables: Nominal variables are categorical variables that do not have a defined order, for these variables it is not recommended to use a label encoder, it is better to use one hot encoder in these cases."
      ],
      "metadata": {
        "id": "O9AcEkVP9HNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ ã©ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æŒã£ã¦ã„ã‚‹ã§ã—ã‚‡ã†ã‹ï¼Ÿ / Which type of categorical variables we have?\n",
        "\n",
        "ãªãŠã€å¿…ãšã—ã‚‚å…¨ã¦ã®ç¨®é¡ã«ç›¸å½“ã™ã‚‹å¤‰æ•°ãŒãªãã¦ã¯ãªã‚‰ãªã„ã¨ã„ã†ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\\\n",
        "Not necessarily must have variables of each type.\n",
        "\n",
        "> **ãƒ’ãƒ³ãƒˆ / Hint**: \\\n",
        "> æ¬¡ã®`check_variable_kind`ã‚’ä½¿ã£ã¦èª¿ã¹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\\\n",
        "> Use `check_variable_kind` to check the kinds of variables."
      ],
      "metadata": {
        "id": "668OwYL8Cai3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ãƒã‚¤ãƒŠãƒªå¤‰æ•° / The binary variables are: \\\n",
        "(ã“ã“ã«å½“ã¦ã¯ã¾ã‚‹å¤‰æ•°ã‚’æ›¸ã„ã¦ãã ã•ã„ / here)\n",
        "\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "* é †åºå¤‰æ•° / The ordinal variables are: \\\n",
        "(ã“ã“ã«å½“ã¦ã¯ã¾ã‚‹å¤‰æ•°ã‚’æ›¸ã„ã¦ãã ã•ã„ / here)\n",
        "\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "* åç¾©å¤‰æ•° / The Norminal variables are: \\\n",
        "(ã“ã“ã«å½“ã¦ã¯ã¾ã‚‹å¤‰æ•°ã‚’æ›¸ã„ã¦ãã ã•ã„ / here)"
      ],
      "metadata": {
        "id": "fRcQKtFea48C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_variable_kind(df):\n",
        "    # check per column\n",
        "    for col in df.columns:\n",
        "        # show the name and the num of unique variables\n",
        "        print(f\"Column: {col} | {df[col].unique().size} unique values\")\n",
        "\n",
        "        # get unique values\n",
        "        unique_sample = df[col].unique().tolist()\n",
        "        print(\"[  \", end=\"\")\n",
        "        if len(unique_sample) < 10:\n",
        "            # when the num of kinds is less 10\n",
        "            for u in unique_sample: print(f\"{u}  \", end=\"\")\n",
        "        else:\n",
        "            # otherwise\n",
        "            for i in range(5): print(f\"{unique_sample[i]}  \", end=\"\")\n",
        "            print(\"\\n\\t... \", end=\"\")\n",
        "            for i in range(5): print(f\"{unique_sample[-(5-i)]}  \", end=\"\")\n",
        "        print(\"]\\n\")"
      ],
      "metadata": {
        "id": "6GUf6AtAE8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the kinds of variables\n",
        "check_variable_kind(df_train)"
      ],
      "metadata": {
        "id": "WqLaYHqxefxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ / Encode binary variables\n",
        "\n",
        "æ©Ÿæ¢°å­¦ç¿’ã®ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹ã®ã¯æ•°å­—ã ã‘ã§ã™ã€‚å¾“ã£ã¦ã€ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã¯0, 1ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†ã€‚\\\n",
        "è©²å½“ã™ã‚‹å¤‰æ•°åˆ—ã®å€¤ã‚’0, 1ã§ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚\n",
        "\n",
        "Machine learning models only understand numbers, so binary variables must be encoded as 1s and 0s. Replace the values in those columns with 1s and 0s\n",
        "\n",
        ">**ãƒ’ãƒ³ãƒˆ / Hint**: \\\n",
        "> `df.replace()`ã‚’ä½¿ã†\n",
        ">\n",
        "> you can use `df.replace()` to do that"
      ],
      "metadata": {
        "id": "VWmNY9SJ86Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when replacing A to B\n",
        "# df.replace(A, B)"
      ],
      "metadata": {
        "id": "GWGkwG60gnr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ é †åºå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ /  Encode ordinal variables\n",
        "\n",
        "`label_encoder ` ã‚’ä½¿ã£ã¦é †åºå¤‰æ•°ã‚’æ•°å€¤ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "You can use the function `label_encoder ` to encode the ordinal variables as numeric variables.\n",
        "\n",
        "> **Hint**:\n",
        "\n",
        "> **ãƒ’ãƒ³ãƒˆ / Hint**: \\\n",
        "> å¤‰æ•°ã®ä¸­ã«ã¯ä¸€è¦‹ã™ã‚‹ã¨æ•°å€¤ã¸ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒæ—¢ã«çµ‚ã‚ã£ã¦ã„ã‚‹ã‚ˆã†ã«æ€ãˆã‚‹ã‚‚ã®ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã—ã‹ã—ãã†ã„ã£ãŸå¤‰æ•°ã®ä¸­ã«ã¯æ¬ æå€¤ã®è£œå®Œã¨ã—ã¦`None`ãŒç´›ã‚Œè¾¼ã‚“ã§ã„ã‚‹æã‚ŒãŒã‚ã‚Šã¾ã™ã€‚ã“ã®`None`ã‚’æ•°å€¤ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯`df.replace`ã‚’ç”¨ã„ã¾ã—ã‚‡ã†ã€‚\n",
        ">\n",
        ">Some variables are already numerical, but have 'None' due to the imputation. You can use `df.replace` to convert that 'None' to numerical"
      ],
      "metadata": {
        "id": "L_VJsNIX-D1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(df_train, cols, df_test=None):\n",
        "  \"\"\"\n",
        "  Scikit-learn ã® LabelEncoder ã‚’ç”¨ã„ã¦ Pandas ã® DataFrame ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°(é †åºå¤‰æ•°)ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°.\n",
        "\n",
        "  å¼•æ•°:\n",
        "  df_train - Pandas DataFrame. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹DataFrame.\n",
        "  cols - list. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆ.\n",
        "  df_test (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«) - Pandas DataFrame. åŒæ™‚ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹DataFrame.\n",
        "\n",
        "  è¿”ã‚Šå€¤:\n",
        "  ä»¥ä¸‹ã‚’å«ã‚€ã‚¿ãƒ—ãƒ«.\n",
        "  - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ df_train\n",
        "  - ä½¿ç”¨ã—ãŸ LabelEncoders ã®è¾æ›¸\n",
        "  - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ df_test (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)\n",
        "  \"\"\"\n",
        "  encoders = {}\n",
        "  for col in cols:\n",
        "    encoders[col] = LabelEncoder()\n",
        "    df_train[col] = encoders[col].fit_transform(df_train[col])\n",
        "    if df_test is not None:\n",
        "      df_test[col] = encoders[col].transform(df_test[col])\n",
        "\n",
        "  if df_test is not None:\n",
        "    return df_train, df_test, encoders\n",
        "  else:\n",
        "    return df_train, encoders"
      ],
      "metadata": {
        "id": "ZnENxfbLXGST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This is not the case, but just to show how to use the function:\n",
        "# train, test, encoders = label_encoder(train, ['insurance'], df_test=test)"
      ],
      "metadata": {
        "id": "Fa28bOmRZycF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ åç¾©å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ / Encode nominal variables\n",
        "\n",
        "ä»¥ä¸‹ã®`column_to_one_hot`ã‚’ç”¨ã„ã¦åç¾©å¤‰æ•°ã‚’[ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ(one-hot)è¡¨è¨˜](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "You can use the function `column_to_one_hot` to encode the nominal variables to a [one-hot representation](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)."
      ],
      "metadata": {
        "id": "aTghaURaB7yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def column_to_one_hot(df_train, column, df_test=None):\n",
        "  # Generate a one-hot representation of the values in the column\n",
        "  train_one_hot = pd.get_dummies(df_train[column], dtype=int, drop_first=True)\n",
        "  # add the one-hot encoded columns to the DataFrame\n",
        "  df_train = pd.concat([df_train, train_one_hot], axis=1)\n",
        "  # drop the original column\n",
        "  df_train = df_train.drop(column, axis=1)\n",
        "\n",
        "  if df_test is not None:\n",
        "    test_one_hot = pd.get_dummies(df_test[column], dtype=int, drop_first=True)\n",
        "\n",
        "    # Add missing columns in test data\n",
        "    missing_cols = set(train_one_hot.columns) - set(test_one_hot.columns)\n",
        "    for c in missing_cols:\n",
        "      test_one_hot[c] = 0\n",
        "\n",
        "    # Ensure the order of column in the test set is in the same order than in train set\n",
        "    test_one_hot = test_one_hot[train_one_hot.columns]\n",
        "    df_test = pd.concat([df_test, test_one_hot], axis=1)\n",
        "    df_test = df_test.drop(column, axis=1)\n",
        "    return df_train, df_test\n",
        "  else:\n",
        "    return df_train"
      ],
      "metadata": {
        "id": "61UE_CRi-cfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E.g. Convert 'insurance' column into one-hot encoding:\n",
        "# train, test = column_to_one_hot(df_train, 'insurance', df_test)"
      ],
      "metadata": {
        "id": "j-Kl9LE_95Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ– / Data Normalization\n",
        "\n",
        "å¤šãã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ããå½±éŸ¿ã—ã¦ãã¾ã™ã€‚ \\\n",
        "ãã®ãŸã‚ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ç”¨ã„ã‚‹å€¤ã®å¤§ãã•ã‚’æ­£è¦åŒ–ã™ã‚‹ã“ã¨ãŒå¤§åˆ‡ã§ã™ã€‚\n",
        "\n",
        "æœ€ã‚‚ä¸€èˆ¬çš„ãªæ‰‹æ³•ã®ä¸€ã¤ã¯ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ã§ã™ãŒã€å¯¾è±¡ã¨ãªã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã¯ä¸é©å½“ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€è€ƒæ…®ã—ã¦æ‰‹æ³•ã‚’æ±ºå®šã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã™ã€‚<[è©³ç´°ã¯ã“ã¡ã‚‰](https://scikit-learn.org/stable/modules/preprocessing.html)>\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯å¹¾ã¤ã‹ã®ã‚¿ã‚¤ãƒ—ã®å¤‰æ•°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚Šã€ãã‚Œã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
        "\n",
        "> * ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•° (ä¾‹: [èŠ±ã®ç¨®é¡] ã²ã¾ã‚ã‚Š, ãŸã‚“ã½ã½, ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—) \\\n",
        "> * é€£ç¶šå‹ã®å¤‰æ•° (ä¾‹: [èº«é•·] 185cm, 160cm, 172cm) \\\n",
        "> * ãƒã‚¤ãƒŠãƒªå¤‰æ•° (ä¾‹: 0, 1)\n",
        "\n",
        "ä¾‹ãˆã°ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¨ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã«ã¤ã„ã¦ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ã‚’è¡Œã†ã“ã¨ã¯å¾€ã€…ã«ã—ã¦å¾—ç­–ã§ã¯ãªã„ã§ã—ã‚‡ã†ã€‚\\\n",
        "ã“ã®ã‚ˆã†ã«ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®å¤‰æ•°ã‚’åŒºåˆ¥ã—ã€ãã‚Œãã‚Œã®å¤‰æ•°ã«ã¤ã„ã¦é©å½“ãªæ­£è¦åŒ–æ–¹æ³•ã‚’é©ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "Many machine learning models are directly influenced by the scale of the features that you input to the model. Therefore, it is important to normalize the scale of values used in your pipeline.\n",
        "\n",
        "One of the most common methods used is data standardization, but the decision should be taken considering the specific use case or model that you are developing. If you are curious, you can further read about this topic [here](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "\n",
        "It is important to consider that in a dataset one might have several types of variables: categorical, continuous, binary. For both categorical and binary variables, it is not a good practice to apply standardization methods. Therefore, it is important to distinguish the different type of variables that one might have and apply normalization methods only to those that make sense."
      ],
      "metadata": {
        "id": "5BD44-g5zsmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœï¸ æ­£è¦åŒ–ã‚’è¡Œã†å‰ã«ã€äºˆæ¸¬ã™ã‚‹é …ç›®ã‚’ X_train ã¨ X_test ã‹ã‚‰å–ã‚Šé™¤ã y_train ã‚„ y_test ã«æ ¼ç´ã—ã¾ã—ã‚‡ã†<br>\n",
        "Split your labels from the remaining dataset"
      ],
      "metadata": {
        "id": "BQpaXABFNtsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels we are going to predict\n",
        "label_cols=['hidden_hypoxemia', 'SaO2']\n",
        "id_col = [\"subject_id\", \"stay_id\"]\n",
        "\n",
        "# train dataset\n",
        "y_train = df_train[label_cols]\n",
        "X_train = df_train.drop(columns=label_cols + id_col)\n",
        "\n",
        "# test dataset\n",
        "y_test = df_test[label_cols]\n",
        "X_test = df_test.drop(columns=label_cols + id_col)"
      ],
      "metadata": {
        "id": "grZV8zKxNyDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check train data\n",
        "y_train.head(2)"
      ],
      "metadata": {
        "id": "x-k1z_YvN24J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(2)"
      ],
      "metadata": {
        "id": "NGrhXzCIN4DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch X_train for binary variables\n",
        "binary_variables = [feat for feat in X_train.columns if X_train[feat].unique().size == 2]"
      ],
      "metadata": {
        "id": "S2UttMiAjWU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check fetched binary variables\n",
        "binary_variables"
      ],
      "metadata": {
        "id": "mtzfguf5N-b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split variables into continuous ones and binary or categorical ones\n",
        "X_train_continuous = X_train.drop(columns=binary_variables)\n",
        "X_test_continuous = X_test.drop(columns=binary_variables)\n",
        "X_train_binary = X_train[binary_variables]\n",
        "X_test_binary = X_test[binary_variables]"
      ],
      "metadata": {
        "id": "4DY6JMqROAjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit and transform the train continuous variables\n",
        "X_train_preprocessed = scaler.fit_transform(X_train_continuous)\n",
        "\n",
        "# check preprocessed train continuous variables (Do not peep test ones)\n",
        "pd.DataFrame(X_train_preprocessed, columns=X_train_continuous.columns)"
      ],
      "metadata": {
        "id": "DZWtnNbaOBLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessor to train continuous variables (Do not peep the preprocessed ones)\n",
        "X_test_preprocessed = scaler.transform(X_test_continuous)"
      ],
      "metadata": {
        "id": "JzmVSRcjODBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å‰å‡¦ç†(æ¨™æº–åŒ–)ã‚’è¡Œã£ãŸé€£ç¶šå‹ã®å¤‰æ•°ã¨ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã‚’çµåˆã—ã€ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã¾ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "Join the processed continuous variables with the binary ones to then feed to the model."
      ],
      "metadata": {
        "id": "k5xoYEfzOI0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join\n",
        "X_train_processed = pd.concat([df_train[\"subject_id\"], X_train_binary, pd.DataFrame(X_train_preprocessed, columns=X_train_continuous.columns)], axis=1, join='inner')\n",
        "X_test_processed = pd.concat([df_test[\"subject_id\"], X_test_binary, pd.DataFrame(X_test_preprocessed, columns=X_test_continuous.columns)], axis=1, join='inner')"
      ],
      "metadata": {
        "id": "hEEd9ef5OH50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0puz3tPOPBe"
      },
      "source": [
        "æ³¨: ãƒ‡ãƒ¼ã‚¿ã«å‡¦ç†ã‚’åŠ ãˆãŸå ´åˆã€å‡¦ç†å¾Œã®åˆ†æ•£ã‚’ã—ã£ã‹ã‚Šã¨èª¿ã¹ã€é©ç”¨ã—ãŸå‰å‡¦ç†ãŒæ„å‘³ã‚’æŒã¡ã€æœŸå¾…ã—ãŸåŠ¹æœã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "Note: After transforming your data, it is important to further explore it's distribution and ensure that the transformations applied make sense and resulted on what was expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz4QaWjiOPBo"
      },
      "source": [
        "###### âœï¸ ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–æ–¹é‡ã‚’è‡ªèº«ã§è€ƒãˆã€å®Ÿè£…ã—ã¦ã¿ã‚ˆã† /  Implement your own data normalization strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoY3d0rNOPBo"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "\n",
        "\n",
        "# Check your normalization, e.g. summary statistics, histograms, scatter plots, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£… / Implement ML Pipeline"
      ],
      "metadata": {
        "id": "iNU51V7mlX9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥é™ã®å‡¦ç†ã§ã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¾ã§ä¸€è²«ã—ã¦å®Ÿè¡Œå¯èƒ½ãªå®Ÿè£…ã‚’è©¦ã¿ã¾ã—ã‚‡ã†ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å†åº¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "In subsequent processes, try to use a pipeline to implement a consistent implementation that can run from data preprocessing to model building. Be sure to reload the training and test data."
      ],
      "metadata": {
        "id": "CsXIo8hYtJOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset again to avoid duplicative encoding\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "EUruuUe0VgRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHghxbBL45fy"
      },
      "source": [
        "### å›å¸°(regression)ã¨åˆ†é¡(classification)ã«å¯¾å¿œã—ãŸãƒ©ãƒ™ãƒ«åˆ†ã‘ / Get label vectors for both possibilities: regression and classification\n",
        "\n",
        "å›å¸°ã‚’è¡Œã„ãŸã„ã®ã‹ã€åˆ†é¡ã‚’è¡Œã„ãŸã„ã®ã‹ã«ã‚ˆã£ã¦ã€å­¦ç¿’ã®éš›ã«ä½¿ç”¨ã™ã‚‹æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ç•°ãªã‚Šã¾ã™ã€‚\n",
        "å›å¸°ã®å ´åˆã¯é€£ç¶šå¤‰æ•°ã§ã‚ã‚‹$SaO_{2}$ã‚’ã€åˆ†é¡ã®å ´åˆã¯æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã®ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "If you want to use regression or classification, the labels used to train the model will be different. In the case of regression, you will have a numerical variables with SaO2 values, whereas in classification you need to define classes for hidden hypoxemia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUbgwdW11oXo"
      },
      "outputs": [],
      "source": [
        "# for regression\n",
        "y_train_r = df_train[['SaO2']].values\n",
        "y_test_r = df_test[['SaO2']].values\n",
        "\n",
        "# check y_train_r (Do not peep y_test_r)\n",
        "print(f\"Size: {len(y_train_r)} * {len(y_train_r[0])}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i}: {y_train_r[i]}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for classification\n",
        "y_train_c = df_train[['hidden_hypoxemia']].values\n",
        "y_test_c = df_test[['hidden_hypoxemia']].values\n",
        "\n",
        "# check y_train_c (Do not peep y_test_c)\n",
        "print(f\"Size: {len(y_train_c)} * {len(y_train_c[0])}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i}: {y_train_c[i]}\")\n",
        "print(\"...\")"
      ],
      "metadata": {
        "id": "qaE4PaOuix6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjypUQTM6Mth"
      },
      "source": [
        "###### âœï¸ å®Ÿè£…ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã«æ²¿ã£ã¦ã€ä»–ã«å¿…è¦ãªãƒ©ãƒ™ãƒ«ã‚’å®šç¾©ã™ã‚‹ / Define label vectors for chosen ML approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ5J-VKFJc9a"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p023Q0o56b6R"
      },
      "source": [
        "### 3.1 ãƒŠã‚¤ãƒ¼ãƒ–ãªæ‰‹æ³•ã§äºˆæ¸¬ã‚’è¡Œã†ãƒ¢ãƒ‡ãƒ«å®Ÿè£… / Naive Model Implementation\n",
        "\n",
        "äºŒã¤ã®ã‚¯ãƒ©ã‚¹(æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã§ã‚ã‚‹ã‹å¦ã‹)ã«ã¤ã„ã¦ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã§å¤šã„æ–¹ã‚’ãã®ã¾ã¾äºˆæ¸¬çµæœã¨ã—ã¦å­¦ç¿’ã•ã›ãŸå ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯ã©ã†ãªã‚‹ã§ã—ã‚‡ã†ã‹ã€‚\n",
        "\n",
        "ã“ã®å ´åˆã€æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã®æ–¹ãŒå¤šã‘ã‚Œã°äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®å€¤ã¯å…¨ã¦0ã§ã‚ã‚Šã€æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã§ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æ–¹ãŒå¤šã‘ã‚Œã°äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®å€¤ã¯å…¨ã¦1ã¨ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã“ã®ã‚ˆã†ãªã€å…¨ã¦ã®æ‚£è€…ã«ã¤ã„ã¦æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã ã¨äºˆæƒ³ã™ã‚‹ãƒ¢ãƒ‡ãƒ«(ä»¥ä¸‹ ãƒŠã‚¤ãƒ¼ãƒ–ãƒ¢ãƒ‡ãƒ«)ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ä½œæˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "<br/>\n",
        "What would our accuracy be if we predicted the most likely class? In this case, our prediction would simply be 1 or 0 for every patient. Using our training dataset we can create a naive model that predicts the most likely class for every patient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BN5M8MIcLJj"
      },
      "source": [
        "###### âœï¸ ãƒŠã‚¤ãƒ¼ãƒ–ãªæ‰‹æ³•ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã€ãã®çµæœã‚’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã‚ˆã† / Test a naive approach to have a performance baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«0ã¨1ã®æ•°ã‚’æ¯”è¼ƒã—ã¦ã€ã©ã¡ã‚‰ãŒå¤šã„ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
        "if np.sum(y_train_c==0) > np.sum(y_train_c==1):\n",
        "    # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«0ãŒå¤šã„å ´åˆã€å¸¸ã«ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«0ã‚’äºˆæ¸¬ã™ã‚‹\n",
        "    y_preds_cc = [0 for _ in range(len(y_test_c))]\n",
        "else:\n",
        "    # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«1ãŒå¤šã„å ´åˆã€å¸¸ã«ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«1ã‚’äºˆæ¸¬ã™ã‚‹\n",
        "    y_preds_cc = [1 for _ in range(len(y_test_c))]"
      ],
      "metadata": {
        "id": "Dya-POISmqLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52rodxtie_0q"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "# Check accuracy score\n",
        "accuracy_score = np.mean(y_preds_cc == y_test_c)\n",
        "print(\"Accuracy: {:.6f}\".format(accuracy_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBtxDigXKf5o"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "# Check F1 score\n",
        "f1 = f1_score(y_test_c, y_preds_cc)\n",
        "print(\"F1 Score for Naive Model: {:.6f}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã¿ã‚ˆã†\n",
        "\n",
        "ã“ã“ã§ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸç‰¹å¾´é‡ã‹ã‚‰$SaO_{2}$ã‚’äºˆæ¸¬ã™ã‚‹å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®$SaO_{2}$ã‚’äºˆæ¸¬ã—ã¦ã¿ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "mJXn-JRyMvbb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMXUBwfmO7Fb"
      },
      "source": [
        "#### 3.2.1 ç·šå½¢å›å¸°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ / Linear Regression Baseline\n",
        "\n",
        "æœ€ã‚‚ç°¡å˜ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¸€ã¤ãŒ**ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«**ã§ã™ã€‚ \\\n",
        "ã“ã‚Œã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å¤‰æ•°ãŒç‰¹å¾´é‡ã®ç·šå½¢å’Œã§è¡¨ã•ã‚Œã‚‹ã“ã¨ã‚’å‰æã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
        "\n",
        "ã“ã®æ‰‹æ³•ã¯ã€å¤šãã®å•é¡Œã«ãŠã„ã¦ã‹ãªã‚Šå¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ã™ã€‚ \\\n",
        "æ‰‹æ³•ãŒå˜ç´”ã§ã‚ã‚‹ãŠã‹ã’ã§ã€ç‰¹å¾´æ•°ã®æ•°ãŒå¤šã„å ´åˆã§ã‚‚ã€éå¸¸ã«æ—©ããƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "ä¸€èˆ¬çš„ã«ã€ç‰¹å¾´é‡ã€å‰å‡¦ç†ã€ãã—ã¦æ¨å®šå€¤ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’ã‚·ãƒ³ã‚°ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¾ã¨ã‚ã¦è¡Œã£ãŸæ–¹ãŒã€ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã€ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã€äºˆæ¸¬ãŒã‚ˆã‚Šå®¹æ˜“ã¨ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã•ã‚‰ã«ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ fit é–¢æ•°ã‚’å‘¼ã³å‡ºã™ã“ã¨ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã“ã§ã¯ã€ pipeline ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã‹ã‚‰ä¸€è²«ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¾ã§ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "On of the easiest Machine Learning model is **linear regression**. It assumes that the target variable can be written as a linear combination of the features.\n",
        "\n",
        "In many problems, this solution can be a very strong baseline. Thanks to its simplicity, it can be fitted very quickly even when the number of features is high.\n",
        "\n",
        "In general, wrapping together the features, preprocessing and fitting the estimator in a single pipeline makes it easier to transform, fit and predict the data.\n",
        "\n",
        "We train the model by calling the fit function on the train data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "# å‰å‡¦ç†ã®ãŸã‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹\n",
        "binary_vars =\n",
        "categorical_vars =\n",
        "ordinal_vars =\n",
        "continuous_vars =\n",
        "\n",
        "X_train = df_train.drop(columns=['SaO2', 'subject_id', 'stay_id', 'hidden_hypoxemia', ...])\n",
        "X_test = df_test.drop(columns=['SaO2', 'subject_id', 'stay_id', 'hidden_hypoxemia', ...])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('bin', 'passthrough', binary_vars),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_vars),\n",
        "        ('ord', OrdinalEncoder(), ordinal_vars),\n",
        "        ('num', StandardScaler(), continuous_vars)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# å‰å‡¦ç†ã¨ç·šå½¢å›å¸°ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "linear_pipeline_regression = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
        "linear_pipeline_regression.fit(X_train, y_train_r.ravel())"
      ],
      "metadata": {
        "id": "0Ob2deLVO_9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ / Interpretation:"
      ],
      "metadata": {
        "id": "5SSBewZ3PKcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å¾´é‡ä¸€ã¤ã«ã¤ãä¸€ã¤ã®é‡ã¿ã‚’ä¸ãˆã‚‹ãŸã‚ã€ä¿‚æ•°ã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãŒç§ãŸã¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å…±å¤‰é‡ã‚’ã©ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã®ã‹ã‚’ç°¡å˜ã«èª¿ã¹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "Because the linear model gives one weight to each feature, we can easily explore how it modeled our target covariate by visualizing the coefficients."
      ],
      "metadata": {
        "id": "8U6v5ht-PKcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ç·šå½¢(å›å¸°)ãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•° / Examine regression coefficients:')\n",
        "feature_names = linear_pipeline_regression.named_steps['preprocessor'].get_feature_names_out()\n",
        "linear_coefficients = pd.DataFrame(\n",
        "    linear_pipeline_regression.named_steps['regressor'].coef_,\n",
        "    index=feature_names,\n",
        "    columns=['coefficient']\n",
        ")\n",
        "\n",
        "# ä¿‚æ•°ã®çµ¶å¯¾å€¤ã«åŸºã¥ã„ã¦ä¸¦ã³æ›¿ãˆ\n",
        "linear_coefficients = linear_coefficients.reindex(linear_coefficients.coefficient.abs().sort_values(ascending=False).index).T\n",
        "\n",
        "linear_coefficients\n"
      ],
      "metadata": {
        "id": "YH6lzZiqPNLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "linear_coefficients.plot(kind='bar', ax=ax)\n",
        "\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Coefficient')\n",
        "plt.title('Linear Regression Coefficients')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3qFogmRQURUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœï¸ ç¢ºèªã—ãŸä¿‚æ•°ã®ä¸­ã§ä¸æ€è­°ã«æ„Ÿã˜ãŸã‚‚ã®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã‹ï¼Ÿ / Don't you see anything strange with some of these coefficients ?\n",
        "\n",
        "<br/>\n",
        "\n",
        "ä¾‹ãˆã°ã€ã»ã¨ã‚“ã©0ã«è¿‘ã„(å¯„ä¸ã—ãªã„)ä¿‚æ•°ãŒã‚ã£ãŸã‚Šã€ç›´æ„Ÿã«ããã‚ãªã„ç¬¦å·ã®ä¿‚æ•°ãŒã‚ã£ãŸã‚Š...\n",
        "\n",
        "ãã‚Œã‚‰ã®å¤‰ãªä¿‚æ•°ã¯ã€è¤‡æ•°ã®ç‰¹å¾´é‡(èª¬æ˜å¤‰æ•°)ãŒäº’ã„ã«é«˜ã„ç›¸é–¢(ç·šå½¢é–¢ä¿‚)ã‚’æŒã£ã¦ã—ã¾ã†ã“ã¨ã§ç”Ÿã˜ã‚‹ã€å¤šé‡å…±ç·šæ€§(multicollinearity)ã¨ã„ã†ç¾è±¡ãŒåŸå› ã§ã™ã€‚\n",
        "\n",
        "ã“ã®ã‚ˆã†ã«ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦å†—é•·ãªæƒ…å ±(ç‰¹å¾´é‡)ã‚’ä¸ãˆã¦ã—ã¾ã†ã¨ã€ä¸æ­£ç¢ºã§ãƒã‚¤ã‚ºã®æ··ã˜ã£ãŸä¿‚æ•°ã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚\n",
        "\n",
        "<å‚è€ƒ: [ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã«ã¤ã„ã¦](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_regularization.html)>\n",
        "\n",
        "\n",
        "ã“ã®å•é¡Œã«å¯¾ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªè§£æ±ºæ–¹æ³•ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦æ¥µç«¯ãªä¿‚æ•°ã‚’å–ã‚‰ã›ãªã„ã‚ˆã†ã«å¼·åˆ¶ã™ã‚‹(æ­£å‰‡åŒ–)ã“ã¨ã§ã™ã€‚\n",
        "\n",
        "æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ãƒªãƒƒã‚¸å›å¸°ã¨å‘¼ã°ã‚Œã‚‹æ­£å‰‡åŒ–æ‰‹æ³•ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "> **ãƒªãƒƒã‚¸(Ridge)å›å¸°** \\\n",
        "> å­¦ç¿’ä¸­ã«ç”¨ã„ã‚‰ã‚Œã‚‹æå¤±(\\*)ã«ä¿‚æ•°ã®äºŒä¹—ã«æ¯”ä¾‹ã™ã‚‹é …ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒçµ¶å¯¾å€¤ã®å¤§ããªä¿‚æ•°ã‚’æ¡ç”¨ã—ãªããªã‚‹ã“ã¨ã‚’ç‹™ã£ãŸæ‰‹æ³• \\\n",
        "> (\\*æå¤±: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å›³ã‚‹æŒ‡æ¨™ã®ä¸€ã¤ã€‚ã“ã®å€¤ãŒå¤§ãã„ã»ã©äºˆæ¸¬å€¤ãŒæœ¬æ¥ã®å€¤ã‹ã‚‰é›¢ã‚Œã¦ã„ã‚‹ã€‚)\n",
        "\n",
        "<br/>\n",
        "\n",
        "These weird coefficients are caused by multiple features measuring almost the same thing --a phenoma called multi-colinearity. Giving redundant information to a linear model, makes it predict unprecise and noisy coefficients.\n",
        "You can learn more about [the limitations of the linear model here](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_regularization.html).\n",
        "\n",
        "A simple solution is to force the model to avoid extreme coefficients, by adding a *regularization*. The subsequent model is called a Ridge regression."
      ],
      "metadata": {
        "id": "K7ULunMCPc9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2 Ridge å›å¸°"
      ],
      "metadata": {
        "id": "qMu6GfBDPocu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgaVu3TePoc9"
      },
      "source": [
        "ãƒªãƒƒã‚¸å›å¸°ã‚’ç”¨ã„ã¦ç®—å‡ºã—ãŸå›å¸°ä¿‚æ•°ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\\\n",
        "å…ˆã»ã©ã®ç‰©ã«æ¯”ã¹ã¦ç­‹ãŒé€šã£ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã¨æ€ã„ã¾ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "Examine the regression coefficients for the Ridge estimator. It looks much more reasonable.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "# å‰å‡¦ç†ã¨Ridgeå›å¸°ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "ridge_pipeline_regression = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', Ridge(alpha=10))\n",
        "])\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
        "ridge_pipeline_regression.fit(X_train, y_train_r.ravel())\n",
        "\n",
        "# Coefficientsã®å–å¾—\n",
        "ridge_coefficients = pd.DataFrame(\n",
        "    ridge_pipeline_regression.named_steps['regressor'].coef_.T,  # transpose the coefficients\n",
        "    index=feature_names,\n",
        "    columns=['coefficient']\n",
        ")\n",
        "\n",
        "ridge_coefficients = ridge_coefficients.reindex(ridge_coefficients['coefficient'].abs().sort_values(ascending=False).index)\n",
        "\n",
        "print('ãƒªãƒƒã‚¸å›å¸°ã‚’ç”¨ã„ã¦ç®—å‡ºã—ãŸå›å¸°ä¿‚æ•° / Coefficients: ')\n",
        "ridge_coefficients.T"
      ],
      "metadata": {
        "id": "mooX53ksPmfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ridge_coefficients.plot(kind='bar', ax=ax)\n",
        "\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Coefficient')\n",
        "plt.title('Ridge Regression Coefficients')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W8tYba_OUZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CahjHSojP2z5"
      },
      "source": [
        "ã§ã¯ã€ç¶šã„ã¦[æ®‹å·®èª¤å·®](https:/www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/residuals.html#:~:text=Definition,yi%E2%88%92%5Eyi)ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã®èª¤å·®ã‚’èª¿ã¹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "We also can explore the errors of the model --called [residuals](https:/www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/residuals.html#:~:text=Definition,yi%E2%88%92%5Eyi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ots8fG9aP20P"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "\n",
        "# Create residual error plot for both training and test data\n",
        "plt.scatter(ridge_pipeline_regression.predict(X_train), ridge_pipeline_regression.predict(X_train) - y_train_r.ravel(), color = \"green\", s = 10, label = 'Train data')\n",
        "plt.scatter(ridge_pipeline_regression.predict(X_test), ridge_pipeline_regression.predict(X_test) - y_test_r.ravel(), color = \"red\", s = 10, label = 'Test data')\n",
        "\n",
        "# Draw a horizontal line at y = 0\n",
        "plt.hlines(y = 0, xmin = ridge_pipeline_regression.predict(X_train).min(), xmax = ridge_pipeline_regression.predict(X_test).max(), linewidth = 2, color=\"black\", linestyle=\"dotted\")\n",
        "\n",
        "# Add legend outside the plot\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=10)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"æ®‹å·®èª¤å·®\")\n",
        "plt.xlabel(r\"äºˆæ¸¬ã•ã‚ŒãŸ SaO2 å€¤: $\\hat y$\")\n",
        "plt.ylabel(r\"äºˆæ¸¬ã«ãŠã‘ã‚‹èª¤å·®: $\\hat y  - y$\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdVknslBQBqw"
      },
      "source": [
        "#### 3.2.3 å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™\n",
        "\n",
        "å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ã¯ã—ã°ã—ã° [$R^2$ score](https://en.wikipedia.org/wiki/Coefficient_of_determination) ãŒç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚$R^2$ score ã®å–ã‚Šã†ã‚‹æœ€å¤§å€¤ã¯ 1.0 ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒæ‚ªã„å ´åˆã«ã¯è² ã«ãªã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚çœŸå€¤(çœŸã®y)ãŒä¸€æ„ã«å®šã¾ã‚‰ãªã„ã‚±ãƒ¼ã‚¹ã§ã¯ã€å®šæ•°ãƒ¢ãƒ‡ãƒ«ã®$R^2$ score ã¯ 0.0 ã¨ãªã‚Šã¾ã™ãŒã€ã“ã‚Œã¯å®šæ•°ãƒ¢ãƒ‡ãƒ«ãŒå…¥åŠ›ç‰¹å¾´ã‚’ç„¡è¦–ã—ã¦å¸¸ã«å¹³å‡ y ã‚’äºˆæ¸¬å€¤ã¨ã™ã‚‹ã‹ã‚‰ã§ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "Regression tasks are often evaluated using the [$R^2$ score](https://en.wikipedia.org/wiki/Coefficient_of_determination). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). In the general case when the true y is non-constant, a constant model that always predicts the average y disregarding the input features would get a $R^2$ score of 0.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msEj0dBCQBq7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the linear model with R2 :\n",
        "y_pred_train_linear = linear_pipeline_regression.predict(X_train)\n",
        "linear_train_r2 = r2_score(y_train_r, y_pred_train_linear)\n",
        "print(f'ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã® R2 score: {linear_train_r2}')\n",
        "\n",
        "# Evaluate the ridge model with R2 :\n",
        "y_pred_train_ridge = ridge_pipeline_regression.predict(X_train)\n",
        "ridge_train_r2 = r2_score(y_train_r, y_pred_train_ridge)\n",
        "print(f'ãƒªãƒƒã‚¸å›å¸°ãƒ¢ãƒ‡ãƒ«ã® R2 score: {ridge_train_r2}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer = ResidualsPlot(ridge_pipeline_regression)\n",
        "visualizer.fit(X_train, y_train_r.ravel())\n",
        "visualizer.score(X_test, y_test_r.ravel())\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "hlQCoIB1Ugiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_pipeline_regression.fit(X_train, y_train_r.ravel())\n",
        "hat_y_test = ridge_pipeline_regression.predict(X_test)\n",
        "\n",
        "fig, ax = plt.subplots(1)\n",
        "sns.regplot(data=None, x=y_test_r, y=hat_y_test, line_kws={\"color\":\"black\", \"linestyle\":\"dotted\"})\n",
        "plt.plot()\n",
        "ax.set(xlabel=r\"$y$\", ylabel=r\"$\\hat y$\")\n",
        "test_r2_score_ = r2_score(y_test_r, hat_y_test)\n",
        "print(f\"R2 score: {test_r2_score_}\")\n"
      ],
      "metadata": {
        "id": "12ZvR7VEUi0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z5eeQQOQBq8"
      },
      "source": [
        "âœï¸ ä»–ã«ã‚‚æ§˜ã€…ãªæ€§èƒ½è©•ä¾¡æŒ‡æ•°ãŒ[sklearnã§ã¯å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)ã€‚ä»Šå›ã®ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€ã‚ˆã‚Šé©å½“ã ã¨æ€ã‚ã‚Œã‚‹æ€§èƒ½è©•ä¾¡æŒ‡æ•°ãŒã‚ã‚Œã°è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\\\n",
        "Different regression metrics are [implemented by sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics). Try another metric that you think is relevant to this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74mP33muQBq8"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### â€»ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ã¤ã„ã¦ / Model Evaluation\n",
        "\n",
        "SpO2å€¤ã®è£œæ­£ç”¨ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã—ã€è‡¨åºŠã«å°å…¥ã™ã‚‹ãŸã‚ã«ã¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ­£ã—ãè©•ä¾¡ã™ã‚‹ã“ã¨ãŒå¿…é ˆã§ã™ã€‚\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ãŒä½¿ã‚ã‚Œã‚‹è‡¨åºŠã®ç’°å¢ƒã ã‘ã§ãªãã€å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã“ã¨ã‚‚è€ƒæ…®ã—ã¦æ€§èƒ½è©•ä¾¡ã®ãŸã‚ã®æŒ‡æ¨™(ãƒ¡ãƒˆãƒªã‚¯ã‚¹: Metrics)ã‚’æ…é‡ã«é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "> è€ƒæ…®ã™ã¹ãç‚¹ã®ä¾‹\n",
        "- å­¦ç¿’ã«ç”¨ã„ã‚‰ã‚ŒãŸåç¾©å¤‰æ•°ã«å«ã¾ã‚Œã¦ã„ãªã„åç¾©å¤‰æ•°ã‚’æŒã¤æ‚£è€…ã«å¯¾ã—ã¦ã¯ã©ã®ã‚ˆã†ãªæ€§èƒ½ã‚’ç¤ºã™ã ã‚ã†ã‹\n",
        "- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å¤‰æ•°ã®ä¸­ã§ã€ç‰¹å®šã®ç¯„å›²ã«åã£ãŸã‚‚ã®ã¯ãªã„ã ã‚ã†ã‹\n",
        "\n",
        "<br/>\n",
        "\n",
        "To be able to develop an ML model for the recalibration of SpO2 levels and implement it in a clinical setting, it is crucial to properly evaluate its performance in the task that is supposed to do.\n",
        "\n",
        "A set of performance metrics should be carefully chosen, considering the clinical setting where the model will be aplied in but also the dataset where is what trained on:\n",
        "- If the dataset contains mostly one racial group, how will it perform on others patients?\n",
        "- Does the dataset have patients from a wide range of ages or is it more focused on a narrow range?\n"
      ],
      "metadata": {
        "id": "qKvH-pPPOjNM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthIUsyqQBq9"
      },
      "source": [
        "å…ˆã®ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚„ãƒªãƒƒã‚¸å›å¸°ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã® $R^2$ score ã¯ã‚ã¾ã‚Šæº€è¶³ã®ã„ãã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã‚Œã«ã¤ã„ã¦ã¯ä»¥ä¸‹ã®äºŒã¤ã®èª¬æ˜ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "1. ç›®çš„å¤‰æ•°ã«å¯¾ã—ã¦ååˆ†ãªèª¬æ˜å¤‰æ•°ã‚’æŒã£ã¦ã„ãªã„\n",
        "2. ç›®çš„å¤‰æ•°ãŒç·šå½¢ã®é–¢ä¿‚ã«ãªã„\n",
        "\n",
        "âœï¸ $R^2$ score ã§è©•ä¾¡ã™ã‚‹å ´åˆã€ã‚ˆã‚Šã‚ˆã„å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’æŒ™ã’ã‚‹ã“ã¨ãŒã§ãã¾ã™ã‹ï¼Ÿä¾‹ãˆã°ã€Œ[æ±ºå®šæœ¨](https://scikit-learn.org/stable/modules/tree.html#regression)ã€ã‚„ã€Œ[ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ãƒ¢ãƒ‡ãƒ«](https://scikit-learn.org/stable/modules/ensemble.html)ã€ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚ã‚‚ã¡ã‚ã‚“ã€ä»–ã®ã‚¿ã‚¤ãƒ—ã®ç·šå½¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦ã‚‚è‡ªç”±ã«è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "We saw that the regression metrics of both the linear regression and the regularized linear ridge regressions are not very satisfying. There are two explanations: either, we have not the proper variables to explain the output variables or these variables do not relate to the SaO2 with a pure linear relationship.\n",
        "\n",
        "âœï¸  Can you fit and evaluate a better regression model in term of $R^2$ ? Consider for example [decision trees](https://scikit-learn.org/stable/modules/tree.html#regression) or [ensemble models](https://scikit-learn.org/stable/modules/ensemble.html). But feel free to experiment with any other type of regression algorithms !"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n"
      ],
      "metadata": {
        "id": "fNEsElG5nLsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã¿ã‚ˆã† / Create classification models\n",
        "\n",
        "ã“ã“ã§ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸç‰¹å¾´é‡ã‹ã‚‰ `hidden_hypoxemia` ã‚’äºˆæ¸¬ã™ã‚‹åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã® `hidden_hypoxemia` ã‚’äºˆæ¸¬ã—ã¦ã¿ã¾ã™ã€‚\n",
        "\n",
        "Here, we use the training data to create a classification model that predicts `hidden_hypoxemia` from the given features and try to predict `hidden_hypoxemia` in the test data."
      ],
      "metadata": {
        "id": "hp-lgVpXDmYC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8EaYDUnQqIm"
      },
      "source": [
        "#### 3.3.1 SVMåˆ†é¡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ / SVM Classification Baseline\n",
        "\n",
        "ä¸–ã®ä¸­ã«ã¯åˆ†é¡ã®ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‰ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ãŒå¤šãã‚ã‚Šã¾ã™ã€‚\\\n",
        "ç§ãŸã¡ã¯ãã®ä¸­ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦æœ€é©ãªã‚‚ã®ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\\\n",
        "ã“ã“ã§ã¯ãã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã®ä¸€ã¤ã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒã‚·ãƒ¼ãƒ³(SVM)ã®å®Ÿè£…ã‚’å¼•ãåˆã„ã«å‡ºã—ã¾ã™ã€‚\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã«å…ˆã‚“ã˜ã¦ã€ãƒ­ãƒã‚¹ãƒˆã§æ­£ç¢ºãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã†ãˆã§éå¸¸ã«é‡è¦ã¨ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒã‚¤ãƒ‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ãƒã‚¤ãƒ‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(\\*)ã¯ãƒã‚¤ã‚¢ã‚¹ã¨å…±åˆ†æ•£ã®å‡è¡¡ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«å½¹ç«‹ã¤ã ã‘ã§ãªãã€ãƒ¢ãƒ‡ãƒ«ã®éå­¦ç¿’(Overfitting)ã‚„å­¦ç¿’ä¸è¶³(Underfitting)ã‚’é˜²æ­¢ã™ã‚‹ã“ã¨ã«ã‚‚å½¹ç«‹ã¡ã¾ã™ã€‚\\\n",
        "<[è©³ç´°ã¯ã“ã¡ã‚‰](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)>\n",
        "\n",
        "> \\*ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼: \\\n",
        "> ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ä¸­ã«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã•ã‚Œãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã§ã‚ã‚Šã€è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã®å‰ã«è¨­å®šã•ã‚Œã¾ã™ã€‚å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ§‹é€ ã‚„å‹•ä½œã‚’æ±ºå®šã—ã¾ã™ã€‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®èª¿æ•´ã¯ã€å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å¤§ããå½±éŸ¿ã™ã‚‹ãŸã‚ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã¨ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "For the classification task, you might choose from a wide range of models that you think are most suitable. Here you have the example of the implementation of a Support Vector Machine (SVM).\n",
        "\n",
        "Before training the model, we need to set a given number of parameters - i.e. hyperparameters - which will be critical in building robust and accurate models. They help us find the balance between bias and variance and thus, prevent the model from overfitting or underfitting.Keep in mind that if you increase the range of hyperparameters to be tested, the training time will increase significantly. If you want more information read [here](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å‰å‡¦ç†ã¨SVCã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "svc_pipeline_classifier = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(probability=True))\n",
        "])\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
        "param_grid = {\n",
        "    'classifier__C': ,\n",
        "    'classifier__gamma': ,\n",
        "    'classifier__kernel':\n",
        "}\n",
        "\n",
        "# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®è¨­å®š\n",
        "svc_grid_search = GridSearchCV(\n",
        "    svc_pipeline_classifier,\n",
        "    param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    refit = True,\n",
        "    verbose = 3,\n",
        ")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
        "svc_grid_search.fit(X_train, y_train_c.ravel())\n",
        "\n",
        "# äºˆæ¸¬\n",
        "svc_predictions = svc_grid_search.predict(X_test)\n",
        "\n",
        "# ã‚¹ã‚³ã‚¢ã®è¡¨ç¤º\n",
        "print('SVC Score: {}'.format(svc_grid_search.score(X_test, y_test_c.ravel())))"
      ],
      "metadata": {
        "id": "c9pYdNVGrzBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tocS4IDsQqIn"
      },
      "source": [
        "ã”è¦§ã®é€šã‚Šã€ã‚¹ã‚³ã‚¢ã¯ã»ã¼100%ã§ã™ï¼å•é¡Œãªã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
        "ãã¡ã‚“ã¨åˆ†é¡ã§ãã¦ã„ã‚‹ã‹ã€æ··åŒè¡Œåˆ—(Confusion Matrix)ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "As you can observe, the scoring is almost 100%! Great, right? \\\n",
        "Well, let's take a closer look at the confusion matrix for a deeper analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUqabzpIQqIn"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix(y_test_c, svc_predictions))\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check balance\n",
        "num_of_hidden_hypoxemia = (df_train['hidden_hypoxemia']==1).sum()\n",
        "num_of_normal= (df_train['hidden_hypoxemia']==0).sum()\n",
        "print(f\"{num_of_hidden_hypoxemia} measurements with hidden hypoxia / {num_of_normal} normal ones\")"
      ],
      "metadata": {
        "id": "nA-bhHTBQqIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2dBKUHQqIn"
      },
      "source": [
        "11,641ä»¶ã®æ­£å¸¸ãªæ¸¬å®šå€¤(= æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã§ãªã„)ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã€190ä»¶ã—ã‹æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã§ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆâ€»ã“ã®ä»¶æ•°ã¯ã‚ãªãŸã®ãƒãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦å¤‰ã‚ã‚Šã¾ã™ï¼‰ã€‚ã“ã‚Œã¯éå¸¸ã«ä¸å‡è¡¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨€ãˆã‚‹ã§ã—ã‚‡ã†ã€‚ã§ã¯ã€ã©ã®ã‚ˆã†ã«å¯¾å‡¦ã—ã¾ã—ã‚‡ã†ï¼Ÿ\n",
        "\n",
        "<br />\n",
        "\n",
        "ã“ã®ã‚ˆã†ãªä¸å‡è¡¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã¯ã„ãã¤ã‹ã®å¯¾å‡¦æ³•ãŒå­˜åœ¨ã—ã¾ã™ã€‚\n",
        "\n",
        "*   å¤šæ•°æ´¾ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’å°‘æ•°æ´¾ã®ãƒ‡ãƒ¼ã‚¿æ•°ã«åˆã‚ã›ã‚‹\n",
        "> ã“ã®æ‰‹æ³•ã¯å˜ç´”ã§ã™ãŒã€å¤šãã®è²´é‡ãªãƒ‡ãƒ¼ã‚¿ã‚’æå¤±ã™ã‚‹ã“ã¨ã«ç¹‹ãŒã‚Šã¾ã™ã€‚\n",
        "\n",
        "<br />\n",
        "\n",
        "*   å°‘æ•°æ´¾ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦SMOTEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãªã©ã‚’ç”¨ã„ã¦ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹\n",
        "> ã“ã®æ‰‹æ³•ã§ã¯ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®å„å¤‰æ•°ã«ã¤ã„ã¦ãã®åˆ†æ•£ã‚’å˜ã«ç¶­æŒã—ã¦ã—ã¾ã†ã¨ã„ã†å•é¡ŒãŒã‚ã‚Šã€æœ€ã‚‚è‰¯ã„æ–¹æ³•ã¨ã¯è¨€ãˆãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n",
        "\n",
        "<br />\n",
        "\n",
        "* ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«é©å¿œã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã†ã€‚\n",
        "> ä¾‹ãˆã° [Imbalanced Learn](https://imbalanced-learn.org/stable/) ã¨ã„ã† sklearn ã¨ã‚ˆãä¼¼ãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯SVMã‚’å«ã‚€ã»ã¨ã‚“ã©ã® sklearn ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã® class_weight ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¸å‡è¡¡ã‚¯ãƒ©ã‚¹ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã‚ˆã‚Šè©³ã—ãçŸ¥ã‚ŠãŸã„æ–¹ã¯[ã“ã¡ã‚‰](https://medium.com/eni-digitalks/imbalanced-data-an-extensive-guide-on-how-to-deal-with-imbalanced-classification-problems-6c8df0bc2cab)ã‹ã‚‰ã”è¦§ãã ã•ã„ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "It looks like we have a really imbalanced dataset, where we have only 190 measurements with hidden hypoxia compared to 11,641 normal ones (the number of records may vary according to your team's dataset). What should we do then?\n",
        "\n",
        "One can take several approaches when dealing with imbalanced datasets:\n",
        "- removing the number of datapoints for the majority class to match the number on the minority class. However, this might lead you to loose a lot of information.\n",
        "- upsample the minority class and generate synthetic data on it, using for example the SMOTE algorithm. This approach has the problem of maintaining the distribution of each variable for that class and might not provide the best results.\n",
        "- another approach might be to use an algorithm approppriate for this type of data. For that, there is a package very similar to sklearn called [Imbalanced Learn](https://imbalanced-learn.org/stable/) and add the class imbalance to the class_weight parameter in most sklearn algorithms (including in [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)).\n",
        "\n",
        "You can read more about it [here](https://medium.com/eni-digitalks/imbalanced-data-an-extensive-guide-on-how-to-deal-with-imbalanced-classification-problems-6c8df0bc2cab)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2 Random Forest Classifier / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨"
      ],
      "metadata": {
        "id": "TUAyzs0pvtf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¶šã„ã¦ã€ Random Forest Classifier (RFC: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨) ã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚<br>\n",
        "RFC ã¯ã€è¤‡æ•°ã®æ±ºå®šæœ¨ã‚’å­¦ç¿’ã—ã€ãã‚Œã‚‰ã®çµæœã®å¤šæ•°æ±ºã‚„å¹³å‡ã«ã‚ˆã£ã¦æœ€çµ‚çš„ãªäºˆæ¸¬ã‚’è¡Œã†ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®æ‰‹æ³•ä¸€ã¤ã§ã™ã€‚<br>\n",
        "RFC ã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)ã«è©³ç´°ãªèª¬æ˜ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "Next, let's create a model using the Random Forest Classifier (RFC). The RFC is a type of ensemble learning method that makes a final prediction based on the majority vote or average of the results obtained from multiple decision trees. For a detailed explanation of RFC, please refer to [this link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ],
      "metadata": {
        "id": "rRbmlh5OxqzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# å‰å‡¦ç†ã¨RandomForestClassifierã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "rfc_pipeline_classifier = Pipeline(\n",
        "    [\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('classifier', RandomForestClassifier(class_weight=\"balanced\"))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': ,  # ãƒ„ãƒªãƒ¼ã®æ•°\n",
        "    'classifier__max_depth': ,  # ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±åº¦\n",
        "    'classifier__min_samples_split': ,  # ãƒãƒ¼ãƒ‰ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "    'classifier__min_samples_leaf': ,  # è‘‰ãƒãƒ¼ãƒ‰ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "}\n",
        "\n",
        "# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®è¨­å®š\n",
        "rfc_grid_search = GridSearchCV(\n",
        "    rfc_pipeline_classifier,\n",
        "    param_distributions=param_dist,\n",
        "    refit=True,\n",
        "    verbose=3,\n",
        "    n_iter=5,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
        "rfc_grid_search.fit(X_train, y_train_c.ravel())\n",
        "\n",
        "# äºˆæ¸¬\n",
        "rfc_predictions = rfc_grid_search.predict(X_test)\n",
        "\n",
        "# ã‚¹ã‚³ã‚¢ã®è¡¨ç¤º\n",
        "print('RFC Score: {}'.format(rfc_grid_search.score(X_test, y_test_c.ravel())))\n"
      ],
      "metadata": {
        "id": "U8wKv4NSv0iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(confusion_matrix(y_test_c, rfc_predictions))\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nRrzz4PGbESH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kz9ajY8QqIo"
      },
      "source": [
        "#### 3.3.3 åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã‚‚è©•ä¾¡æŒ‡æ¨™ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚<br>\n",
        "åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ã€æ··åŒè¡Œåˆ—ã«åŸºã¥ãæ§˜ã€…ãªæŒ‡æ¨™ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚<br>\n",
        "ä»£è¡¨çš„ãªã‚‚ã®ã¯ã€ accuracyï¼ˆç²¾åº¦ï¼‰ã‚„ precisionï¼ˆé©åˆç‡ï¼‰ã€recall ï¼ˆå†ç¾ç‡ï¼‰ã€ F1 ã‚¹ã‚³ã‚¢ãªã©ãŒã‚ã‚Šã¾ã™ã€‚<br>\n",
        "ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã¯ãã‚Œãã‚Œç•°ãªã‚‹æ€§è³ªã‚’æŒã£ã¦ã„ã¾ã™ã€‚<br>\n",
        "accuracy ã¯ã€å…¨ä½“ã®äºˆæ¸¬ã®æ­£ç¢ºã•ã‚’ç¤ºã—ã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ãŒã‚ã‚‹å ´åˆã«ã¯é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã¨ã¯ãªã‚Šã¾ã›ã‚“ã€‚<br>\n",
        "precision ã¯ã€æ­£ã¨äºˆæ¸¬ã—ãŸã‚‚ã®ã®ä¸­ã§å®Ÿéš›ã«æ­£ã§ã‚ã‚‹ã‚‚ã®ã®å‰²åˆã‚’è¡¨ã—ã€ recall ã¯å®Ÿéš›ã«æ­£ã§ã‚ã‚‹ã‚‚ã®ã®ä¸­ã§æ­£ã¨äºˆæ¸¬ã§ããŸã‚‚ã®ã®å‰²åˆã‚’ç¤ºã—ã¾ã™ã€‚<br>\n",
        "F1ã‚¹ã‚³ã‚¢ã¯é©åˆç‡ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡ã§ã€ä¸¡è€…ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹æŒ‡æ¨™ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚<br>\n",
        "\n",
        "\n",
        "Let's also take a look at evaluation metrics for classification models. <br>\n",
        "There are various metrics known for evaluating classification models, many of which are based on the confusion matrix. <br>\n",
        "Prominent among these are accuracy, precision, recall, and the F1 score.\n",
        "<br>\n",
        "These metrics each have different characteristics. <br>\n",
        "Accuracy represents the overall correctness of the predictions, but it is not an appropriate evaluation metric when there is class imbalance in the data. <br>\n",
        "Precision represents the proportion of the instances predicted as positive that are actually positive, while recall represents the proportion of actual positive instances that were correctly predicted as such. <br>\n",
        "The F1 score is the harmonic mean of precision and recall and is used as a metric to balance the two.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SoGhgjQHQqIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt_vkTRjQqIp"
      },
      "outputs": [],
      "source": [
        "# SVCã®ClassificationReport\n",
        "svc_visualizer = ClassificationReport(svc_grid_search.best_estimator_, classes=[0,1], support=True)\n",
        "\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®fitã‚’è¡Œã„ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
        "svc_visualizer.fit(X_train, y_train_c.ravel())\n",
        "svc_visualizer.score(X_test, y_test_c.ravel())\n",
        "svc_visualizer.show()\n",
        "\n",
        "# RandomForestClassifierã®ClassificationReport\n",
        "rfc_visualizer = ClassificationReport(rfc_random_search.best_estimator_, classes=[0,1], support=True)\n",
        "\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®fitã‚’è¡Œã„ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
        "rfc_visualizer.fit(X_train, y_train_c.ravel())\n",
        "rfc_visualizer.score(X_test, y_test_c.ravel())\n",
        "rfc_visualizer.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW95nMHyQqIp"
      },
      "outputs": [],
      "source": [
        "# SVCã®ClassPredictionError\n",
        "svc_cpe_visualizer = ClassPredictionError(svc_grid_search.best_estimator_, classes=[0,1])\n",
        "\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®fitã‚’è¡Œã„ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
        "svc_cpe_visualizer.fit(X_train, y_train_c.ravel())\n",
        "svc_cpe_visualizer.score(X_test, y_test_c.ravel())\n",
        "svc_cpe_visualizer.show()\n",
        "\n",
        "# RandomForestClassifierã®ClassPredictionError\n",
        "rfc_cpe_visualizer = ClassPredictionError(rfc_random_search.best_estimator_, classes=[0,1])\n",
        "\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®fitã‚’è¡Œã„ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
        "rfc_cpe_visualizer.fit(X_train, y_train_c.ravel())\n",
        "rfc_cpe_visualizer.score(X_test, y_test_c.ravel())\n",
        "rfc_cpe_visualizer.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PR-AUCã«ã¤ã„ã¦ / PR-AUC\n",
        "PR-AUC (Precision-Recall Area Under the Curve)ã¯ã€Precision-Recallæ›²ç·šï¼ˆé©åˆç‡-å†ç¾ç‡æ›²ç·šï¼‰ã®ä¸‹ã®é¢ç©ã‚’è¨ˆæ¸¬ã™ã‚‹è©•ä¾¡æŒ‡æ¨™ã§ã™ã€‚ä¸å‡è¡¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹éš›ã«ã€ã‚ˆãä½¿ç”¨ã•ã‚Œã¾ã™ã€‚PRæ›²ç·šã¯ã€ç•°ãªã‚‹é–¾å€¤è¨­å®šã§ã®ãƒ¢ãƒ‡ãƒ«ã®é©åˆç‡ã¨å†ç¾ç‡ã®é–¢ä¿‚ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ãŸã‚‚ã®ã§ã€PR-AUCã¯ã“ã®æ›²ç·šä¸‹ã®é¢ç©ã‚’ç¤ºã—ã¾ã™ã€‚å€¤ãŒ1ã«è¿‘ã„ã»ã©ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒé«˜ã„ã¨è©•ä¾¡ã•ã‚Œã¾ã™ã€‚<br>\n",
        "PR-AUCã«ã¤ã„ã¦ã€è©³ç´°ãªèª¬æ˜ã¯[ã“ã¡ã‚‰ã®ãƒšãƒ¼ã‚¸](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "PR-AUC (Precision-Recall Area Under the Curve) is an evaluation metric that measures the area under the Precision-Recall curve. It is often used to evaluate the performance of a model on imbalanced datasets. The Precision-Recall curve plots the relationship between the model's precision and recall at different threshold settings, and the PR-AUC represents the area under this curve. The closer the value is to 1, the higher the model's performance is evaluated.\n",
        " If you want more information read [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)."
      ],
      "metadata": {
        "id": "WEDMsz-i0XvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# SVCãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—\n",
        "svc_probs = svc_grid_search.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# RFCãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—\n",
        "rfc_probs = rfc_random_search.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—\n",
        "svc_f1 = f1_score(y_test_c.ravel(), svc_predictions)\n",
        "svc_roc_auc = roc_auc_score(y_test_c.ravel(), svc_probs)\n",
        "svc_pr_auc = average_precision_score(y_test_c.ravel(), svc_probs)\n",
        "\n",
        "rfc_f1 = f1_score(y_test_c.ravel(), rfc_predictions)\n",
        "rfc_roc_auc = roc_auc_score(y_test_c.ravel(), rfc_probs)\n",
        "rfc_pr_auc = average_precision_score(y_test_c.ravel(), rfc_probs)\n",
        "\n",
        "# çµæœã‚’å‡ºåŠ›\n",
        "print(\"SVC Metrics:\")\n",
        "print(f\"F1 Score: {svc_f1}\")\n",
        "print(f\"ROC-AUC: {svc_roc_auc}\")\n",
        "print(f\"PR-AUC: {svc_pr_auc}\")\n",
        "\n",
        "print(\"\\nRFC Metrics:\")\n",
        "print(f\"F1 Score: {rfc_f1}\")\n",
        "print(f\"ROC-AUC: {rfc_roc_auc}\")\n",
        "print(f\"PR-AUC: {rfc_pr_auc}\")\n"
      ],
      "metadata": {
        "id": "nm_GKD2BdD1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# ROC curveã®è¨ˆç®—\n",
        "fpr_svc, tpr_svc, _ = roc_curve(y_test_c.ravel(), svc_probs)\n",
        "roc_auc_svc = auc(fpr_svc, tpr_svc)\n",
        "\n",
        "fpr_rfc, tpr_rfc, _ = roc_curve(y_test_c.ravel(), rfc_probs)\n",
        "roc_auc_rfc = auc(fpr_rfc, tpr_rfc)\n",
        "\n",
        "# PR curveã®è¨ˆç®—\n",
        "precision_svc, recall_svc, _ = precision_recall_curve(y_test_c.ravel(), svc_probs)\n",
        "pr_auc_svc = auc(recall_svc, precision_svc)\n",
        "\n",
        "precision_rfc, recall_rfc, _ = precision_recall_curve(y_test_c.ravel(), rfc_probs)\n",
        "pr_auc_rfc = auc(recall_rfc, precision_rfc)\n",
        "\n",
        "# å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# ROC curveã®å¯è¦–åŒ–\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr_svc, tpr_svc, color='blue', label=f'SVC ROC curve (area = {roc_auc_svc:.2f})')\n",
        "plt.plot(fpr_rfc, tpr_rfc, color='green', label=f'RFC ROC curve (area = {roc_auc_rfc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# PR curveã®å¯è¦–åŒ–\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall_svc, precision_svc, color='blue', label=f'SVC PR curve (area = {pr_auc_svc:.2f})')\n",
        "plt.plot(recall_rfc, precision_rfc, color='green', label=f'RFC PR curve (area = {pr_auc_rfc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nzwKFKyVejXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWirsNQgQqIq"
      },
      "source": [
        "#### 3.3.4 âœï¸ é©åˆ‡ãªæ–¹æ³•ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¦ã¿ã¾ã—ã‚‡ã† / Properly evaluate your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2fsrODSQqIq"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBUfvvStQqIq"
      },
      "source": [
        "### 3.4 ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ / Your model\n",
        "\n",
        "ã“ã“ã¾ã§ã«å­¦ã‚“ã ãƒ¢ãƒ‡ãƒ«ä½œæˆã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’ç”¨ã„ã¦ã€æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã‚’äºˆæ¸¬ã™ã‚‹ã®ã«æœ€ã‚‚é©åˆ‡ã¨æ€ã‚ã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é¸ã³ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "å¿…è¦ã§ã‚ã‚Œã°æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã®ã‚³ãƒ¼ãƒ‰ã‚’è‡ªç”±ã«ä½¿ã£ã¦ã‚‚ã‚‰ã£ã¦æ§‹ã„ã¾ã›ã‚“ã€‚\n",
        "\n",
        "<br/>\n",
        "\n",
        "Use the insigths learned from the models previously presented to build your own model with the framework you think is most suitable. You are free to use any of the code presented in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuTTZlTMQqIq"
      },
      "source": [
        "###### 3.4.1 âœï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã‚ˆã† /  Implement your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JwjeZQjQqIq"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKuNtyjDQqIq"
      },
      "source": [
        "## 4. ç‰¹å¾´é‡ã®é‡è¦åº¦ / Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAWCruDPQqIq"
      },
      "source": [
        "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦SHAPå€¤ã‚’è¨ˆç®—ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚SHAPã«ã¤ã„ã¦ã®è©³ç´°ã¯[ã“ã¡ã‚‰ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://shap-lrjball.readthedocs.io/en/latest/index.html)ã‚’å¾¡è¦§ãã ã•ã„ã€‚\n",
        "\n",
        "Compute the SHAP values for the test data. If you want more information about SHAP, read [here](https://shap-lrjball.readthedocs.io/en/latest/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "import shap\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
        "rfc_model = rfc_random_search.best_estimator_.named_steps['classifier']\n",
        "fitted_custom_transform = rfc_random_search.best_estimator_.named_steps['custom_transform']\n",
        "fitted_preprocessor = rfc_random_search.best_estimator_.named_steps['preprocessor']\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†\n",
        "X_train_transformed = fitted_custom_transform.transform(X_train)\n",
        "X_train_preprocessed = fitted_preprocessor.transform(X_train_transformed)\n",
        "\n",
        "X_test_transformed = fitted_custom_transform.transform(X_test)\n",
        "X_test_preprocessed = fitted_preprocessor.transform(X_test_transformed)\n",
        "\n",
        "# äº‹å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—\n",
        "background_data = shap.sample(X_train_preprocessed, 100)  # 100ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨\n",
        "\n",
        "# TreeExplainerã‚’ä½œæˆ\n",
        "explainer = shap.TreeExplainer(rfc_model, background_data, check_additivity=False)\n",
        "\n",
        "# SHAPå€¤ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—\n",
        "shap_values_test = explainer.shap_values(X_test_preprocessed, check_additivity=False)\n",
        "\n",
        "def get_feature_names(column_transformer):\n",
        "    \"\"\"Get feature names from all transformers.\"\"\"\n",
        "    return column_transformer.get_feature_names_out()\n",
        "\n",
        "# ãƒ•ã‚£ãƒƒãƒˆæ¸ˆã¿ã®ColumnTransformerã‹ã‚‰ç‰¹å¾´åã‚’å–å¾—\n",
        "fitted_feature_names = fitted_preprocessor.get_feature_names_out()\n",
        "\n",
        "# ã“ã®æ–°ã—ã„ç‰¹å¾´åãƒªã‚¹ãƒˆã§SHAPã®è¦ç´„ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
        "shap.summary_plot(shap_values_test[1], X_test_preprocessed, feature_names=fitted_feature_names)\n"
      ],
      "metadata": {
        "id": "S5X1atxgh5G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Random Forest Classifier ã®å ´åˆã€ feature importance ã‚’å–å¾—ã™ã‚‹ã“ã¨ãŒå¯èƒ½\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ€è‰¯ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨ã‚’å–å¾—\n",
        "best_rfc = rfc_random_search.best_estimator_.named_steps['classifier']\n",
        "\n",
        "# ç‰¹å¾´é‡ã®åå‰ã‚’å–å¾—ï¼ˆå‰å‡¦ç†ã®æƒ…å ±ã‚‚å«ã‚ã‚‹ãŸã‚ã€'preprocessor'ã‚’é€šã—ã¦å–å¾—ï¼‰\n",
        "features = rfc_random_search.best_estimator_.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—\n",
        "feature_importance = best_rfc.feature_importances_\n",
        "\n",
        "# é‡è¦åº¦ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
        "\n",
        "# é‡è¦åº¦ã®é™é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
        "plt.title('Feature Importance - Random Forest Classifier')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EcpeP6BGj7Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. äº¤å·®æ¤œè¨¼ / Cross validation"
      ],
      "metadata": {
        "id": "KBMuDX9O7v2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "äº¤å·®æ¤œè¨¼ï¼ˆã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã®ä¸€ã¤ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¤‡æ•°ã®ã‚µãƒ–ã‚»ãƒƒãƒˆï¼ˆé€šå¸¸ã¯ç­‰åˆ†ï¼‰ã«åˆ†å‰²ã—ã€ãã®ã†ã¡ã®ä¸€ã¤ã‚’æ¤œè¨¼ç”¨ã‚»ãƒƒãƒˆã¨ã—ã¦æ®‹ã‚Šã‚’è¨“ç·´ã‚»ãƒƒãƒˆã¨ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚ã“ã®éç¨‹ã‚’å„ã‚µãƒ–ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸€åº¦ã¯æ¤œè¨¼ã‚»ãƒƒãƒˆã¨ã—ã¦ç”¨ã„ã‚‰ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã‚ˆã‚Šä¿¡é ¼æ€§é«˜ãè©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»£è¡¨çš„ãªäº¤å·®æ¤œè¨¼æ³•ã«ã¯k-åˆ†å‰²äº¤å·®æ¤œè¨¼ãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã¯[ã“ã¡ã‚‰ã®ãƒšãƒ¼ã‚¸](https://scikit-learn.org/stable/modules/cross_validation.html)ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "Cross-validation is a technique used to evaluate the generalization ability of a model. It involves dividing the dataset into several subsets (usually equal parts), using one as the test set and the rest as the training set. By repeating this process for each subset, all data are used at least once as a test set, enabling a more reliable evaluation of the model's performance. A popular method of cross-validation is k-fold cross-validation. For more information, please read [this page](https://scikit-learn.org/stable/modules/cross_validation.html)."
      ],
      "metadata": {
        "id": "Jdr0Izr07z3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!"
      ],
      "metadata": {
        "id": "Qz8CFM5q3bcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgement\n",
        "æ—¥æœ¬èªè¨³ (Translation):é½‹è—¤ å¹¸å²éƒ (Koshiro Saito)"
      ],
      "metadata": {
        "id": "TFvc_-zQQqIr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fLfLhYkUlLzc",
        "aDGC2hIMlN9P"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}