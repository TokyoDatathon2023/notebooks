{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2chWXcuEuQN6"
      },
      "source": [
        "# Workshop 3: Let's get our model! / 実際にモデルを作りましょう！"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🎨 チーム/Team __**\n",
        "\n",
        "`上記の下線部にチーム番号を記入してください` \\\n",
        "`Add your team number above`"
      ],
      "metadata": {
        "id": "3iSlTBpwvWjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**👥 メンバー/Authors**\n",
        "\n",
        "`50音順でチームメンバーの氏名を全て記載してください`\\\n",
        "`Add your names here (alphabetically)`"
      ],
      "metadata": {
        "id": "P-SOUUOevhjs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOhnU575voaW"
      },
      "source": [
        "**🎯 このワークショップの目標/Goals of this Workshop**\n",
        "1. 訓練データとテストデータを適当に分割できるようになる\n",
        "2. 作成したモデルの評価に用いる性能指標(メトリクス)を定義できるようになる\n",
        "3. 以下の機能を備えた機械学習モデルを開発する\n",
        "  - $SaO_2$の値を予測する (回帰問題)\n",
        "  - $SaO_2$と$SpO_2$の差を予測する (回帰問題)\n",
        "  - 潜在性低酸素血症を検知する (分類問題)\n",
        "\n",
        "  (モデルは線形でも非線形でもよい)\n",
        "\n",
        "4. パラメータの最適化手法の一つであるグリッドサーチを実装できるようになる\n",
        "5. 回帰問題や分類問題について最も寄与する特徴量がなんであったか評価できるようになる\n",
        "\n",
        "<br/>\n",
        "\n",
        "1. Understand the best way to split the data into train and test\n",
        "2. Define the performance metrics that you are going to use in the evaluation of your model\n",
        "3. Develop a machine learning (ML) model to either:\n",
        "  - predict $SaO_2$ values -> regression\n",
        "  - predict the gap between $SaO_2$ and $SpO_2$ -> regression\n",
        "  - detect cases of Hidden Hypoxemia (HH) -> classification\n",
        "\n",
        "  The developed model can either be linear or non-linear.\n",
        "\n",
        "4. Implement grid-search to further optimize parameters.\n",
        "5. Assess what were the most relevant features for the regression/classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZTkWnmCzKvb"
      },
      "source": [
        "**✏️ 最終目標 / Expected Deliverables**\n",
        " - 評価指標や特徴量の重要度を正しく理解した上でモデルを実際に作成すること\n",
        "\n",
        " - Developed models with the performance metrics and feature importance properly reported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeADt57czpeB"
      },
      "source": [
        "**❗ 注意事項 / Highlighted Pitfall(s)**\n",
        "- リーケージ(Data Leakage)を起こさないようにしましょう\n",
        "- 適切でないモデル評価の指標(メトリクス)を用いないようにしましょう\n",
        "- 与えられた基本モデルと比べて性能が向上しないケースや学習が進んでいないケースに注意しましょう\n",
        "- あまりにも複雑なモデルは作成しないようにしましょう\n",
        "\n",
        "- Outcome leakage\n",
        "- Suboptimal metrics for model evaluation\n",
        "- No improvement compared to the presented baseline / Models not learning\n",
        "- Overly complex models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 準備 / Setup Environment"
      ],
      "metadata": {
        "id": "CUby9hdLkhY8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8of3shDt1WW-"
      },
      "source": [
        "作成するモデルに対して、あなたがより慣れている他のライブラリがある場合は追加しても構いません。\n",
        "\n",
        "You can add more libraries if you are familiar with them for your own model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKnijQhgy-os"
      },
      "outputs": [],
      "source": [
        "# If your datasets exist in Google Drive, you need to mount Google Drive to access files\n",
        "# If you upload your datasets directly to Google Colab, you don't need it.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "fLfLhYkUlLzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbnBPPKh7mnL"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXG-Ekog8m46"
      },
      "outputs": [],
      "source": [
        "!pip install yellowbrick"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for Japanese characters in matplotlib\n",
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "id": "SNPzaPuxylXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "aDGC2hIMlN9P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvY3nMXkEAmc"
      },
      "outputs": [],
      "source": [
        "# Data reading in Dataframe format and data preprocessing\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_columns\", 160)\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Dataset Creation\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Dataset Processing\n",
        "from sklearn import datasets, linear_model, metrics\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "\n",
        "# Model Development\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from yellowbrick.classifier import ClassificationReport, ClassPredictionError\n",
        "from yellowbrick.regressor import ResidualsPlot, PredictionError\n",
        "\n",
        "# Feature Importance\n",
        "import shap\n",
        "\n",
        "# Japanize Matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htgztOyb2Kt9"
      },
      "source": [
        "## 2. 前処理 / Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFlCvmfM4PKs"
      },
      "source": [
        "###### ✏️ work_dirを正しく修正し、データセットを取ってきましょう\n",
        "✏️ Set your path to the datasets, and fetch them"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set pathes\n",
        "train_csv = \"path_to_your_train.csv\"\n",
        "test_csv = \"path_to_your_test.csv\"\n",
        "\n",
        "# fetch dataset\n",
        "df_train = pd.read_csv(train_csv)\n",
        "df_test = pd.read_csv(test_csv)"
      ],
      "metadata": {
        "id": "AmDOk43T7wqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check dataset (Do not peep test data)\n",
        "df_train.head(2)"
      ],
      "metadata": {
        "id": "FBcj2VbAgPa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Variable Dictionary (GitHub Link)](https://github.com/TokyoDatathon2023/notebooks/blob/main/variable_dictionary.md): データフレームで使用されている変数の一覧と簡易的な説明"
      ],
      "metadata": {
        "id": "SafoHqgvsEln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 エンコード / Encoding\n",
        "\n",
        "Backgroundの機械学習モデルは私たちの持っているデータをモデル化し、そこからパターンを見つけます。この処理は数値的な値にしか適用できません。つまり、カテゴリ変数は数値的な値に変換しなくてはならないのです。\n",
        "\n",
        "カテゴリ変数には次の三種類が存在します。\n",
        "- バイナリ変数 (Binary Variables):\n",
        "> バイナリ変数は1と0の二つの値を表現することができる。例えば、対象が特定のグループに対して所属(1)、非所属(0)といったことを表現するのに用いられる。\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "- 順序変数 (Ordinal Variables):\n",
        "> 順序変数はある特定の順番を持っており、ラベルによるエンコード(Label Encoding)をすることによって数値的な値に表現しなおすことができる。例えば、High, Medium, Lowといった三つのデータは順序変数である。なぜなら、これらは決まった順序を持つので、それぞれ3, 2, 1という数値的な値に表現しなおすことができるからだ。\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "- 名義変数 (Nominal Variables):\n",
        "> 名義変数は決まった順序を持たない。従ってこの種の変数にはラベルによるエンコードが適していない。代わりに、ワンホットエンコードを行うのが良い。\n",
        "\n",
        "<br />\n",
        "\n",
        "Since the machine learning model in the background models and finds patterns in our data. It only supports numeric values. For this reason, categorical variables must be coded to numeric values.\n",
        "\n",
        "Categorical variables can be of 3 types:\n",
        "- Binary variables: Binary variables can be represented with two values, 1 and 0. Examples are whether or not the variable belongs to a group.\n",
        "- Ordinal variables: Ordinal variables are a type of variables that have a specific order and can be represented with numeric variables through a label encoder. An example is High, Medium, and Low which can be represented as 3, 2, 1.\n",
        "- Nominal variables: Nominal variables are categorical variables that do not have a defined order, for these variables it is not recommended to use a label encoder, it is better to use one hot encoder in these cases."
      ],
      "metadata": {
        "id": "O9AcEkVP9HNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ どのカテゴリ変数を持っているでしょうか？ / Which type of categorical variables we have?\n",
        "\n",
        "なお、必ずしも全ての種類に相当する変数がなくてはならないというわけではありません。\\\n",
        "Not necessarily must have variables of each type.\n",
        "\n",
        "> **ヒント / Hint**: \\\n",
        "> 次の`check_variable_kind`を使って調べてみましょう。\\\n",
        "> Use `check_variable_kind` to check the kinds of variables."
      ],
      "metadata": {
        "id": "668OwYL8Cai3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* バイナリ変数 / The binary variables are: \\\n",
        "(ここに当てはまる変数を書いてください / here)\n",
        "\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "* 順序変数 / The ordinal variables are: \\\n",
        "(ここに当てはまる変数を書いてください / here)\n",
        "\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "* 名義変数 / The Norminal variables are: \\\n",
        "(ここに当てはまる変数を書いてください / here)"
      ],
      "metadata": {
        "id": "fRcQKtFea48C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_variable_kind(df):\n",
        "    # check per column\n",
        "    for col in df.columns:\n",
        "        # show the name and the num of unique variables\n",
        "        print(f\"Column: {col} | {df[col].unique().size} unique values\")\n",
        "\n",
        "        # get unique values\n",
        "        unique_sample = df[col].unique().tolist()\n",
        "        print(\"[  \", end=\"\")\n",
        "        if len(unique_sample) < 10:\n",
        "            # when the num of kinds is less 10\n",
        "            for u in unique_sample: print(f\"{u}  \", end=\"\")\n",
        "        else:\n",
        "            # otherwise\n",
        "            for i in range(5): print(f\"{unique_sample[i]}  \", end=\"\")\n",
        "            print(\"\\n\\t... \", end=\"\")\n",
        "            for i in range(5): print(f\"{unique_sample[-(5-i)]}  \", end=\"\")\n",
        "        print(\"]\\n\")"
      ],
      "metadata": {
        "id": "6GUf6AtAE8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the kinds of variables\n",
        "check_variable_kind(df_train)"
      ],
      "metadata": {
        "id": "WqLaYHqxefxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ バイナリ変数のエンコード / Encode binary variables\n",
        "\n",
        "機械学習のモデルが理解できるのは数字だけです。従って、バイナリ変数は0, 1でエンコードしましょう。\\\n",
        "該当する変数列の値を0, 1で置き換えてください。\n",
        "\n",
        "Machine learning models only understand numbers, so binary variables must be encoded as 1s and 0s. Replace the values in those columns with 1s and 0s\n",
        "\n",
        ">**ヒント / Hint**: \\\n",
        "> `df.replace()`を使う\n",
        ">\n",
        "> you can use `df.replace()` to do that"
      ],
      "metadata": {
        "id": "VWmNY9SJ86Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when replacing A to B\n",
        "# df.replace(A, B)"
      ],
      "metadata": {
        "id": "GWGkwG60gnr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 順序変数のエンコード /  Encode ordinal variables\n",
        "\n",
        "`label_encoder ` を使って順序変数を数値にエンコードしましょう。\n",
        "\n",
        "You can use the function `label_encoder ` to encode the ordinal variables as numeric variables.\n",
        "\n",
        "> **Hint**:\n",
        "\n",
        "> **ヒント / Hint**: \\\n",
        "> 変数の中には一見すると数値へのエンコードが既に終わっているように思えるものがあるかもしれません。しかしそういった変数の中には欠損値の補完として`None`が紛れ込んでいる恐れがあります。この`None`を数値にエンコードするには`df.replace`を用いましょう。\n",
        ">\n",
        ">Some variables are already numerical, but have 'None' due to the imputation. You can use `df.replace` to convert that 'None' to numerical"
      ],
      "metadata": {
        "id": "L_VJsNIX-D1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(df_train, cols, df_test=None):\n",
        "  \"\"\"\n",
        "  Scikit-learn の LabelEncoder を用いて Pandas の DataFrame に含まれているカテゴリ変数(順序変数)をエンコードする関数.\n",
        "\n",
        "  引数:\n",
        "  df_train - Pandas DataFrame. エンコードするDataFrame.\n",
        "  cols - list. エンコードされるカラム名のリスト.\n",
        "  df_test (オプショナル) - Pandas DataFrame. 同時にエンコードするDataFrame.\n",
        "\n",
        "  返り値:\n",
        "  以下を含むタプル.\n",
        "  - エンコードされた df_train\n",
        "  - 使用した LabelEncoders の辞書\n",
        "  - エンコードされた df_test (オプショナル)\n",
        "  \"\"\"\n",
        "  encoders = {}\n",
        "  for col in cols:\n",
        "    encoders[col] = LabelEncoder()\n",
        "    df_train[col] = encoders[col].fit_transform(df_train[col])\n",
        "    if df_test is not None:\n",
        "      df_test[col] = encoders[col].transform(df_test[col])\n",
        "\n",
        "  if df_test is not None:\n",
        "    return df_train, df_test, encoders\n",
        "  else:\n",
        "    return df_train, encoders"
      ],
      "metadata": {
        "id": "ZnENxfbLXGST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This is not the case, but just to show how to use the function:\n",
        "# train, test, encoders = label_encoder(train, ['insurance'], df_test=test)"
      ],
      "metadata": {
        "id": "Fa28bOmRZycF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 名義変数のエンコード / Encode nominal variables\n",
        "\n",
        "以下の`column_to_one_hot`を用いて名義変数を[ワンホット(one-hot)表記](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)にエンコードしましょう。\n",
        "\n",
        "You can use the function `column_to_one_hot` to encode the nominal variables to a [one-hot representation](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)."
      ],
      "metadata": {
        "id": "aTghaURaB7yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def column_to_one_hot(df_train, column, df_test=None):\n",
        "  # Generate a one-hot representation of the values in the column\n",
        "  train_one_hot = pd.get_dummies(df_train[column], dtype=int, drop_first=True)\n",
        "  # add the one-hot encoded columns to the DataFrame\n",
        "  df_train = pd.concat([df_train, train_one_hot], axis=1)\n",
        "  # drop the original column\n",
        "  df_train = df_train.drop(column, axis=1)\n",
        "\n",
        "  if df_test is not None:\n",
        "    test_one_hot = pd.get_dummies(df_test[column], dtype=int, drop_first=True)\n",
        "\n",
        "    # Add missing columns in test data\n",
        "    missing_cols = set(train_one_hot.columns) - set(test_one_hot.columns)\n",
        "    for c in missing_cols:\n",
        "      test_one_hot[c] = 0\n",
        "\n",
        "    # Ensure the order of column in the test set is in the same order than in train set\n",
        "    test_one_hot = test_one_hot[train_one_hot.columns]\n",
        "    df_test = pd.concat([df_test, test_one_hot], axis=1)\n",
        "    df_test = df_test.drop(column, axis=1)\n",
        "    return df_train, df_test\n",
        "  else:\n",
        "    return df_train"
      ],
      "metadata": {
        "id": "61UE_CRi-cfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E.g. Convert 'insurance' column into one-hot encoding:\n",
        "# train, test = column_to_one_hot(df_train, 'insurance', df_test)"
      ],
      "metadata": {
        "id": "j-Kl9LE_95Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 データの正規化 / Data Normalization\n",
        "\n",
        "多くの機械学習モデルには、モデルに入力する特徴量のスケールが大きく影響してきます。 \\\n",
        "そのため、パイプラインに用いる値の大きさを正規化することが大切です。\n",
        "\n",
        "最も一般的な手法の一つはデータの標準化ですが、対象となるユースケースやモデルによっては不適当である可能性があり、考慮して手法を決定することが必要です。<[詳細はこちら](https://scikit-learn.org/stable/modules/preprocessing.html)>\n",
        "\n",
        "データセットには幾つかのタイプの変数が含まれていることがあり、それを考慮することが重要です。\n",
        "\n",
        "> * カテゴリー変数 (例: [花の種類] ひまわり, たんぽぽ, チューリップ) \\\n",
        "> * 連続型の変数 (例: [身長] 185cm, 160cm, 172cm) \\\n",
        "> * バイナリ変数 (例: 0, 1)\n",
        "\n",
        "例えば、カテゴリ変数とバイナリ変数についてデータの標準化を行うことは往々にして得策ではないでしょう。\\\n",
        "このように、異なるタイプの変数を区別し、それぞれの変数について適当な正規化方法を適用することが重要です。\n",
        "\n",
        "<br/>\n",
        "\n",
        "Many machine learning models are directly influenced by the scale of the features that you input to the model. Therefore, it is important to normalize the scale of values used in your pipeline.\n",
        "\n",
        "One of the most common methods used is data standardization, but the decision should be taken considering the specific use case or model that you are developing. If you are curious, you can further read about this topic [here](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "\n",
        "It is important to consider that in a dataset one might have several types of variables: categorical, continuous, binary. For both categorical and binary variables, it is not a good practice to apply standardization methods. Therefore, it is important to distinguish the different type of variables that one might have and apply normalization methods only to those that make sense."
      ],
      "metadata": {
        "id": "5BD44-g5zsmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ 正規化を行う前に、予測する項目を X_train と X_test から取り除き y_train や y_test に格納しましょう<br>\n",
        "Split your labels from the remaining dataset"
      ],
      "metadata": {
        "id": "BQpaXABFNtsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels we are going to predict\n",
        "label_cols=['hidden_hypoxemia', 'SaO2']\n",
        "id_col = [\"subject_id\", \"stay_id\"]\n",
        "\n",
        "# train dataset\n",
        "y_train = df_train[label_cols]\n",
        "X_train = df_train.drop(columns=label_cols + id_col)\n",
        "\n",
        "# test dataset\n",
        "y_test = df_test[label_cols]\n",
        "X_test = df_test.drop(columns=label_cols + id_col)"
      ],
      "metadata": {
        "id": "grZV8zKxNyDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check train data\n",
        "y_train.head(2)"
      ],
      "metadata": {
        "id": "x-k1z_YvN24J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(2)"
      ],
      "metadata": {
        "id": "NGrhXzCIN4DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch X_train for binary variables\n",
        "binary_variables = [feat for feat in X_train.columns if X_train[feat].unique().size == 2]"
      ],
      "metadata": {
        "id": "S2UttMiAjWU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check fetched binary variables\n",
        "binary_variables"
      ],
      "metadata": {
        "id": "mtzfguf5N-b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split variables into continuous ones and binary or categorical ones\n",
        "X_train_continuous = X_train.drop(columns=binary_variables)\n",
        "X_test_continuous = X_test.drop(columns=binary_variables)\n",
        "X_train_binary = X_train[binary_variables]\n",
        "X_test_binary = X_test[binary_variables]"
      ],
      "metadata": {
        "id": "4DY6JMqROAjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit and transform the train continuous variables\n",
        "X_train_preprocessed = scaler.fit_transform(X_train_continuous)\n",
        "\n",
        "# check preprocessed train continuous variables (Do not peep test ones)\n",
        "pd.DataFrame(X_train_preprocessed, columns=X_train_continuous.columns)"
      ],
      "metadata": {
        "id": "DZWtnNbaOBLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessor to train continuous variables (Do not peep the preprocessed ones)\n",
        "X_test_preprocessed = scaler.transform(X_test_continuous)"
      ],
      "metadata": {
        "id": "JzmVSRcjODBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前処理(標準化)を行った連続型の変数とバイナリ変数を結合し、モデルに与えます。\n",
        "\n",
        "<br/>\n",
        "\n",
        "Join the processed continuous variables with the binary ones to then feed to the model."
      ],
      "metadata": {
        "id": "k5xoYEfzOI0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join\n",
        "X_train_processed = pd.concat([df_train[\"subject_id\"], X_train_binary, pd.DataFrame(X_train_preprocessed, columns=X_train_continuous.columns)], axis=1, join='inner')\n",
        "X_test_processed = pd.concat([df_test[\"subject_id\"], X_test_binary, pd.DataFrame(X_test_preprocessed, columns=X_test_continuous.columns)], axis=1, join='inner')"
      ],
      "metadata": {
        "id": "hEEd9ef5OH50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0puz3tPOPBe"
      },
      "source": [
        "注: データに処理を加えた場合、処理後の分散をしっかりと調べ、適用した前処理が意味を持ち、期待した効果をもたらしているかどうかを確認することが重要である。\n",
        "\n",
        "<br/>\n",
        "\n",
        "Note: After transforming your data, it is important to further explore it's distribution and ensure that the transformations applied make sense and resulted on what was expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz4QaWjiOPBo"
      },
      "source": [
        "###### ✏️ データの正規化方針を自身で考え、実装してみよう /  Implement your own data normalization strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoY3d0rNOPBo"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "\n",
        "\n",
        "# Check your normalization, e.g. summary statistics, histograms, scatter plots, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 機械学習パイプラインの実装 / Implement ML Pipeline"
      ],
      "metadata": {
        "id": "iNU51V7mlX9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以降の処理では、パイプラインを使用してデータの前処理からモデルの構築まで一貫して実行可能な実装を試みましょう。訓練データ、テストデータを再度読み込むようにしてください。\n",
        "\n",
        "In subsequent processes, try to use a pipeline to implement a consistent implementation that can run from data preprocessing to model building. Be sure to reload the training and test data."
      ],
      "metadata": {
        "id": "CsXIo8hYtJOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset again to avoid duplicative encoding\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "EUruuUe0VgRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHghxbBL45fy"
      },
      "source": [
        "### 回帰(regression)と分類(classification)に対応したラベル分け / Get label vectors for both possibilities: regression and classification\n",
        "\n",
        "回帰を行いたいのか、分類を行いたいのかによって、学習の際に使用する正解ラベルは異なります。\n",
        "回帰の場合は連続変数である$SaO_{2}$を、分類の場合は潜在性低酸素血症のクラスを定義する必要があります。\n",
        "\n",
        "<br/>\n",
        "If you want to use regression or classification, the labels used to train the model will be different. In the case of regression, you will have a numerical variables with SaO2 values, whereas in classification you need to define classes for hidden hypoxemia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUbgwdW11oXo"
      },
      "outputs": [],
      "source": [
        "# for regression\n",
        "y_train_r = df_train[['SaO2']].values\n",
        "y_test_r = df_test[['SaO2']].values\n",
        "\n",
        "# check y_train_r (Do not peep y_test_r)\n",
        "print(f\"Size: {len(y_train_r)} * {len(y_train_r[0])}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i}: {y_train_r[i]}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for classification\n",
        "y_train_c = df_train[['hidden_hypoxemia']].values\n",
        "y_test_c = df_test[['hidden_hypoxemia']].values\n",
        "\n",
        "# check y_train_c (Do not peep y_test_c)\n",
        "print(f\"Size: {len(y_train_c)} * {len(y_train_c[0])}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i}: {y_train_c[i]}\")\n",
        "print(\"...\")"
      ],
      "metadata": {
        "id": "qaE4PaOuix6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjypUQTM6Mth"
      },
      "source": [
        "###### ✏️ 実装する機械学習手法に沿って、他に必要なラベルを定義する / Define label vectors for chosen ML approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ5J-VKFJc9a"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p023Q0o56b6R"
      },
      "source": [
        "### 3.1 ナイーブな手法で予測を行うモデル実装 / Naive Model Implementation\n",
        "\n",
        "二つのクラス(潜在性低酸素血症であるか否か)について、訓練データの中で多い方をそのまま予測結果として学習させた場合、モデルの精度はどうなるでしょうか。\n",
        "\n",
        "この場合、潜在性低酸素血症でないデータの方が多ければ予測ベクトルの値は全て0であり、潜在性低酸素血症であるデータの方が多ければ予測ベクトルの値は全て1となります。\n",
        "\n",
        "このような、全ての患者について最も可能性の高いクラスだと予想するモデル(以下 ナイーブモデル)を訓練データセットを用いて作成してみましょう。\n",
        "\n",
        "<br/>\n",
        "What would our accuracy be if we predicted the most likely class? In this case, our prediction would simply be 1 or 0 for every patient. Using our training dataset we can create a naive model that predicts the most likely class for every patient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BN5M8MIcLJj"
      },
      "source": [
        "###### ✏️ ナイーブな手法をテストして、その結果をパフォーマンスのベースラインとしよう / Test a naive approach to have a performance baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニングセットのクラスラベル0と1の数を比較して、どちらが多いかを判定する\n",
        "if np.sum(y_train_c==0) > np.sum(y_train_c==1):\n",
        "    # クラスラベル0が多い場合、常にクラスラベル0を予測する\n",
        "    y_preds_cc = [0 for _ in range(len(y_test_c))]\n",
        "else:\n",
        "    # クラスラベル1が多い場合、常にクラスラベル1を予測する\n",
        "    y_preds_cc = [1 for _ in range(len(y_test_c))]"
      ],
      "metadata": {
        "id": "Dya-POISmqLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52rodxtie_0q"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "# Check accuracy score\n",
        "accuracy_score = np.mean(y_preds_cc == y_test_c)\n",
        "print(\"Accuracy: {:.6f}\".format(accuracy_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBtxDigXKf5o"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "# Check F1 score\n",
        "f1 = f1_score(y_test_c, y_preds_cc)\n",
        "print(\"F1 Score for Naive Model: {:.6f}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 回帰モデルを作ってみよう\n",
        "\n",
        "ここでは、訓練データを使用して、与えられた特徴量から$SaO_{2}$を予測する回帰モデルを作成し、テストデータの$SaO_{2}$を予測してみます。"
      ],
      "metadata": {
        "id": "mJXn-JRyMvbb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMXUBwfmO7Fb"
      },
      "source": [
        "#### 3.2.1 線形回帰のベースライン / Linear Regression Baseline\n",
        "\n",
        "最も簡単な機械学習モデルの一つが**線形回帰モデル**です。 \\\n",
        "これは、ターゲットとなる変数が特徴量の線形和で表されることを前提としたモデルです。\n",
        "\n",
        "この手法は、多くの問題においてかなり強力なベースラインです。 \\\n",
        "手法が単純であるおかげで、特徴数の数が多い場合でも、非常に早くフィッティングを行うことができます。\n",
        "\n",
        "一般的に、特徴量、前処理、そして推定値のフィッティングをシングルパイプラインでまとめて行った方が、データの変換、フィッティング、予測がより容易となります。\n",
        "\n",
        "さらに、訓練データに対して fit 関数を呼び出すことにより、モデルを学習させることができます。ここでは、 pipeline を使用して、データの正規化から一貫してモデルの学習までを実行してみましょう。\n",
        "\n",
        "<br/>\n",
        "\n",
        "On of the easiest Machine Learning model is **linear regression**. It assumes that the target variable can be written as a linear combination of the features.\n",
        "\n",
        "In many problems, this solution can be a very strong baseline. Thanks to its simplicity, it can be fitted very quickly even when the number of features is high.\n",
        "\n",
        "In general, wrapping together the features, preprocessing and fitting the estimator in a single pipeline makes it easier to transform, fit and predict the data.\n",
        "\n",
        "We train the model by calling the fit function on the train data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "# 前処理のためのパイプラインを作成する\n",
        "binary_vars =\n",
        "categorical_vars =\n",
        "ordinal_vars =\n",
        "continuous_vars =\n",
        "\n",
        "X_train = df_train.drop(columns=['SaO2', 'subject_id', 'stay_id', 'hidden_hypoxemia', ...])\n",
        "X_test = df_test.drop(columns=['SaO2', 'subject_id', 'stay_id', 'hidden_hypoxemia', ...])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('bin', 'passthrough', binary_vars),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_vars),\n",
        "        ('ord', OrdinalEncoder(), ordinal_vars),\n",
        "        ('num', StandardScaler(), continuous_vars)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 前処理と線形回帰を組み合わせたパイプライン\n",
        "linear_pipeline_regression = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# モデルの訓練\n",
        "linear_pipeline_regression.fit(X_train, y_train_r.ravel())"
      ],
      "metadata": {
        "id": "0Ob2deLVO_9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 線形回帰モデルの解釈 / Interpretation:"
      ],
      "metadata": {
        "id": "5SSBewZ3PKcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "線形モデルは特徴量一つにつき一つの重みを与えるため、係数を可視化することによって、線形モデルが私たちのターゲットとなる共変量をどのようにモデル化したのかを簡単に調べることができます。\n",
        "\n",
        "<br/>\n",
        "\n",
        "Because the linear model gives one weight to each feature, we can easily explore how it modeled our target covariate by visualizing the coefficients."
      ],
      "metadata": {
        "id": "8U6v5ht-PKcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('線形(回帰)モデルの係数 / Examine regression coefficients:')\n",
        "feature_names = linear_pipeline_regression.named_steps['preprocessor'].get_feature_names_out()\n",
        "linear_coefficients = pd.DataFrame(\n",
        "    linear_pipeline_regression.named_steps['regressor'].coef_,\n",
        "    index=feature_names,\n",
        "    columns=['coefficient']\n",
        ")\n",
        "\n",
        "# 係数の絶対値に基づいて並び替え\n",
        "linear_coefficients = linear_coefficients.reindex(linear_coefficients.coefficient.abs().sort_values(ascending=False).index).T\n",
        "\n",
        "linear_coefficients\n"
      ],
      "metadata": {
        "id": "YH6lzZiqPNLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "linear_coefficients.plot(kind='bar', ax=ax)\n",
        "\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Coefficient')\n",
        "plt.title('Linear Regression Coefficients')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3qFogmRQURUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ 確認した係数の中で不思議に感じたものはありませんでしたか？ / Don't you see anything strange with some of these coefficients ?\n",
        "\n",
        "<br/>\n",
        "\n",
        "例えば、ほとんど0に近い(寄与しない)係数があったり、直感にそぐわない符号の係数があったり...\n",
        "\n",
        "それらの変な係数は、複数の特徴量(説明変数)が互いに高い相関(線形関係)を持ってしまうことで生じる、多重共線性(multicollinearity)という現象が原因です。\n",
        "\n",
        "このように線形モデルに対して冗長な情報(特徴量)を与えてしまうと、不正確でノイズの混じった係数を予測するようになってしまいます。\n",
        "\n",
        "<参考: [線形モデルの限界について](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_regularization.html)>\n",
        "\n",
        "\n",
        "この問題に対するシンプルな解決方法は、モデルに対して極端な係数を取らせないように強制する(正則化)ことです。\n",
        "\n",
        "次のモデルではリッジ回帰と呼ばれる正則化手法を採用しています。\n",
        "> **リッジ(Ridge)回帰** \\\n",
        "> 学習中に用いられる損失(\\*)に係数の二乗に比例する項を追加することによって、モデルが絶対値の大きな係数を採用しなくなることを狙った手法 \\\n",
        "> (\\*損失: モデルの性能を図る指標の一つ。この値が大きいほど予測値が本来の値から離れている。)\n",
        "\n",
        "<br/>\n",
        "\n",
        "These weird coefficients are caused by multiple features measuring almost the same thing --a phenoma called multi-colinearity. Giving redundant information to a linear model, makes it predict unprecise and noisy coefficients.\n",
        "You can learn more about [the limitations of the linear model here](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_regularization.html).\n",
        "\n",
        "A simple solution is to force the model to avoid extreme coefficients, by adding a *regularization*. The subsequent model is called a Ridge regression."
      ],
      "metadata": {
        "id": "K7ULunMCPc9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2 Ridge 回帰"
      ],
      "metadata": {
        "id": "qMu6GfBDPocu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgaVu3TePoc9"
      },
      "source": [
        "リッジ回帰を用いて算出した回帰係数を見てみましょう。\\\n",
        "先ほどの物に比べて筋が通っているように見えると思います。\n",
        "\n",
        "<br/>\n",
        "\n",
        "Examine the regression coefficients for the Ridge estimator. It looks much more reasonable.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "# 前処理とRidge回帰を組み合わせたパイプライン\n",
        "ridge_pipeline_regression = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', Ridge(alpha=10))\n",
        "])\n",
        "\n",
        "# モデルの訓練\n",
        "ridge_pipeline_regression.fit(X_train, y_train_r.ravel())\n",
        "\n",
        "# Coefficientsの取得\n",
        "ridge_coefficients = pd.DataFrame(\n",
        "    ridge_pipeline_regression.named_steps['regressor'].coef_.T,  # transpose the coefficients\n",
        "    index=feature_names,\n",
        "    columns=['coefficient']\n",
        ")\n",
        "\n",
        "ridge_coefficients = ridge_coefficients.reindex(ridge_coefficients['coefficient'].abs().sort_values(ascending=False).index)\n",
        "\n",
        "print('リッジ回帰を用いて算出した回帰係数 / Coefficients: ')\n",
        "ridge_coefficients.T"
      ],
      "metadata": {
        "id": "mooX53ksPmfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ridge_coefficients.plot(kind='bar', ax=ax)\n",
        "\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Coefficient')\n",
        "plt.title('Ridge Regression Coefficients')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W8tYba_OUZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CahjHSojP2z5"
      },
      "source": [
        "では、続いて[残差誤差](https:/www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/residuals.html#:~:text=Definition,yi%E2%88%92%5Eyi)というモデルの誤差を調べてみましょう。\n",
        "\n",
        "<br/>\n",
        "\n",
        "We also can explore the errors of the model --called [residuals](https:/www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/residuals.html#:~:text=Definition,yi%E2%88%92%5Eyi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ots8fG9aP20P"
      },
      "outputs": [],
      "source": [
        "# Code here!\n",
        "\n",
        "# Create residual error plot for both training and test data\n",
        "plt.scatter(ridge_pipeline_regression.predict(X_train), ridge_pipeline_regression.predict(X_train) - y_train_r.ravel(), color = \"green\", s = 10, label = 'Train data')\n",
        "plt.scatter(ridge_pipeline_regression.predict(X_test), ridge_pipeline_regression.predict(X_test) - y_test_r.ravel(), color = \"red\", s = 10, label = 'Test data')\n",
        "\n",
        "# Draw a horizontal line at y = 0\n",
        "plt.hlines(y = 0, xmin = ridge_pipeline_regression.predict(X_train).min(), xmax = ridge_pipeline_regression.predict(X_test).max(), linewidth = 2, color=\"black\", linestyle=\"dotted\")\n",
        "\n",
        "# Add legend outside the plot\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=10)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"残差誤差\")\n",
        "plt.xlabel(r\"予測された SaO2 値: $\\hat y$\")\n",
        "plt.ylabel(r\"予測における誤差: $\\hat y  - y$\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdVknslBQBqw"
      },
      "source": [
        "#### 3.2.3 回帰モデルの評価指標\n",
        "\n",
        "回帰モデルの評価にはしばしば [$R^2$ score](https://en.wikipedia.org/wiki/Coefficient_of_determination) が用いられます。$R^2$ score の取りうる最大値は 1.0 であり、モデルが悪い場合には負になることもあります。真値(真のy)が一意に定まらないケースでは、定数モデルの$R^2$ score は 0.0 となりますが、これは定数モデルが入力特徴を無視して常に平均 y を予測値とするからです。\n",
        "\n",
        "<br/>\n",
        "Regression tasks are often evaluated using the [$R^2$ score](https://en.wikipedia.org/wiki/Coefficient_of_determination). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). In the general case when the true y is non-constant, a constant model that always predicts the average y disregarding the input features would get a $R^2$ score of 0.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msEj0dBCQBq7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the linear model with R2 :\n",
        "y_pred_train_linear = linear_pipeline_regression.predict(X_train)\n",
        "linear_train_r2 = r2_score(y_train_r, y_pred_train_linear)\n",
        "print(f'線形モデルの R2 score: {linear_train_r2}')\n",
        "\n",
        "# Evaluate the ridge model with R2 :\n",
        "y_pred_train_ridge = ridge_pipeline_regression.predict(X_train)\n",
        "ridge_train_r2 = r2_score(y_train_r, y_pred_train_ridge)\n",
        "print(f'リッジ回帰モデルの R2 score: {ridge_train_r2}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer = ResidualsPlot(ridge_pipeline_regression)\n",
        "visualizer.fit(X_train, y_train_r.ravel())\n",
        "visualizer.score(X_test, y_test_r.ravel())\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "hlQCoIB1Ugiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_pipeline_regression.fit(X_train, y_train_r.ravel())\n",
        "hat_y_test = ridge_pipeline_regression.predict(X_test)\n",
        "\n",
        "fig, ax = plt.subplots(1)\n",
        "sns.regplot(data=None, x=y_test_r, y=hat_y_test, line_kws={\"color\":\"black\", \"linestyle\":\"dotted\"})\n",
        "plt.plot()\n",
        "ax.set(xlabel=r\"$y$\", ylabel=r\"$\\hat y$\")\n",
        "test_r2_score_ = r2_score(y_test_r, hat_y_test)\n",
        "print(f\"R2 score: {test_r2_score_}\")\n"
      ],
      "metadata": {
        "id": "12ZvR7VEUi0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z5eeQQOQBq8"
      },
      "source": [
        "✏️ 他にも様々な性能評価指数が[sklearnでは実装されています](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)。今回のタスクについて、より適当だと思われる性能評価指数があれば試してみましょう。\\\n",
        "Different regression metrics are [implemented by sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics). Try another metric that you think is relevant to this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74mP33muQBq8"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ※モデルの評価について / Model Evaluation\n",
        "\n",
        "SpO2値の補正用の機械学習モデルを開発し、臨床に導入するためにはモデルの性能を正しく評価することが必須です。\n",
        "\n",
        "モデルが使われる臨床の環境だけでなく、学習に使用したデータセットのことも考慮して性能評価のための指標(メトリクス: Metrics)を慎重に選択する必要があります。\n",
        "\n",
        "> 考慮すべき点の例\n",
        "- 学習に用いられた名義変数に含まれていない名義変数を持つ患者に対してはどのような性能を示すだろうか\n",
        "- 学習データに含まれる変数の中で、特定の範囲に偏ったものはないだろうか\n",
        "\n",
        "<br/>\n",
        "\n",
        "To be able to develop an ML model for the recalibration of SpO2 levels and implement it in a clinical setting, it is crucial to properly evaluate its performance in the task that is supposed to do.\n",
        "\n",
        "A set of performance metrics should be carefully chosen, considering the clinical setting where the model will be aplied in but also the dataset where is what trained on:\n",
        "- If the dataset contains mostly one racial group, how will it perform on others patients?\n",
        "- Does the dataset have patients from a wide range of ages or is it more focused on a narrow range?\n"
      ],
      "metadata": {
        "id": "qKvH-pPPOjNM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthIUsyqQBq9"
      },
      "source": [
        "先の線形モデルやリッジ回帰線形モデルの $R^2$ score はあまり満足のいくものではありませんでした。これについては以下の二つの説明が考えられます。\n",
        "1. 目的変数に対して十分な説明変数を持っていない\n",
        "2. 目的変数が線形の関係にない\n",
        "\n",
        "✏️ $R^2$ score で評価する場合、よりよい回帰モデルを挙げることができますか？例えば「[決定木](https://scikit-learn.org/stable/modules/tree.html#regression)」や「[アンサンブル モデル](https://scikit-learn.org/stable/modules/ensemble.html)」はどうでしょうか。もちろん、他のタイプの線形アルゴリズムについても自由に試してみてください。\n",
        "\n",
        "<br/>\n",
        "\n",
        "We saw that the regression metrics of both the linear regression and the regularized linear ridge regressions are not very satisfying. There are two explanations: either, we have not the proper variables to explain the output variables or these variables do not relate to the SaO2 with a pure linear relationship.\n",
        "\n",
        "✏️  Can you fit and evaluate a better regression model in term of $R^2$ ? Consider for example [decision trees](https://scikit-learn.org/stable/modules/tree.html#regression) or [ensemble models](https://scikit-learn.org/stable/modules/ensemble.html). But feel free to experiment with any other type of regression algorithms !"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n"
      ],
      "metadata": {
        "id": "fNEsElG5nLsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 分類モデルを作ってみよう / Create classification models\n",
        "\n",
        "ここでは、訓練データを使用して、与えられた特徴量から `hidden_hypoxemia` を予測する分類モデルを作成し、テストデータの `hidden_hypoxemia` を予測してみます。\n",
        "\n",
        "Here, we use the training data to create a classification model that predicts `hidden_hypoxemia` from the given features and try to predict `hidden_hypoxemia` in the test data."
      ],
      "metadata": {
        "id": "hp-lgVpXDmYC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8EaYDUnQqIm"
      },
      "source": [
        "#### 3.3.1 SVM分類のベースライン / SVM Classification Baseline\n",
        "\n",
        "世の中には分類のタスクに用いられるモデルが多くあります。\\\n",
        "私たちはその中からタスクに応じて最適なものを選択する必要があります。\\\n",
        "ここではそのようなモデルの一つ、サポートベクタマシーン(SVM)の実装を引き合いに出します。\n",
        "\n",
        "モデルを訓練するに先んじて、ロバストで正確なモデルを構築するうえで非常に重要となるパラメータ、ハイパパラメータを設定する必要があります。\n",
        "\n",
        "ハイパパラメータ(\\*)はバイアスと共分散の均衡点を見つけるのに役立つだけでなく、モデルの過学習(Overfitting)や学習不足(Underfitting)を防止することにも役立ちます。\\\n",
        "<[詳細はこちら](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)>\n",
        "\n",
        "> \\*ハイパーパラメーター: \\\n",
        "> ハイパーパラメーターは、モデルの訓練中にデータから学習されないパラメーターであり、訓練プロセスの前に設定されます。学習アルゴリズムの構造や動作を決定します。ハイパーパラメーターの調整は、学習アルゴリズムのパフォーマンスに大きく影響するため、ハイパーパラメーターのチューニングは機械学習モデルを構築する際の重要なステップとなります。\n",
        "\n",
        "<br/>\n",
        "\n",
        "For the classification task, you might choose from a wide range of models that you think are most suitable. Here you have the example of the implementation of a Support Vector Machine (SVM).\n",
        "\n",
        "Before training the model, we need to set a given number of parameters - i.e. hyperparameters - which will be critical in building robust and accurate models. They help us find the balance between bias and variance and thus, prevent the model from overfitting or underfitting.Keep in mind that if you increase the range of hyperparameters to be tested, the training time will increase significantly. If you want more information read [here](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理とSVCを組み合わせたパイプライン\n",
        "svc_pipeline_classifier = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(probability=True))\n",
        "])\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "param_grid = {\n",
        "    'classifier__C': ,\n",
        "    'classifier__gamma': ,\n",
        "    'classifier__kernel':\n",
        "}\n",
        "\n",
        "# グリッドサーチの設定\n",
        "svc_grid_search = GridSearchCV(\n",
        "    svc_pipeline_classifier,\n",
        "    param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    refit = True,\n",
        "    verbose = 3,\n",
        ")\n",
        "\n",
        "# モデルの訓練\n",
        "svc_grid_search.fit(X_train, y_train_c.ravel())\n",
        "\n",
        "# 予測\n",
        "svc_predictions = svc_grid_search.predict(X_test)\n",
        "\n",
        "# スコアの表示\n",
        "print('SVC Score: {}'.format(svc_grid_search.score(X_test, y_test_c.ravel())))"
      ],
      "metadata": {
        "id": "c9pYdNVGrzBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tocS4IDsQqIn"
      },
      "source": [
        "ご覧の通り、スコアはほぼ100%です！問題ないでしょうか？\n",
        "きちんと分類できているか、混同行列(Confusion Matrix)を見てみましょう。\n",
        "\n",
        "As you can observe, the scoring is almost 100%! Great, right? \\\n",
        "Well, let's take a closer look at the confusion matrix for a deeper analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUqabzpIQqIn"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix(y_test_c, svc_predictions))\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check balance\n",
        "num_of_hidden_hypoxemia = (df_train['hidden_hypoxemia']==1).sum()\n",
        "num_of_normal= (df_train['hidden_hypoxemia']==0).sum()\n",
        "print(f\"{num_of_hidden_hypoxemia} measurements with hidden hypoxia / {num_of_normal} normal ones\")"
      ],
      "metadata": {
        "id": "nA-bhHTBQqIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2dBKUHQqIn"
      },
      "source": [
        "11,641件の正常な測定値(= 潜在性低酸素血症でない)のデータに対し、190件しか潜在性低酸素血症であるデータはありません（※この件数はあなたのチームのデータセットによって変わります）。これは非常に不均衡なデータセットと言えるでしょう。では、どのように対処しましょう？\n",
        "\n",
        "<br />\n",
        "\n",
        "このような不均衡なデータセットに対してはいくつかの対処法が存在します。\n",
        "\n",
        "*   多数派のデータ数を少数派のデータ数に合わせる\n",
        "> この手法は単純ですが、多くの貴重なデータを損失することに繋がります。\n",
        "\n",
        "<br />\n",
        "\n",
        "*   少数派のデータに対してSMOTEアルゴリズムなどを用いてアップサンプリングを行い、合成データを作成する\n",
        "> この手法では、少数派クラスの各変数についてその分散を単に維持してしまうという問題があり、最も良い方法とは言えないかもしれません。\n",
        "\n",
        "<br />\n",
        "\n",
        "* 不均衡データに適応したアルゴリズムを使う。\n",
        "> 例えば [Imbalanced Learn](https://imbalanced-learn.org/stable/) という sklearn とよく似たパッケージがあります。これはSVMを含むほとんどの sklearn のアルゴリズムの class_weight パラメータに不均衡クラスが追加されています。\n",
        "\n",
        "不均衡データについてより詳しく知りたい方は[こちら](https://medium.com/eni-digitalks/imbalanced-data-an-extensive-guide-on-how-to-deal-with-imbalanced-classification-problems-6c8df0bc2cab)からご覧ください。\n",
        "\n",
        "<br/>\n",
        "\n",
        "It looks like we have a really imbalanced dataset, where we have only 190 measurements with hidden hypoxia compared to 11,641 normal ones (the number of records may vary according to your team's dataset). What should we do then?\n",
        "\n",
        "One can take several approaches when dealing with imbalanced datasets:\n",
        "- removing the number of datapoints for the majority class to match the number on the minority class. However, this might lead you to loose a lot of information.\n",
        "- upsample the minority class and generate synthetic data on it, using for example the SMOTE algorithm. This approach has the problem of maintaining the distribution of each variable for that class and might not provide the best results.\n",
        "- another approach might be to use an algorithm approppriate for this type of data. For that, there is a package very similar to sklearn called [Imbalanced Learn](https://imbalanced-learn.org/stable/) and add the class imbalance to the class_weight parameter in most sklearn algorithms (including in [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)).\n",
        "\n",
        "You can read more about it [here](https://medium.com/eni-digitalks/imbalanced-data-an-extensive-guide-on-how-to-deal-with-imbalanced-classification-problems-6c8df0bc2cab)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2 Random Forest Classifier / ランダムフォレスト分類器"
      ],
      "metadata": {
        "id": "TUAyzs0pvtf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "続いて、 Random Forest Classifier (RFC: ランダムフォレスト分類器) を用いたモデルを作成してみましょう。<br>\n",
        "RFC は、複数の決定木を学習し、それらの結果の多数決や平均によって最終的な予測を行う、アンサンブル学習の手法一つです。<br>\n",
        "RFC については[こちら](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)に詳細な説明があります。\n",
        "\n",
        "Next, let's create a model using the Random Forest Classifier (RFC). The RFC is a type of ensemble learning method that makes a final prediction based on the majority vote or average of the results obtained from multiple decision trees. For a detailed explanation of RFC, please refer to [this link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ],
      "metadata": {
        "id": "rRbmlh5OxqzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# 前処理とRandomForestClassifierを組み合わせたパイプライン\n",
        "rfc_pipeline_classifier = Pipeline(\n",
        "    [\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('classifier', RandomForestClassifier(class_weight=\"balanced\"))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# パラメータの設定\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': ,  # ツリーの数\n",
        "    'classifier__max_depth': ,  # ツリーの最大深度\n",
        "    'classifier__min_samples_split': ,  # ノードを分割するために必要な最小サンプル数\n",
        "    'classifier__min_samples_leaf': ,  # 葉ノードに必要な最小サンプル数\n",
        "}\n",
        "\n",
        "# グリッドサーチの設定\n",
        "rfc_grid_search = GridSearchCV(\n",
        "    rfc_pipeline_classifier,\n",
        "    param_distributions=param_dist,\n",
        "    refit=True,\n",
        "    verbose=3,\n",
        "    n_iter=5,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# モデルの訓練\n",
        "rfc_grid_search.fit(X_train, y_train_c.ravel())\n",
        "\n",
        "# 予測\n",
        "rfc_predictions = rfc_grid_search.predict(X_test)\n",
        "\n",
        "# スコアの表示\n",
        "print('RFC Score: {}'.format(rfc_grid_search.score(X_test, y_test_c.ravel())))\n"
      ],
      "metadata": {
        "id": "U8wKv4NSv0iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(confusion_matrix(y_test_c, rfc_predictions))\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nRrzz4PGbESH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kz9ajY8QqIo"
      },
      "source": [
        "#### 3.3.3 分類モデルの評価指標"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分類モデルについても評価指標を見てみましょう。<br>\n",
        "分類モデルの評価指標として、混同行列に基づく様々な指標が知られています。<br>\n",
        "代表的なものは、 accuracy（精度）や precision（適合率）、recall （再現率）、 F1 スコアなどがあります。<br>\n",
        "これらの指標はそれぞれ異なる性質を持っています。<br>\n",
        "accuracy は、全体の予測の正確さを示しますが、データのクラスの不均衡がある場合には適切な評価指標とはなりません。<br>\n",
        "precision は、正と予測したものの中で実際に正であるものの割合を表し、 recall は実際に正であるものの中で正と予測できたものの割合を示します。<br>\n",
        "F1スコアは適合率と再現率の調和平均で、両者のバランスを取る指標として利用されます。<br>\n",
        "\n",
        "\n",
        "Let's also take a look at evaluation metrics for classification models. <br>\n",
        "There are various metrics known for evaluating classification models, many of which are based on the confusion matrix. <br>\n",
        "Prominent among these are accuracy, precision, recall, and the F1 score.\n",
        "<br>\n",
        "These metrics each have different characteristics. <br>\n",
        "Accuracy represents the overall correctness of the predictions, but it is not an appropriate evaluation metric when there is class imbalance in the data. <br>\n",
        "Precision represents the proportion of the instances predicted as positive that are actually positive, while recall represents the proportion of actual positive instances that were correctly predicted as such. <br>\n",
        "The F1 score is the harmonic mean of precision and recall and is used as a metric to balance the two.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SoGhgjQHQqIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt_vkTRjQqIp"
      },
      "outputs": [],
      "source": [
        "# SVCのClassificationReport\n",
        "svc_visualizer = ClassificationReport(svc_grid_search.best_estimator_, classes=[0,1], support=True)\n",
        "\n",
        "# 訓練データでモデルのfitを行い、テストデータでスコアを計算\n",
        "svc_visualizer.fit(X_train, y_train_c.ravel())\n",
        "svc_visualizer.score(X_test, y_test_c.ravel())\n",
        "svc_visualizer.show()\n",
        "\n",
        "# RandomForestClassifierのClassificationReport\n",
        "rfc_visualizer = ClassificationReport(rfc_random_search.best_estimator_, classes=[0,1], support=True)\n",
        "\n",
        "# 訓練データでモデルのfitを行い、テストデータでスコアを計算\n",
        "rfc_visualizer.fit(X_train, y_train_c.ravel())\n",
        "rfc_visualizer.score(X_test, y_test_c.ravel())\n",
        "rfc_visualizer.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW95nMHyQqIp"
      },
      "outputs": [],
      "source": [
        "# SVCのClassPredictionError\n",
        "svc_cpe_visualizer = ClassPredictionError(svc_grid_search.best_estimator_, classes=[0,1])\n",
        "\n",
        "# 訓練データでモデルのfitを行い、テストデータでスコアを計算\n",
        "svc_cpe_visualizer.fit(X_train, y_train_c.ravel())\n",
        "svc_cpe_visualizer.score(X_test, y_test_c.ravel())\n",
        "svc_cpe_visualizer.show()\n",
        "\n",
        "# RandomForestClassifierのClassPredictionError\n",
        "rfc_cpe_visualizer = ClassPredictionError(rfc_random_search.best_estimator_, classes=[0,1])\n",
        "\n",
        "# 訓練データでモデルのfitを行い、テストデータでスコアを計算\n",
        "rfc_cpe_visualizer.fit(X_train, y_train_c.ravel())\n",
        "rfc_cpe_visualizer.score(X_test, y_test_c.ravel())\n",
        "rfc_cpe_visualizer.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PR-AUCについて / PR-AUC\n",
        "PR-AUC (Precision-Recall Area Under the Curve)は、Precision-Recall曲線（適合率-再現率曲線）の下の面積を計測する評価指標です。不均衡なデータセットに対するモデルの性能を評価する際に、よく使用されます。PR曲線は、異なる閾値設定でのモデルの適合率と再現率の関係をプロットしたもので、PR-AUCはこの曲線下の面積を示します。値が1に近いほど、モデルの性能が高いと評価されます。<br>\n",
        "PR-AUCについて、詳細な説明は[こちらのページ](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)も参考にしてください。\n",
        "\n",
        "PR-AUC (Precision-Recall Area Under the Curve) is an evaluation metric that measures the area under the Precision-Recall curve. It is often used to evaluate the performance of a model on imbalanced datasets. The Precision-Recall curve plots the relationship between the model's precision and recall at different threshold settings, and the PR-AUC represents the area under this curve. The closer the value is to 1, the higher the model's performance is evaluated.\n",
        " If you want more information read [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)."
      ],
      "metadata": {
        "id": "WEDMsz-i0XvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# SVCモデルからの予測確率を取得\n",
        "svc_probs = svc_grid_search.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# RFCモデルからの予測確率を取得\n",
        "rfc_probs = rfc_random_search.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 評価指標を計算\n",
        "svc_f1 = f1_score(y_test_c.ravel(), svc_predictions)\n",
        "svc_roc_auc = roc_auc_score(y_test_c.ravel(), svc_probs)\n",
        "svc_pr_auc = average_precision_score(y_test_c.ravel(), svc_probs)\n",
        "\n",
        "rfc_f1 = f1_score(y_test_c.ravel(), rfc_predictions)\n",
        "rfc_roc_auc = roc_auc_score(y_test_c.ravel(), rfc_probs)\n",
        "rfc_pr_auc = average_precision_score(y_test_c.ravel(), rfc_probs)\n",
        "\n",
        "# 結果を出力\n",
        "print(\"SVC Metrics:\")\n",
        "print(f\"F1 Score: {svc_f1}\")\n",
        "print(f\"ROC-AUC: {svc_roc_auc}\")\n",
        "print(f\"PR-AUC: {svc_pr_auc}\")\n",
        "\n",
        "print(\"\\nRFC Metrics:\")\n",
        "print(f\"F1 Score: {rfc_f1}\")\n",
        "print(f\"ROC-AUC: {rfc_roc_auc}\")\n",
        "print(f\"PR-AUC: {rfc_pr_auc}\")\n"
      ],
      "metadata": {
        "id": "nm_GKD2BdD1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# ROC curveの計算\n",
        "fpr_svc, tpr_svc, _ = roc_curve(y_test_c.ravel(), svc_probs)\n",
        "roc_auc_svc = auc(fpr_svc, tpr_svc)\n",
        "\n",
        "fpr_rfc, tpr_rfc, _ = roc_curve(y_test_c.ravel(), rfc_probs)\n",
        "roc_auc_rfc = auc(fpr_rfc, tpr_rfc)\n",
        "\n",
        "# PR curveの計算\n",
        "precision_svc, recall_svc, _ = precision_recall_curve(y_test_c.ravel(), svc_probs)\n",
        "pr_auc_svc = auc(recall_svc, precision_svc)\n",
        "\n",
        "precision_rfc, recall_rfc, _ = precision_recall_curve(y_test_c.ravel(), rfc_probs)\n",
        "pr_auc_rfc = auc(recall_rfc, precision_rfc)\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# ROC curveの可視化\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr_svc, tpr_svc, color='blue', label=f'SVC ROC curve (area = {roc_auc_svc:.2f})')\n",
        "plt.plot(fpr_rfc, tpr_rfc, color='green', label=f'RFC ROC curve (area = {roc_auc_rfc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# PR curveの可視化\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall_svc, precision_svc, color='blue', label=f'SVC PR curve (area = {pr_auc_svc:.2f})')\n",
        "plt.plot(recall_rfc, precision_rfc, color='green', label=f'RFC PR curve (area = {pr_auc_rfc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nzwKFKyVejXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWirsNQgQqIq"
      },
      "source": [
        "#### 3.3.4 ✏️ 適切な方法でモデルを評価してみましょう / Properly evaluate your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2fsrODSQqIq"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBUfvvStQqIq"
      },
      "source": [
        "### 3.4 オリジナルモデルの作成 / Your model\n",
        "\n",
        "ここまでに学んだモデル作成に関する知識を用いて、潜在性低酸素血症を予測するのに最も適切と思われるフレームワークを選び、モデルを作成しましょう。\n",
        "\n",
        "必要であれば本ノートブック内のコードを自由に使ってもらって構いません。\n",
        "\n",
        "<br/>\n",
        "\n",
        "Use the insigths learned from the models previously presented to build your own model with the framework you think is most suitable. You are free to use any of the code presented in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuTTZlTMQqIq"
      },
      "source": [
        "###### 3.4.1 ✏️ オリジナルモデルを作成しよう /  Implement your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JwjeZQjQqIq"
      },
      "outputs": [],
      "source": [
        "# Code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKuNtyjDQqIq"
      },
      "source": [
        "## 4. 特徴量の重要度 / Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAWCruDPQqIq"
      },
      "source": [
        "テストデータについてSHAP値を計算してみましょう。SHAPについての詳細は[こちらの公式ドキュメント](https://shap-lrjball.readthedocs.io/en/latest/index.html)を御覧ください。\n",
        "\n",
        "Compute the SHAP values for the test data. If you want more information about SHAP, read [here](https://shap-lrjball.readthedocs.io/en/latest/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "import shap\n",
        "\n",
        "# モデルと前処理データを取得\n",
        "rfc_model = rfc_random_search.best_estimator_.named_steps['classifier']\n",
        "fitted_custom_transform = rfc_random_search.best_estimator_.named_steps['custom_transform']\n",
        "fitted_preprocessor = rfc_random_search.best_estimator_.named_steps['preprocessor']\n",
        "\n",
        "# データを前処理\n",
        "X_train_transformed = fitted_custom_transform.transform(X_train)\n",
        "X_train_preprocessed = fitted_preprocessor.transform(X_train_transformed)\n",
        "\n",
        "X_test_transformed = fitted_custom_transform.transform(X_test)\n",
        "X_test_preprocessed = fitted_preprocessor.transform(X_test_transformed)\n",
        "\n",
        "# 事前のデータセットを取得\n",
        "background_data = shap.sample(X_train_preprocessed, 100)  # 100のインスタンスを使用\n",
        "\n",
        "# TreeExplainerを作成\n",
        "explainer = shap.TreeExplainer(rfc_model, background_data, check_additivity=False)\n",
        "\n",
        "# SHAP値をテストデータで計算\n",
        "shap_values_test = explainer.shap_values(X_test_preprocessed, check_additivity=False)\n",
        "\n",
        "def get_feature_names(column_transformer):\n",
        "    \"\"\"Get feature names from all transformers.\"\"\"\n",
        "    return column_transformer.get_feature_names_out()\n",
        "\n",
        "# フィット済みのColumnTransformerから特徴名を取得\n",
        "fitted_feature_names = fitted_preprocessor.get_feature_names_out()\n",
        "\n",
        "# この新しい特徴名リストでSHAPの要約プロットを表示\n",
        "shap.summary_plot(shap_values_test[1], X_test_preprocessed, feature_names=fitted_feature_names)\n"
      ],
      "metadata": {
        "id": "S5X1atxgh5G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Random Forest Classifier の場合、 feature importance を取得することが可能\n",
        "\n",
        "# モデルから最良のランダムフォレスト分類器を取得\n",
        "best_rfc = rfc_random_search.best_estimator_.named_steps['classifier']\n",
        "\n",
        "# 特徴量の名前を取得（前処理の情報も含めるため、'preprocessor'を通して取得）\n",
        "features = rfc_random_search.best_estimator_.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# 特徴量の重要度を取得\n",
        "feature_importance = best_rfc.feature_importances_\n",
        "\n",
        "# 重要度をデータフレームに変換\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
        "\n",
        "# 重要度の降順にソート\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# プロット\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
        "plt.title('Feature Importance - Random Forest Classifier')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EcpeP6BGj7Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 交差検証 / Cross validation"
      ],
      "metadata": {
        "id": "KBMuDX9O7v2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "交差検証（クロスバリデーション）は、モデルの汎化能力を評価するための手法の一つです。データセットを複数のサブセット（通常は等分）に分割し、そのうちの一つを検証用セットとして残りを訓練セットとして利用します。この過程を各サブセットに対して繰り返すことで、全てのデータが一度は検証セットとして用いられ、モデルの性能をより信頼性高く評価することができます。代表的な交差検証法にはk-分割交差検証があります。詳細は[こちらのページ](https://scikit-learn.org/stable/modules/cross_validation.html)を参考にしてください。\n",
        "\n",
        "Cross-validation is a technique used to evaluate the generalization ability of a model. It involves dividing the dataset into several subsets (usually equal parts), using one as the test set and the rest as the training set. By repeating this process for each subset, all data are used at least once as a test set, enabling a more reliable evaluation of the model's performance. A popular method of cross-validation is k-fold cross-validation. For more information, please read [this page](https://scikit-learn.org/stable/modules/cross_validation.html)."
      ],
      "metadata": {
        "id": "Jdr0Izr07z3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!"
      ],
      "metadata": {
        "id": "Qz8CFM5q3bcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgement\n",
        "日本語訳 (Translation):齋藤 幸史郎 (Koshiro Saito)"
      ],
      "metadata": {
        "id": "TFvc_-zQQqIr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fLfLhYkUlLzc",
        "aDGC2hIMlN9P"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}