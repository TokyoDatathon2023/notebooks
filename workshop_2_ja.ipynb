{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop 2: Clinical Variables Selection & Feature Engineering / è‡¨åºŠå¤‰æ•°ã®é¸æŠã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
      ],
      "metadata": {
        "id": "M65WOuF6u62h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¯ Workshop Goals\n",
        "The goal of this workshop is to provide participants with a deep understanding of data preprocessing in the data science workflow. Specifically, by the end of this workshop, participants should be able to:\n",
        "\n",
        "1. **Understand the importance of Data Preprocessing Techniques**: Understanding thes significance of data preprocessing in the data science workflow. This includes to be able to apply common techniques such as cleaning, normalization, transformation, and reduction of data. This also includes handling missing data, outliers, skewed data, and data with different scales.\n",
        "\n",
        "3. **Familiarize with Data Pre-processing Techniques**: such as feature scaling, dimensionality reduction, and feature engineering.\n",
        "\n",
        "4. **Apply data pre-processing techniques**: This involved the practical application of data preprocessing techniques to real-world datasets and to be able to evaluate the impact of different preprocessing techniques on machine learning model performance.\n",
        "\n",
        "Be aware of the potential biases that can be introduced in data preprocessing, and how to identify and mitigate them.\n",
        "Throughout the workshop, participants will engage in hands-on activities, case studies, and real-world examples. They will work in groups to apply the concepts learned to real datasets, and engage in discussions to share their experiences and insights. By the end of the workshop, participants should have gained a solid understanding of data preprocessing techniques and their importance in the data science workflow, and be able to apply these techniques to improve the performance of machine learning models."
      ],
      "metadata": {
        "id": "p-DviQlnd7vT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¯ã“ã®Workshopã®ç›®æ¨™\n",
        "æœ¬ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«ã¤ã„ã¦ã®ç†è§£ã‚’æ·±ã‚ã¦ã‚‚ã‚‰ã†ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "æœ¬ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã‚’é€šã˜ã¦å­¦ã¶ã“ã¨ï¼š\n",
        "\n",
        "1. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æŠ€è¡“ã®é‡è¦æ€§ã®ç†è§£**: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«ã¤ã„ã¦ã®çŸ¥è­˜ã‚’ä¼šå¾—ã—ã¾ã™ã€‚ä¸€èˆ¬çš„ãªæŠ€æ³•ã§ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿æ•´ç†ã€æ­£è¦åŒ–ã€å¤‰æ›ã€å‰Šæ¸›ãªã©ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨ã«åŠ ãˆã€æ¬ æãƒ‡ãƒ¼ã‚¿ã‚„å¤–ã‚Œå€¤ã€æ­ªã‚“ã ãƒ‡ãƒ¼ã‚¿ã€ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã¸ã®å¯¾å‡¦ã‚‚å«ã¿ã¾ã™ã€‚\n",
        "\n",
        "3. **ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†æŠ€è¡“ã®ç¿’å¾—**: ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€æ¬¡å…ƒå‰Šæ¸›ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãªã©\n",
        "\n",
        "4. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æŠ€è¡“ã®é©ç”¨**: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’æ–½ã—ã€ãã‚Œãã‚Œã®å‡¦ç†ãŒæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«ã‚ˆã£ã¦å°å…¥ã•ã‚Œã†ã‚‹æ½œåœ¨çš„ãªãƒã‚¤ã‚¢ã‚¹ã¨ãã‚Œã‚’ç‰¹å®šã—ã¦è»½æ¸›ã™ã‚‹æ–¹æ³•ã‚’èªè­˜ã—ã¾ã™ã€‚\n",
        "ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã‚’é€šã˜ã¦ã€å‚åŠ è€…ã®çš†æ§˜ã«ã¯å®Ÿæˆ¦çš„ãªæ´»å‹•ã‚„ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã€å®Ÿéš›ã®äº‹ä¾‹ã«å–ã‚Šçµ„ã‚“ã§ã„ãŸã ãã¾ã™ã€‚ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‹ã‚Œã¦å­¦ã‚“ã æ¦‚è¦ã‚’å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã—ã€ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦çµŒé¨“ã¨æ´å¯Ÿã‚’å…±æœ‰ã—ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—çµ‚äº†æ™‚ã«ã¯ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†æŠ€è¡“ã¨ãã®é‡è¦æ€§ã«ã¤ã„ã¦ã—ã£ã‹ã‚Šã¨ç†è§£ã—ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã«æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "knt1O68zdxWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœï¸ Expected Deliverables\n",
        "\n",
        "1. A report or dashboard summarizing the results of EDA, including visualizations and statistical summaries of the data distribution and correlations.\n",
        "\n",
        "2. A set of code scripts or pipelines that automate the data preprocessing process, making it easier and more efficient to apply these techniques to future datasets.\n",
        "\n",
        "3. A cleaned dataset that has undergone preprocessing techniques such as removal of duplicates, handling missing data, and dealing with outliers. The cleaned dataset should be ready to be fed into machine learning models."
      ],
      "metadata": {
        "id": "lHt_KgdnGilV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœï¸ æœ€çµ‚ç›®æ¨™\n",
        "\n",
        "1. EDA* ã®çµæœã‚’ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã¨ç›¸é–¢ã«ã¤ã„ã¦çµ±è¨ˆçš„ã«ã¾ã¨ã‚ãŸãƒ¬ãƒãƒ¼ãƒˆã‚„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹\n",
        "\n",
        "2. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€å°†æ¥çš„ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ç°¡å˜ã‹ã¤åŠ¹ç‡çš„ã«é©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "\n",
        "3. é‡è¤‡ã®é™¤å»ã€æ¬ æãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã€å¤–ã‚Œå€¤å‡¦ç†ãªã©ã‚’æ–½ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ•´å‚™ã—ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«å…¥ã‚Œã‚‹æº–å‚™ã‚’æ•´ãˆã‚‹\n",
        "\n",
        "\\* Exploratory Data Analysis: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ"
      ],
      "metadata": {
        "id": "Zqoq81Iaps7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# â— Highlighted Pitfall(s)\n",
        "1. Make sure to apply preprocessing steps only to the training data and avoid using information from the validation set to prevent data leakage.\n",
        "\n",
        "2. Check for potential biases that could be introduced or amplified by preprocessing techniques, and evaluate the impact of these techniques on different subgroups of the data.\n",
        "\n",
        "3. Carefully evaluate the appropriateness of different preprocessing techniques for a given dataset and ensure that the techniques are applied correctly to avoid incorrect preprocessing that could lead to poor model performance or incorrect conclusions about the data."
      ],
      "metadata": {
        "id": "EnR2K_EKGrEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# â— æ³¨æ„äº‹é …\n",
        "1. ãƒ‡ãƒ¼ã‚¿ã®æ¼æ´©ã‚’é˜²ããŸã‚ã€å‰å‡¦ç†ã«åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ç”¨ã„ãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã€‚\n",
        "\n",
        "2. å‰å‡¦ç†ã«ã‚ˆã£ã¦å°å…¥ãƒ»å¼·èª¿ã•ã‚Œã†ã‚‹æ½œåœ¨çš„ãªãƒã‚¤ã‚¢ã‚¹ã‚’ç¢ºèªã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç•°ãªã‚‹ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã«ãŠã‘ã‚‹ã“ã‚Œã‚‰ã®å‡¦ç†æŠ€è¡“ã®å½±éŸ¿ã‚’è©•ä¾¡ã™ã‚‹ã€‚\n",
        "\n",
        "3. ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹æ§˜ã€…ãªå‰å‡¦ç†æŠ€è¡“ã®é©åˆ‡æ€§ã‚’æ…é‡ã«è©•ä¾¡ã™ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ä½ä¸‹ã‚„ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹èª¤ã£ãŸçµè«–ã«ã¤ãªãŒã‚‹èª¤ã£ãŸå‰å‡¦ç†ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€å‰å‡¦ç†ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "ZlnuhKzvp1fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add visual aid here (figures without preprocessing)\n",
        "å‰å‡¦ç†ãªã—ã®å›³ã®è¿½åŠ "
      ],
      "metadata": {
        "id": "G3qRzUX_2qke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.  Setup environment / ç’°å¢ƒæ§‹ç¯‰**\n",
        "---"
      ],
      "metadata": {
        "id": "EffSajZhq7lo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otcVcnAOQEUH"
      },
      "outputs": [],
      "source": [
        "# Data reading in Dataframe format and data preprocessing\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For missing values\n",
        "import missingno as msno\n",
        "\n",
        "# Linear algebra operations\n",
        "import numpy as np\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif, mutual_info_regression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount user's Google Drive to Google Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Frc9RcmKR3Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive ã®ä¸­èº«ãŒå…¨éƒ¨è¦‹ã‚‰ã‚ŒãŸããªã„äººã®ãŸã‚ã®ã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆç”¨\n",
        "# ã“ã®å ´åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŒã€æ™‚é–“åˆ¶é™ã«ã‚ˆã£ã¦å‰Šé™¤ã•ã‚Œã‚‹ãŸã‚æ³¨æ„ã™ã‚‹ã“ã¨\n",
        "\n",
        "# drive.flush_and_unmount('/content/drive')"
      ],
      "metadata": {
        "id": "03evg_cYdkC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **2. Data Analysis / ãƒ‡ãƒ¼ã‚¿åˆ†æ**\n",
        "---"
      ],
      "metadata": {
        "id": "91xUTkd2rOUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we understand the problem, we have formed a multidisciplinary team, formulated the research question,  hypothesis, we have to start working with the data. What data do we have? How do they look? What distributions do they have?\n",
        "\n",
        "Let's remember that in data science, data is the most important thing, and whether or not we can solve a problem depends on the data quality. At the same time, understanding the data also helps us to have a clearer vision of what we are facing and in case the data is not very good, we can at least fix it.\n",
        "\n",
        "Specifically, understanding the problem and the data are the essential phases in a data science project. An error in this phase is much more critical than an error in the modeling and evaluation phases. We must bear in mind that machine learning is not a magical tool that solves any type of problem, but rather a mathematical/statistical tool that learns from what we teach it, therefore if the data has biases, the model will also have them."
      ],
      "metadata": {
        "id": "wsZkeFBG08pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å•é¡Œã‚’ç†è§£ã—ãŸã‚‰ã€ãƒãƒ¼ãƒ ã§ç ”ç©¶èª²é¡Œã‚’æ±ºã‚ã¦ä»®èª¬ã‚’ç«‹ã¦ã¦ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†ã€‚ã©ã‚“ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã©ã®ã‚ˆã†ã«è¦‹ãˆã¾ã™ã‹ï¼Ÿã©ã‚“ãªåˆ†å¸ƒã‚’æŒã£ã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã«ãŠã„ã¦ã¯ãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚é‡è¦ã§ã‚ã‚Šã€èª²é¡Œè§£æ±ºã¯ãƒ‡ãƒ¼ã‚¿ã®è³ªã®è‰¯ã—æ‚ªã—ã«ã‹ã‹ã£ã¦ã„ã¾ã™ã€‚\n",
        "ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ã¯ç›´é¢ã—ã¦ã„ã‚‹å•é¡Œã‚’æ˜ç¢ºã«æ‰ãˆã‚‹ã“ã¨ã«ç¹‹ãŒã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã®è‰¯ããªã„éƒ¨åˆ†ã‚’è¦‹ã¤ã‘ã€å¯¾å‡¦ã‚’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "å•é¡Œã¨ãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã«ãŠã„ã¦ã¯ä¸å¯æ¬ ãªè¦ç´ ã§ã™ã€‚\n",
        "ã“ã®éç¨‹ã§ã®å¤±æ•—ã¯å¾Œã«å‡ºã¦ãã‚‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚„è©•ä¾¡ã¨ã„ã£ãŸéç¨‹ã§ã®å¤±æ•—ã‚ˆã‚Šã‚‚é‡å¤§ãªã‚‚ã®ã¨ãªã‚Šã¾ã™ã€‚\n",
        "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯æˆ‘ã€…ãŒæ•™ãˆãŸã“ã¨ã‚’å­¦ç¿’ã™ã‚‹æ•°å­¦çš„/çµ±è¨ˆçš„ãªé“å…·ã§ã™ã€‚\n",
        "ã—ãŸãŒã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚Œã°ãƒ¢ãƒ‡ãƒ«ã«ã‚‚ãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿã˜ã¾ã™ã€‚\n",
        "**æ©Ÿæ¢°å­¦ç¿’ã¯ã©ã‚“ãªå•é¡Œã§ã‚‚è§£æ±ºã—ã¦ãã‚Œã‚‹é­”æ³•ã®é“å…·ã§ã¯ãªã„**ã¨ã„ã†ã“ã¨ã‚’é ­ã«ç½®ã„ã¦ãŠãã¾ã—ã‚‡ã†ã€‚\n"
      ],
      "metadata": {
        "id": "gBRUjzAOdzIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the dataset\n",
        "\n",
        "Let's asume that you already have the dataset of workshop 1:"
      ],
      "metadata": {
        "id": "Hzzgjobkzz1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
        "\n",
        "workshop 1ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ—¢ã«æŒã£ã¦ã„ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "ZjRd4w2OfJxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Read the dataset and use the function null_values to see which columns has missing data."
      ],
      "metadata": {
        "id": "ewfSG4dLHFxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€`null_values`é–¢æ•°ã‚’ç”¨ã„ã¦æ¬ æå€¤ã®ã‚ã‚‹åˆ—ã‚’æ¢ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "CuHjFiIntohu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset and use the function null_values to see which columns has missing data.\n",
        "\n",
        "PATH = \"\"\n",
        "\n",
        "# Read file here:\n",
        "df = pd.read_csv(PATH)\n",
        "print(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns')"
      ],
      "metadata": {
        "id": "8F5j91Ii3WpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def null_values(df: pd.DataFrame) -> None:\n",
        "  \"\"\"\n",
        "  ãƒ‡ãƒ¼ã‚¿å†…ã«æ¬ æå€¤ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°ã€‚\n",
        "  æ¬ æå€¤ãŒãªã„å ´åˆ: There aren't null values in the dataframe\n",
        "  æ¬ æå€¤ãŒã‚ã‚‹å ´åˆ: è©²å½“ç®‡æ‰€ã®åˆ—åã¨æ¬ æå€¤ã®æ•°ã‚’å‡ºåŠ›ã™ã‚‹\n",
        "  \"\"\"\n",
        "  nulls = df.isnull().sum()\n",
        "  if nulls.sum() == 0:\n",
        "      print(\"There aren't null values in the dataframe\")\n",
        "  else:\n",
        "      print('Null values:')\n",
        "      print(nulls[nulls > 0])"
      ],
      "metadata": {
        "id": "T5NLdL3K1cib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there are null values here:\n",
        "null_values(df)"
      ],
      "metadata": {
        "id": "vwAy1XAHR3Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **3. Understand your Data / ãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹**\n",
        "---\n"
      ],
      "metadata": {
        "id": "sKiLCcTe_1s2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the variables:\n",
        "\n",
        "The first step is to understand what variables your dataset has and how these variables are distributed. The columns and data types are described in [Variable Dictionary (GitHub Link)](https://github.com/TokyoDatathon2023/notebooks/blob/main/variable_dictionary.md).\n",
        "\n",
        "Discuss with your team about the problem and what variables are needed."
      ],
      "metadata": {
        "id": "zqwtcefTVxdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the variables:\n",
        "\n",
        "ã¾ãšã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã©ã®ã‚ˆã†ãªå¤‰æ•°ã‚’æŒã¡ã€ãã‚Œã‚‰ã®å¤‰æ•°ãŒã©ã®ã‚ˆã†ã«åˆ†å¸ƒã—ã¦ã„ã‚‹ã®ã‹ã‚’ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ•°ä¸€è¦§ï¼š[Variable Dictionary (GitHub Link)](https://github.com/TokyoDatathon2023/notebooks/blob/main/variable_dictionary.md).\n",
        "\n",
        "ã©ã®å¤‰æ•°ãŒå¿…è¦ã‹ã‚’ãƒãƒ¼ãƒ ã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ã€‚\n"
      ],
      "metadata": {
        "id": "pQsQaYbBqH3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select your variables\n",
        "**Task:** To simplify the task we will use a reduced group of variables for the following analysis. Ideally you should decide with your groups which variables you consider to be really important.\n",
        "\n",
        "The variables should be selected:\n",
        "1. Always thinking about the question to solve and the hypothesis.\n",
        "2. Using the clinical experience of the members of the group and if possible supported by literature.\n",
        "3. Using mathematical and statistical methods."
      ],
      "metadata": {
        "id": "ZEzfCAqiVRx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å¤‰æ•°ã®é¸æŠ\n",
        "**ã‚¿ã‚¹ã‚¯ï¼š**\n",
        "ä½œæ¥­ã‚’å˜ç´”åŒ–ã™ã‚‹ãŸã‚ã«ã€ã“ã‚Œä»¥é™ã®åˆ†æã§ä½¿ç”¨ã™ã‚‹å¤‰æ•°ã‚’æ¸›ã‚‰ã—ã¦ä½œæ¥­ã‚’é€²ã‚ã¾ã™ã€‚\n",
        "é‡è¦ãªå¤‰æ•°ãŒä½•ã‹ã€åˆ†æã«å¿…è¦ãªå¤‰æ•°ã¯ä½•ã‹ã€ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã§è©±ã—åˆã„ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "æ°—ã‚’ä»˜ã‘ã‚‹ã“ã¨\n",
        "1. è§£æ±ºã™ã¹ãèª²é¡Œã¨ä»®èª¬ã‚’å¸¸ã«æ„è­˜ã™ã‚‹ã“ã¨ã€‚\n",
        "2. ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ¡ãƒ³ãƒãƒ¼ã®è‡¨åºŠçµŒé¨“ã‚’åŸºã«ã™ã‚‹ã“ã¨ã€‚å¯èƒ½ã§ã‚ã‚Œã°æ–‡çŒ®ã«ã‚ˆã‚‹è£ä»˜ã‘ã‚‚ã¨ã‚‹ã“ã¨ã€‚\n",
        "3. æ•°å­¦çš„ãƒ»çµ±è¨ˆçš„ãªæ‰‹æ³•ã‚’ç”¨ã„ã‚‹ã“ã¨ã€‚\n"
      ],
      "metadata": {
        "id": "_K4UPphxJxbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Select demographic variables:\n",
        "Due to the challenge we need some demographic information select the relevant columns:"
      ],
      "metadata": {
        "id": "npybroq6Ju2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ æ‚£è€…èƒŒæ™¯ã«é–¢ã™ã‚‹äººå£çµ±è¨ˆå¤‰æ•°ã®é¸æŠ:\n",
        "èª²é¡Œè§£æ±ºã®ãŸã‚ã«ã¯ã„ãã¤ã‹ã®æ‚£è€…èƒŒæ™¯ã«é–¢ã™ã‚‹äººå£çµ±è¨ˆå­¦çš„æƒ…å ±ãŒå¿…è¦ã§ã™ã€‚"
      ],
      "metadata": {
        "id": "4xcftuvbMeHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patient main information (include identifiers)\n",
        "patient_info = ['variable1', 'variable2', ... ]\n",
        "\n",
        "# Demographic variables\n",
        "demographic_variables = ['variable1', 'variable2', ... ]"
      ],
      "metadata": {
        "id": "ZIUc7QwLJuDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clinical variables selected were selected based on the variables that are most related with SaO2 and Spo2 and a correlation method was used to filter those variables.\n",
        "You can use other methods and criteria!"
      ],
      "metadata": {
        "id": "gZxXW_09KCOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SaO2ã¨SpO2ã¨é–¢é€£æ€§ã®é«˜ã„å¤‰æ•°ã‚’é¸æŠã—ã€ãã‚Œã‚‰ã®å¤‰æ•°ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ç›¸é–¢æ³•ã‚’ç”¨ã„ã¾ã™ã€‚ã‚‚ã¡ã‚ã‚“ã€ä»–ã®æ–¹æ³•ã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ï¼"
      ],
      "metadata": {
        "id": "RMnIyfbVDOup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Select clinical variables\n",
        "\n",
        "1. Manual variable selection: Manually select with you team the variables that are clinically important. Don't worry if there are many, later we will use other methods to filter the variables"
      ],
      "metadata": {
        "id": "Ucr8SD4lW0Eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### è‡¨åºŠå¤‰æ•°ã®é¸æŠ\n",
        "\n",
        "1. æ‰‹å‹•ã§ã®å¤‰æ•°é¸æŠ: ãƒãƒ¼ãƒ ã§è©±ã—åˆã£ã¦ã€è‡¨åºŠçš„ã«é‡è¦ãªå¤‰æ•°ã‚’æ‰‹å‹•ã§é¸æŠã—ã¾ã™ã€‚\n",
        "å¾Œã§ä»–ã®æ–¹æ³•ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†ã®ã§ã€æ•°ãŒå¤šããªã£ã¦ã‚‚æ°—ã«ã›ãšé¸æŠã—ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "_MLRB7YbOcWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Static clinical variables\n",
        "static_clinical_variables = ['variable1', 'variable2', ... ]\n",
        "\n",
        "# Temporal clinical variables (E.g. Sofa related variables)\n",
        "sofa_variables = ['variable1', 'variable2', ... ]\n",
        "temporal_clinical_variables = ['variable1', 'variable2', ... 'variable3'] + sofa_variables\n",
        "\n",
        "outcomes = ['variable1', 'variable2', ... ]\n",
        "\n",
        "treatment = ['variable1', 'variable2', ... ]"
      ],
      "metadata": {
        "id": "GLYKUYWtKjNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Variable filtering\n",
        "Let's now employ a few techniques to filter the variables. You should choose a technique to do it.\n",
        "\n",
        "Hint: To choose the variables most connected with the columns SpO2 and SaO2, we utilised the correlation as an example. You are not required to employ the identical column(s). Select the columns in accordance with your hypothesis."
      ],
      "metadata": {
        "id": "lhKfPsmRW8TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ å¤‰æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "å¤‰æ•°ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹æ‰‹æ³•ã‚’ã„ãã¤ã‹ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "ã©ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ä½¿ã†ã®ã‹ã¯ãƒãƒ¼ãƒ ã§æ±ºå®šã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ãƒ’ãƒ³ãƒˆ: å…ˆã»ã©ã¯ä¾‹ã¨ã—ã¦ã€SpO2åˆ—ã¨SaO2åˆ—ã¨æœ€ã‚‚é–¢ä¿‚ã®ã‚ã‚‹å¤‰æ•°ã‚’é¸æŠã™ã‚‹ãŸã‚ã«ã€ç›¸é–¢ã‚’ç”¨ã„ã¦ã„ã¾ã—ãŸãŒã€ãƒãƒ¼ãƒ ã§ç«‹ã¦ãŸä»®èª¬ã«åŸºã¥ã„ã¦é©å®œå¿…è¦ãªåˆ—ã‚’é¸æŠã—ã¦ä¸‹ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "cvgNoOalTMH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Union\n",
        "\n",
        "def get_most_correlated(df: pd.DataFrame, variable: str, n: int) -> List[str]:\n",
        "  \"\"\"\n",
        "  ç‰¹å®šã®å¤‰æ•°ã«æœ€ã‚‚ç›¸é–¢(æ­£ãƒ»è² ã¨ã‚‚ã«)ã®ã‚ã‚‹å¤‰æ•°ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°\n",
        "  å¼•æ•°(å‹å):\n",
        "    df(pandas dataframe): è¨ˆç®—å¯¾è±¡ã®å¤‰æ•°ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿\n",
        "    variable(string): ç›¸é–¢ã‚’è¨ˆç®—ã—ãŸã„å¤‰æ•°å(Eg. 'SaO2')\n",
        "    n(integer): æœ€ã‚‚ç›¸é–¢ã®ã‚ã‚‹å¤‰æ•°ã¨ã—ã¦å–å¾—ã—ãŸã„å¤‰æ•°ã®æ•°ã‚’è¡¨ã™æ•´æ•°\n",
        "  æˆ»ã‚Šå€¤:\n",
        "    æœ€ã‚‚ç›¸é–¢(æ­£ãƒ»è² ã¨ã‚‚ã«)ã®å¼·ã„ä¸Šä½nå€‹ã®å¤‰æ•°åã®ãƒªã‚¹ãƒˆ\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the correlation matrix\n",
        "  corr_matrix = df.corr()\n",
        "\n",
        "  # Print the correlation matrix\n",
        "  correlation = corr_matrix[variable].sort_values(ascending=False)\n",
        "  positive_correlated = correlation[:n]\n",
        "  inversely_correlated = correlation[-n:]\n",
        "  print('#'*40, f' {variable} ' , '#'*40)\n",
        "  print(f'The {n} most correlated variables to variable {variable} are: ')\n",
        "  print(positive_correlated)\n",
        "  print(f'The {n} most inversely correlated variables to variable {variable} are: ')\n",
        "  print(inversely_correlated)\n",
        "\n",
        "  return list(positive_correlated.index) + list(inversely_correlated.index)"
      ],
      "metadata": {
        "id": "bqsjG9cFIVTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: add another feature selection process to improve the model at a later stage\n",
        "array_like = Union[pd.DataFrame, np.ndarray, List, Any]\n",
        "\n",
        "outcome_var = \"mortality_in\"\n",
        "K = 7\n",
        "method = f_classif  # f_classif, chi2, mutual_info_classif, mutual_info_regression, etc\n",
        "\n",
        "def feat_select(features: List[str], df: pd.DataFrame, outcome_var: str, K: int, method: Any) -> array_like:\n",
        "  X = df[features]\n",
        "  y = df[outcome_var]\n",
        "  return SelectKBest(method, k=K).fit(X, y).get_feature_names_out()\n",
        "\n",
        "# resulting_temporal_clinical_variables = feat_select(resulting_temporal_clinical_variables, df, outcome_var, K, method)"
      ],
      "metadata": {
        "id": "Nlys9bNkJb2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The number of initial temporal clinical variables is {len(temporal_clinical_variables)}')\n",
        "\n",
        "# Select the number of related columns:\n",
        "n1 =  # Eg. 8\n",
        "n2 =  # Eg. 10\n",
        "ref_col1 = # Eg. 'SaO2'\n",
        "ref_col2 = # Eg. 'SpO2'\n",
        "\n",
        "# Get variables correlated to 'ref_col'\n",
        "columns_1 = get_most_correlated(df[temporal_clinical_variables], variable=ref_col1, n=n1)\n",
        "\n",
        "# Get variables correlated to 'ref_col_2'\n",
        "columns_2 = get_most_correlated(df[temporal_clinical_variables], variable=ref_col2, n=n2)\n",
        "\n",
        "# Merge the resulting columns in a single list:\n",
        "resulting_temporal_clinical_variables = list(set(columns_1+columns_2))\n",
        "\n",
        "print(f'The number of resulting variables is {len(resulting_temporal_clinical_variables)}')"
      ],
      "metadata": {
        "id": "rxZJnP7bIVWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the resulting dataframe\n",
        "variables = list(set(patient_info + demographic_variables + static_clinical_variables + resulting_temporal_clinical_variables + outcomes + treatment))\n",
        "\n",
        "df = df [variables]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NMQJzYj-DPk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Outcome Leakage\n",
        "Outcome leakage occurs when the model is inadvertently given access to data it shouldn't have until the time of prediction. For instance, when predicting hidden hypoxemia, including the SOFA respiratory score (sofa_resp), which includes PaO2 in its definition, in our model could lead to leakage. This is because PaO2 is directly related to the outcome we're trying to predict. It's important to be mindful of potential outcome leakages like this when selecting variables for model training."
      ],
      "metadata": {
        "id": "3ovYuX8y46H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Outcome Leakageï¼ˆçµæœã®æ¼æ´©ï¼‰\n",
        "Outcome leakage ã¨ã¯ã€äºˆæ¸¬æ™‚ã¾ã§ã«ãƒ¢ãƒ‡ãƒ«ãŒã‚¢ã‚¯ã‚»ã‚¹ã™ã¹ãã§ãªã„ãƒ‡ãƒ¼ã‚¿ã«èª¤ã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å ´åˆã‚’æŒ‡ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€æ½œåœ¨æ€§ä½é…¸ç´ è¡€ç—‡ã‚’äºˆæ¸¬ã™ã‚‹éš›ã«ã€ãã‚Œè‡ªèº«ã®å®šç¾©ã« PaO2 ã‚’å«ã‚€ SOFA å‘¼å¸å™¨ã‚¹ã‚³ã‚¢ï¼ˆsofa_respï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ã«å«ã‚ã¦ã—ã¾ã†ã¨ã€æ¼æ´©ãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ PaO2 ãŒæˆ‘ã€…ãŒäºˆæ¸¬ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹çµæœã¨ç›´æ¥é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã®ãŸã‚ã«å¤‰æ•°ã‚’é¸æŠã™ã‚‹éš›ã«ã¯ã€ã“ã®ã‚ˆã†ãªæ½œåœ¨çš„ãª outcome leakage ã«æ³¨æ„ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚"
      ],
      "metadata": {
        "id": "8JcYd5MZ5KKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's discuss in the team and eliminate features that could potentially cause outcome leakage!"
      ],
      "metadata": {
        "id": "BMrqdre8_vXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand how variables are distributed:"
      ],
      "metadata": {
        "id": "-MHV8nXH_pZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’ç†è§£ã™ã‚‹:"
      ],
      "metadata": {
        "id": "oXzM0DSpUm4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see a description of some statistics of each numeric variable, you can use `df.describe()`:\n"
      ],
      "metadata": {
        "id": "36D4dSeJK2zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å„æ•°å€¤å¤‰æ•°ã®çµ±è¨ˆæƒ…å ±ã‚’è¦‹ã‚‹ã«ã¯`describe()`é–¢æ•°ã‚’ç”¨ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "PJvJy3brU7lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "gyYJzvip-4-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with `data.describe()` is that it groups using all the data of each variable, this means that if in our group there are populations that are underestimated or overestimated, the description will not take that into account. An alternative to solve that problem is using the function `groupby()`. With group by we can generate groups of populations using variables. An exmple is:"
      ],
      "metadata": {
        "id": "q8hv2yMShisk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`data.describe()`é–¢æ•°ã®å•é¡Œã¯ã€å„å¤‰æ•°ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚è©•ä¾¡ã—ã¦ã—ã¾ã†ã“ã¨ã§ã™ã€‚ãã®ãŸã‚éå°è©•ä¾¡ã‚‚ã—ãã¯éå¤§è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ãŒã„ãŸã¨ã—ã¦ã‚‚ã€è€ƒæ…®ã•ã‚Œã¾ã›ã‚“ã€‚ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€`groupby()`é–¢æ•°ã‚’ä½¿ã†ã“ã¨ã§ã€ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æ¯ã«è©•ä¾¡ãŒã§ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "m_sTYda7C1_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the mean of oxygen saturation of arterial blood (SaO2) by gender\n",
        "avg_sao2_by_gender = df.groupby('gender')['SaO2'].mean()\n",
        "avg_sao2_by_gender"
      ],
      "metadata": {
        "id": "KmRNIklehdSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also generate groups using more than one variable and more than one grouping methods"
      ],
      "metadata": {
        "id": "cUtoT_Z2yoN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¤‡æ•°ã®å¤‰æ•°ã¨è¤‡æ•°ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”¨ã„ã¦ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "cie41YCPYABp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain count, mean, standard deviation, min value and max value for (SaO2) and (SpO2) grouping by gender and race group:\n",
        "avg_sao2_by_ethnicity = df.groupby(['gender', 'race_group']).agg({\n",
        "    'SaO2': ['count', 'mean', 'std', 'min', 'max'],\n",
        "    'SpO2': ['count', 'mean', 'std', 'min', 'max']\n",
        "})\n",
        "avg_sao2_by_ethnicity"
      ],
      "metadata": {
        "id": "ZmqCwMY6hdVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, there are also libraries that allow us to do this process automatically. Below is an example using tableone library.\n",
        "\n",
        "Install using the command `!pip install tableone`"
      ],
      "metadata": {
        "id": "O-tm_ARoz-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»–ã«ã‚‚ã€åŒæ§˜ã®å‡¦ç†ã‚’è‡ªå‹•ã§è¡Œã£ã¦ãã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
        "ä»¥ä¸‹ã«`tableone`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚\n",
        "\n",
        "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ï¼š`!pip install tableone`"
      ],
      "metadata": {
        "id": "s321miP3YRF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As a first step let's do a preprocessing to some variables like:\n",
        "df['language'] = df['language'].replace({'ENGLISH': 'Proficient', '?': 'Limited Proficiency'})"
      ],
      "metadata": {
        "id": "pPzYMKLP9LJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tableone\n",
        "from tableone import TableOne\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "bw0nGk2YJwJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groupby = ['race_group']\n",
        "\n",
        "categorical_variables = ['mortality_in','gender', 'language', 'invasive_vent']\n",
        "numerical_variables = ['anchor_age', 'los_icu', 'los_hospital', 'CCI', 'SOFA_admission']\n",
        "columns = categorical_variables + numerical_variables\n",
        "\n",
        "labels ={'anchor_age': 'age',\n",
        "         'SOFA_admission': 'SOFA'}\n",
        "\n",
        "\n",
        "mytable = TableOne(df, columns=columns, categorical=categorical_variables, groupby=groupby, nonnormal=numerical_variables, rename=labels, pval=False)\n",
        "mytable"
      ],
      "metadata": {
        "id": "foYkDJqQocnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Use tableone\n",
        "\n",
        "Now let's create a new tableone using all the variables to to measure differences between ethnic groups!\n",
        "\n",
        "Hint: Use the function `get_categorical_numerical_variables` to get all the numeric and categorical variables. Then use those variables to generate a tableone of the whole dataset\n"
      ],
      "metadata": {
        "id": "MWKI47ieLIIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ tableone ã‚’ä½¿ã†\n",
        "\n",
        "ã§ã¯ã€äººç¨®é–“ã®å·®ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ã€å…¨ã¦ã®å¤‰æ•°ã‚’ç”¨ã„ã¦æ–°ã—ã„tableoneã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "ãƒ’ãƒ³ãƒˆ:\n",
        "æœ€åˆã«ã€`get_categorical_numerical_variables` é–¢æ•°ã‚’ä½¿ã£ã¦ã™ã¹ã¦ã®æ•°å€¤å¤‰æ•°ã¨ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã‚’å–å¾—ã—ã¾ã™ã€‚æ¬¡ã«ã€ãã‚Œã‚‰ã®å¤‰æ•°ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®tableoneã‚’ä½œæˆã—ã¾ã™ã€‚\n"
      ],
      "metadata": {
        "id": "4OB1wKepYy9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def get_categorical_numerical_variables(df: pd.DataFrame, limit: int, ignore: List[str], verbose: bool = True) -> Tuple[List[str], List[str]]:\n",
        "  \"\"\"\n",
        "  æ•°å€¤å¤‰æ•°ã¨ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®é–¢æ•°\n",
        "  å¼•æ•°:\n",
        "    df: å…¨ã¦ã®å¤‰æ•°ã‚’å«ã‚€ pandas ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
        "    limit: å¤‰æ•°ãŒã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã§ã‚ã‚‹ã¨åˆ¤å®šã™ã‚‹ãŸã‚ã®æœ€å¤§ã®è¦ç´ æ•°ã‚’è¡¨ã™æ•´æ•°\n",
        "    ignore: ç„¡è¦–ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆ\n",
        "\n",
        "  æˆ»ã‚Šå€¤:\n",
        "    categorical_columns, numerical_columns: ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã¨æ•°å€¤å¤‰æ•°ã®åå‰ã‚’å«ã‚€ãƒªã‚¹ãƒˆ\n",
        "  \"\"\"\n",
        "\n",
        "  categorical_columns = []\n",
        "  numerical_columns = []\n",
        "\n",
        "  for column in df.columns:\n",
        "    if ignore:\n",
        "      if column in ignore:\n",
        "        continue\n",
        "    unique_values = len(pd.unique(df[column]))\n",
        "    if unique_values <= limit:\n",
        "      if verbose:\n",
        "        print(f'The column {column} has {unique_values}, so is categorical')\n",
        "      categorical_columns.append(column)\n",
        "    else:\n",
        "      if verbose:\n",
        "        print(f'The column {column} has {unique_values}, so is numerical')\n",
        "      numerical_columns.append(column)\n",
        "\n",
        "  return categorical_columns, numerical_columns"
      ],
      "metadata": {
        "id": "ouIrWL4AiEM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns for the tableone\n",
        "groupby = #\n",
        "ignore = ['column1', 'column2', ...] + groupby\n",
        "limit = # Max. Number of unique values to be categorical (E.g. 15)\n",
        "\n",
        "categorical_columns, numerical_columns = get_categorical_numerical_variables(df, limit=limit, ignore=ignore)"
      ],
      "metadata": {
        "id": "ilDdtS_tex3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total_columns =\n",
        "Total_columns = categorical_columns + numerical_columns\n",
        "\n",
        "TableOne(df, columns=Total_columns, categorical=categorical_columns, nonnormal=numerical_columns, groupby=groupby)"
      ],
      "metadata": {
        "id": "JXtGKC6jfcct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize your data\n",
        "\n",
        "While statistics are important, it is also very important and key to visualize the data. This will help to share the information and results in a visual and intuitive way, and identify anomalous and data patterns that could be useful when choosing the model."
      ],
      "metadata": {
        "id": "y-8mdEVQLSKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã™ã‚‹\n",
        "\n",
        "çµ±è¨ˆæƒ…å ±ãŒå¤§åˆ‡ã§ã‚ã‚‹ã¨åŒæ™‚ã«ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚‚å¤§åˆ‡ãªéµã¨ãªã£ã¦ãã¾ã™ã€‚\n",
        "ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã¯è¦–è¦šçš„ãƒ»ç›´æ„Ÿçš„ã«æƒ…å ±ã¨çµæœã‚’å…±æœ‰ã™ã‚‹ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã¶ã¨ãã«æœ‰ç”¨ãªã€ç•°å¸¸ã‚„ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "gkdLoH9aAwgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bar charts and histograms\n",
        "Bar charts and histograms can be used to see how our data is distributed and how many instances there are of each class, this way we can identify imbalances in the classes and avoid overfitting in the models\n",
        "\n",
        "To create a histogram with seaborn, you can use the code:\n",
        "\n",
        "\n",
        "```\n",
        "sns.histplot(data=df, x=\"column_name\", kde=False) #optional kwarg: hue=\"column_2\"\n",
        "plt.title('Title')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0L5teBSObmED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### æ£’ã‚°ãƒ©ãƒ•ã¨ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n",
        "\n",
        "æ£’ã‚°ãƒ©ãƒ•ã‚„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½¿ã†ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚„å„ã‚¯ãƒ©ã‚¹ã®ä»¶æ•°ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "ã“ã®ã‚ˆã†ã«ã—ã¦ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã‚’è¦‹ã¤ã‘ã€éå­¦ç¿’ã‚’é˜²ãã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "seabornã§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆã™ã‚‹ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "```\n",
        "sns.histplot(data=df, x=\"column_name\", kde=False) #optional kwarg: hue=\"column_2\"\n",
        "plt.title('Title')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hDVbBnSkT6In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Plot the distribution by race group"
      ],
      "metadata": {
        "id": "Op0QYpYxMvp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ äººç¨®åˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆ"
      ],
      "metadata": {
        "id": "9NdYcQ9RXpzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "nIarH7T8bmbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Plot the distribution by gender"
      ],
      "metadata": {
        "id": "WP4GUREQM_0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ æ€§åˆ¥åˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆ"
      ],
      "metadata": {
        "id": "n-lruYd8X1of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "21beiKNxcJGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Plot the distribution by age and race group"
      ],
      "metadata": {
        "id": "47abe3cjNCiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ å¹´é½¢ã¨äººç¨®ã®åˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆ"
      ],
      "metadata": {
        "id": "VWvLPN3UX_lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "gKDp62Uldi6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Box plots and violin plots\n",
        "**Outilers â—â—â—**: While in some cases there are no null values, could be outliers values. These outliers are sometimes indicators of biases in data collection, or social/cultural biases. However, in some cases the outliers are erroneous values during data collection and storage. Identifying values such as very high SaO2 (99999) or negative is an indicator in many cases of missing values and should be taken as such.\n",
        "\n",
        "Bar and violin charts are one of the best ways to identify outliers, because they show us around which values most of our data is clustered."
      ],
      "metadata": {
        "id": "oRt_xYq87hLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ç®±ã²ã’å›³ã¨ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "**å¤–ã‚Œå€¤ â—â—â—**: æ¬ æå€¤ã¯ãªãã¦ã‚‚ã€å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®å¤–ã‚Œå€¤ã¯ã€ãƒ‡ãƒ¼ã‚¿åé›†ã®åã‚Šã‚„ç¤¾ä¼šçš„ãƒ»ç¤¾ä¼šçš„åã‚Šã®æŒ‡æ¨™ã¨ãªã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ãŒã€æ™‚ã«ã¯ãƒ‡ãƒ¼ã‚¿åé›†æ™‚ã€ä¿å­˜æ™‚ã®èª¤ã£ãŸå€¤ã§ã‚ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
        "éå¸¸ã«é«˜ã„SaO2(99999)ã‚„è² ã®å€¤ãªã©ã®è­˜åˆ¥ã¯ã€å¤šãã®å ´åˆæ¬ æå€¤ã®æŒ‡æ¨™ã§ã‚ã‚Šã€ãã®ã‚ˆã†ã«æ‰±ã†ã¹ãã§ã™ã€‚\n",
        "\n",
        "æ£’ã‚°ãƒ©ãƒ•ã‚„ãƒã‚¤ã‚ªãƒªãƒ³ãƒ»ãƒãƒ£ãƒ¼ãƒˆã¯ç•°å¸¸å€¤ã‚’è­˜åˆ¥ã™ã‚‹æœ€è‰¯ã®æ–¹æ³•ã®ä¸€ã¤ã§ã™ã€‚ãªãœãªã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã®ãŒã©ã®å€¤ã®å‘¨ã‚Šã«é›†ã¾ã£ã¦ã„ã‚‹ã‹ã‚’ç¤ºã—ã¦ãã‚Œã‚‹ã‹ã‚‰ã§ã™ã€‚\n"
      ],
      "metadata": {
        "id": "-yJZbTinYhzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_groupby(df: pd.DataFrame, group1: str, group2: str, variable: str, barplot=True, boxplot=True, violinplot=False) -> None:\n",
        "  \"\"\"\n",
        "  ã“ã®é–¢æ•°ã¯ã€data frameå†…ã®2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®å¤‰æ•°ã®å¹³å‡å€¤ã‚’æ¯”è¼ƒã™ã‚‹2ã¤ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚\n",
        "  ã“ã®é–¢æ•°ã§ã¯ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¤ã„ã¦æ£’ã‚°ãƒ©ãƒ•ã€ç®±ã²ã’å›³ã€ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã®ã„ãšã‚Œã‹ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "  æœ€åˆã®ãƒ—ãƒ­ãƒƒãƒˆã¯æœ€åˆã®ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡ã‚’æ¯”è¼ƒã—ã€2ç•ªç›®ã®ãƒ—ãƒ­ãƒƒãƒˆã¯2ç•ªç›®ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\n",
        "\n",
        "  å¼•æ•° (å‹å)ï¼š\n",
        "    df (pd.DataFrame): æ‰±ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "    group1 (string): ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹æœ€åˆã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åå‰\n",
        "    group2 (string): ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹2ç•ªç›®ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åå‰\n",
        "    variable (string): ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹æ•°å€¤å¤‰æ•°ã®åå‰\n",
        "    barplot (boolean): æ£’ã‚°ãƒ©ãƒ•ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã‹ã©ã†ã‹(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯True) *\n",
        "    boxplot (boolean): ç®±ã²ã’å›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã‹ã©ã†ã‹(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯True) *\n",
        "    violinplot (boolean): ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã‹ã©ã†ã‹(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯False)*\n",
        "    *ï¼šçœç•¥å¯èƒ½\n",
        "\n",
        "  æˆ»ã‚Šå€¤ï¼š\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  avg_by_group1 = df.groupby(group1)[variable].mean()\n",
        "  avg_by_group2 = df.groupby(group2)[variable].mean()\n",
        "\n",
        "  if barplot:\n",
        "    # Bar plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    print('Bar Plot: ')\n",
        "    ax1.bar(avg_by_group1.index, avg_by_group1.values)\n",
        "    ax1.set_xlabel(group1)\n",
        "    ax1.set_ylabel(f'Average {variable}')\n",
        "    ax2.bar(avg_by_group2.index, avg_by_group2.values)\n",
        "    ax2.set_xlabel(group2)\n",
        "    ax2.set_ylabel(f'Average {variable}')\n",
        "    plt.show()\n",
        "\n",
        "  if boxplot:\n",
        "    # Create box plots to visualize the distribution of SaO2 values by gender and ethnicity\n",
        "    print('Box Plot: ')\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.boxplot(x=group1, y=variable, hue=group2, data=df, ax=ax)\n",
        "    plt.show()\n",
        "\n",
        "  if violinplot:\n",
        "    print('Violin Plot: ')\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.violinplot(x=group1, y=variable, hue=group2, data=df, ax=ax)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IOWP5b6IZSyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Plot the distribution of SaO2 by gender and race group"
      ],
      "metadata": {
        "id": "Mhi4eVrrhzJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ æ€§åˆ¥ãƒ»äººç¨®åˆ¥ã®**SaO2**ã®ãƒ—ãƒ­ãƒƒãƒˆ"
      ],
      "metadata": {
        "id": "Qe3DDEinj7KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot_groupby()"
      ],
      "metadata": {
        "id": "kd8Y6R_P9qag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Plot the distribution of SpO2 by gender and race group"
      ],
      "metadata": {
        "id": "5VbxyRjYh6UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ æ€§åˆ¥ãƒ»äººç¨®åˆ¥ã®**SpO2**ã®ãƒ—ãƒ­ãƒƒãƒˆ"
      ],
      "metadata": {
        "id": "TfbJjrjqkF7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot_groupby()\n"
      ],
      "metadata": {
        "id": "mcbHq8-0jIMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Plot the distribution of any Time Offset by gender and race group (Use violin plot)\n",
        "\n",
        "Hint: You can use the next line of code to get all the variables related to a time offset"
      ],
      "metadata": {
        "id": "_IApC0RKfjcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ æ€§åˆ¥ãƒ»äººç¨®ã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«ã‚¿ã‚¤ãƒ ã‚ªãƒ•ã‚»ãƒƒãƒˆ(è¨ˆæ¸¬ã•ã‚ŒãŸæ™‚åˆ»ã®å·®)ã®åˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆ(ãƒ´ã‚¡ã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½¿ã†)\n",
        "\n",
        "ãƒ’ãƒ³ãƒˆ: ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ç”¨ã„ã¦ã‚¿ã‚¤ãƒ ã‚ªãƒ•ã‚»ãƒƒãƒˆã«é–¢é€£ã™ã‚‹å¤‰æ•°ã‚’å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "HQ9l9QROkTBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df.columns:\n",
        "  if 'delta' in column:\n",
        "    print(column)"
      ],
      "metadata": {
        "id": "OSwa5fEdhQT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot_groupby()"
      ],
      "metadata": {
        "id": "QCGZGTHfeuiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing / å‰å‡¦ç†\n",
        "---"
      ],
      "metadata": {
        "id": "keW_Fk-gruDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that there are some outliers so let's assume that those values are missing values and then analyze the amount of missing values for preprocessing"
      ],
      "metadata": {
        "id": "OQxBAUV7SCcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å‰ç« ã®ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚Šã„ãã¤ã‹ã®å¤–ã‚Œå€¤ãŒã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚‰ã®å€¤ã¯æ¬ æå€¤ã¨ã—ã¦ä»®å®šã—ã¦ã€å‰å‡¦ç†ã®ãŸã‚ã«æ¬ æå€¤ã®æ•°ã‚’åˆ†æã—ã¾ã—ã‚‡ã†ã€‚\n"
      ],
      "metadata": {
        "id": "XrvY8MdwpLUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing values\n",
        "\n",
        "Let's see which columns have variables with missing data"
      ],
      "metadata": {
        "id": "Th51UTXwSRYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¬ æå€¤ã‚’è¦‹ã¤ã‘ã‚‹\n",
        "\n",
        "ã©ã®åˆ—ã«æ¬ æå€¤ãŒã‚ã‚‹ã®ã‹ã‚’æ¢ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "13yPhjmvrBmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "cW7ysOZGeunN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ detect columns with null values\n",
        "Use the function `null_values` to see the columns with missing data and the number of rows missing.\n",
        "\n",
        "if the number of values with missing data is very high compared to the size of the dataset (E.g. 85%), in some cases it is better just to remove them"
      ],
      "metadata": {
        "id": "OcfF2HSqOM3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ nullã‚’å«ã‚€åˆ—ã‚’ç‰¹å®šã™ã‚‹\n",
        "`null_values` é–¢æ•°ã‚’ç”¨ã„ã¦æ¬ æå€¤ã®ã‚ã‚‹åˆ—ã¨ãã®è¡Œç•ªå·ã‚’æ±‚ã‚ã¾ã™ã€‚\n",
        "\n",
        "if the number of values with missing data is very high compared to the size of the dataset (E.g. 85%), in some cases it is better just to remove them\n",
        "\n",
        "ã‚‚ã—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤§åŠ(E.g. 85%)ãŒæ¬ æã—ã¦ã„ãŸå ´åˆã€ãã®åˆ—ã¯çœã„ã¦ã—ã¾ã£ãŸã»ã†ãŒã„ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ],
      "metadata": {
        "id": "0nrgrXKurTpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### List of columns with missing values ###\n",
        "### Code here ###\n",
        "null_values(df)"
      ],
      "metadata": {
        "id": "DNVsJ8FRSJ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split\n",
        "Dividing your dataset into training and test is important, since what we want is for the model to learn, not memorize.\n",
        "We want to test the behavior of the model on unknown data. For that reason, the dataset should always be divided into training and test.\n",
        "\n",
        "The way to divide it may vary... You can use 60% (training) / 40% (testing), or 70% (training) / 30% (testing), or even 50/50. The most important part is that you **make sure that in the test set you have a well-distributed data set that covers all the possibilities.**"
      ],
      "metadata": {
        "id": "LwRUZnXdsMB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰²ã™ã‚‹\n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã¯çµæœã«é–¢ã‚ã‚‹é‡è¦ãªäº‹ã§ã™ã€‚(ãªãœãªã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã«æ±‚ã‚ã¦ã„ã‚‹ã“ã¨ã¯\"æš—è¨˜\"ã§ã¯ãªã\"å­¦ç¿’\"ã ã‹ã‚‰ã§ã™)\n",
        "\n",
        "ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã«ã‚ãŸã‚Šã€æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«ã€ä»Šã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
        "\n",
        "åˆ†å‰²ã®æ¯”ç‡ã¯è‡ªç”±ã§ã™ã€‚è¨“ç·´ç”¨:ãƒ†ã‚¹ãƒˆç”¨ã‚’6:4ã€7:3ã€5:5ãªã©æ±ºã¾ã‚Šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã®æ™‚ã«æ°—ã‚’ä»˜ã‘ã¦é ‚ããŸã„ã“ã¨ã¯**ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã«åã‚ŠãŒãªãã€å…¨ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¶²ç¾…ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã“ã¨**ã§ã™ã€‚"
      ],
      "metadata": {
        "id": "T3dPEzpMsglU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "def train_test_split_with_id(df: pd.DataFrame, test_size: float, id_column: str, stratify: array_like=None, random_state: Optional[int]=42) -> Tuple[pd.DataFrame]:\n",
        "  \"\"\"\n",
        "  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã—ã€ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŒã˜IDã‚’æŒã¤è¡ŒãŒç¾ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹é–¢æ•°ã€‚\n",
        "\n",
        "  å¼•æ•° (å‹å):\n",
        "    df (pandas.DataFrame): åˆ†å‰²ã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "    test_size (float): ãƒ†ã‚¹ãƒˆç”¨ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ(ex 0.3)\n",
        "    id_column (string): æ‚£è€…ã®è­˜åˆ¥IDã‚’å«ã‚€åˆ—ã®åå‰ (è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åŒã˜æ‚£è€…ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“)\n",
        "    stratify (array-like): å±¤åˆ¥åŒ–(å„ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã‚’æƒãˆã‚‹ãŸã‚)ã«ä½¿ã†é–¢æ•°ã€‚ç‰¹ã«æŒ‡å®šã—ãªã„(Noneã®)å ´åˆã€å±¤åˆ¥åŒ–ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
        "    random_state (int): åˆ†å‰²ã«ä½¿ç”¨ã™ã‚‹ä¹±æ•°ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®æ•´æ•°(ã‚ªãƒ—ã‚·ãƒ§ãƒ³).\n",
        "\n",
        "  æˆ»ã‚Šå€¤ (å‹å):\n",
        "    tuple (pandas.DataFrames): è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã®2ã¤ã®ã‚¿ãƒ—ãƒ«\n",
        "  \"\"\"\n",
        "  if stratify:\n",
        "    id_groups = df.groupby(id_column)\n",
        "    ids = []\n",
        "    ys = []\n",
        "    for _, group in id_groups:\n",
        "        ids.append(group[id_column].iloc[0])\n",
        "        ys.append(group[stratify].iloc[0])\n",
        "    ids = np.array(ids)\n",
        "    ys = np.array(ys)\n",
        "\n",
        "    train_ids, test_ids, train_ys, test_ys = train_test_split(ids, ys, test_size=test_size, stratify=ys, random_state=random_state)\n",
        "\n",
        "  else:\n",
        "    unique_ids = df[id_column].unique()\n",
        "    train_ids, test_ids = train_test_split(unique_ids, test_size=test_size, random_state=random_state)\n",
        "\n",
        "  train = df[df[id_column].isin(train_ids)]\n",
        "  test = df[df[id_column].isin(test_ids)]\n",
        "\n",
        "  print(f'The train shape is {train.shape}')\n",
        "  print(f'The test shape is {test.shape}')\n",
        "\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "BVzkNkfFsOhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Split your data\n",
        "Use the provided function to split your data into train and test. We have to split the dataset now so we can avoid data leakage during data imputation and data normalization"
      ],
      "metadata": {
        "id": "RcbYiAi0PFP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹\n",
        "å…ˆã»ã©å®šç¾©ã—ãŸ`train_test_split_with_id`é–¢æ•°ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰²ã—ã¾ã™ã€‚\n",
        "ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã«ã‚ˆã‚Šã€æ¬ æå€¤è£œå®Œã‚„æ­£è¦åŒ–ã®éš›ã®ãƒ‡ãƒ¼ã‚¿ã®æ¼ã‚Œã‚’é˜²ãã“ã¨ãŒã§ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "cchPehBrzZi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "test_size =\n",
        "id_column = # Avoid same patient in train and test!!!\n",
        "stratify =\n",
        "\n",
        "#train, test = train_test_split_with_id('...')"
      ],
      "metadata": {
        "id": "L60Ly3JNxAZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Imputation\n",
        "\n",
        "There are various imputation methods of different complexity, ranging from imputing using the most common value (mode) (usually used for categorical variables) or the average or median value depending on the distribution of the data (usually used for numerical variables), to imputation methods using machine learning algorithms to predict the missing value using the other variables as a reference. While any method of data imputation can be valid, you have to take care to avoid data leakage or adding bias when doing this process."
      ],
      "metadata": {
        "id": "TGfy5tf4SndZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have the columns with missing data, let's divide the columns into two sub groups:\n",
        "1. Categorical columns with missing\n",
        "2. Numerical columns with missing"
      ],
      "metadata": {
        "id": "Vsi0qe3cQzmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¬ æå€¤è£œå®Œ\n",
        "\n",
        "æ¬ æå€¤è£œå®Œã®æ–¹æ³•ã«ã¯å˜ç´”ãªã‚‚ã®ã‹ã‚‰è¤‡é›‘ãªã‚‚ã®ã¾ã§ã€æ§˜ã€…ãªæ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "æœ€é »å€¤(é€šå¸¸ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«ä½¿ç”¨)ã‚’ç”¨ã„ã‚‹æ–¹æ³•ã‚„ã€ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã«å¿œã˜ãŸå¹³å‡å€¤ã¾ãŸã¯ä¸­å¤®å€¤(é€šå¸¸ã€æ•°å€¤å¤‰æ•°ã«ä½¿ç”¨)ã‚’ç”¨ã„ã‚‹æ–¹æ³•ã®ä»–ã«ã‚‚ã€ä»–ã®å¤‰æ•°ã‚’å‚ç…§ã—ã¦å€¤ã‚’äºˆæ¸¬ã—ãªãŒã‚‰è£œå®Œã™ã‚‹ã€æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸæ–¹æ³•ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã©ã®ã‚ˆã†ãªæ–¹æ³•ã‚’ç”¨ã„ã¦ã‚‚æœ‰ç”¨ãªè£œå®ŒãŒå¯èƒ½ã§ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã®æ¼ã‚Œã‚„ãƒã‚¤ã‚¢ã‚¹ã®ä»˜ä¸ã‚’è€ƒæ…®ã—ã¦è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "MaHRF6uX1oKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¬ æå€¤ã®ã‚ã‚‹åˆ—ã‚’2ã¤ã®ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã—ã¾ã™:\n",
        "1. ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°åˆ—ã®æ¬ æ\n",
        "2. æ•°å€¤å¤‰æ•°åˆ—ã®æ¬ æ"
      ],
      "metadata": {
        "id": "gTZVVJYd6KeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Get numerical, categorical columns and columns with missing data\n",
        "\n",
        "Hint: You can use the function `get_categorical_numerical_variables` to get the list of categorical and mumerical columns.\n",
        "\n",
        "Hint 2: You can use the function `null_values` as part of the logic to get the columns with missing data"
      ],
      "metadata": {
        "id": "CPJYmaBJQUIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ æ•°å€¤å¤‰æ•°ã‚’æŒã¤åˆ—ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã‚’æŒã¤åˆ—ã€æ¬ æå€¤ã‚’æŒã¤åˆ—ã‚’å–å¾—ã™ã‚‹\n",
        "\n",
        "ãƒ’ãƒ³ãƒˆ1: `get_categorical_numerical_variables` é–¢æ•°ã‚’ç”¨ã„ã¦æ•°å€¤å¤‰æ•°ã¨ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã‚’æŒã¤åˆ—ã®ãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã™ã€‚\n",
        "\n",
        "ãƒ’ãƒ³ãƒˆ 2: `null_values` é–¢æ•°ã¯æ¬ æå€¤ã®ã‚ã‚‹åˆ—ã‚’å–å¾—ã™ã‚‹æ–¹æ³•ã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "C6tf68HE6prx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numerical and categorical columns:\n",
        "categorical_cols, numerical_cols ="
      ],
      "metadata": {
        "id": "qExWrf1JxPoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the columns with missing data\n",
        "# columns_missing =\n",
        "columns_missing = train.isnull().sum()[train.isnull().sum() > 0].index\n",
        "columns_missing"
      ],
      "metadata": {
        "id": "0RLwYEoKynP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_data_imputation(df: pd.DataFrame, impute_cols: List[str], test_df: Optional[pd.DataFrame]=None, groupby_cols: List[str]=[], method: str='new_category_numeric', imputation_model=LinearRegression()):\n",
        "  \"\"\"\n",
        "  å¿…è¦ã«å¿œã˜ã¦ä¸ãˆã‚‰ãˆãŸåˆ—(impute_cols)ã« groupby ã‚’é©ç”¨ã—ãŸå¾Œã«ã€åˆ—æ¯ã®æ¬ æå€¤è£œå®Œ(Data Imputation)ã‚’è¡Œã†.\n",
        "  !!! æ³¨æ„äº‹é … !!!\n",
        "  ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã«å¯¾ã—ã¦new_categoryé–¢æ•°ã‚’ã€æ•°å€¤å¤‰æ•°ã«å¯¾ã—ã¦new_category_numericé–¢æ•°ä»¥å¤–ã‚’é©ç”¨ã™ã‚‹å ´åˆä»¥å¤–ã¯ã€å¿…ãšãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²å¾Œã«è¡Œã†ã“ã¨ã€‚ãã†ã—ãªã„ã¨ãƒ‡ãƒ¼ã‚¿æ¼ã‚Œã‚’æ‹›ãæã‚ŒãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "  å¼•æ•°(å‹å):\n",
        "    df(pd.DataFrame): æ¬ æå€¤è£œå®Œã‚’è¡Œã†ãƒ‡ãƒ¼ã‚¿\n",
        "    impute_cols(list): æ¬ æå€¤è£œå®Œã‚’é©ç”¨ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆ\n",
        "    test_df(pd.DataFrame): è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ã¦æ¬ æå€¤è£œå®Œã‚’è¡Œã†ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
        "    groupby_cols(list): method='median', 'mean', 'mode'ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹ãŸã‚ã®ã‚«ãƒ©ãƒ åã€ method='model'ã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’è¡¨ã™ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆ (E.g.: groupby_cols=['race_group', 'gender']).\n",
        "    method(string): æ¬ æå€¤è£œå®Œã®æ–¹æ³•ã€‚ 'median'ã€'mean'ã€'mode'ã€'new_category'ã€'new_category_numeric'ã€'model'ã®6ã¤ãŒé¸æŠå¯èƒ½\n",
        "    model: methodãŒ'model'ã®å ´åˆã«æ¬ æå€¤ã®äºˆæ¸¬ã«ç”¨ã„ã‚‹sklearnãƒ¢ãƒ‡ãƒ«\n",
        "\n",
        "  æˆ»ã‚Šå€¤:\n",
        "    æ¬ æå€¤ã®ã‚ã‚‹åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œå¾Œã®ãƒ‡ãƒ¼ã‚¿\n",
        "  \"\"\"\n",
        "  imputer = {}\n",
        "\n",
        "  for col in impute_cols:\n",
        "\n",
        "    # Group the data and calculate the method (E.g. Median) for each group\n",
        "    if method == 'median':\n",
        "      # Impute using median (for numerical values)\n",
        "      imputation_values = df.groupby(groupby_cols)[col].median()\n",
        "\n",
        "    elif method == 'mean':\n",
        "      # Impute using mean (for numerical values)\n",
        "      imputation_values = df.groupby(groupby_cols)[col].mean()\n",
        "\n",
        "    elif method == 'mode':\n",
        "      # Impute using mode (for categorical values)\n",
        "      imputation_values = df.groupby(groupby_cols)[col].agg(pd.Series.mode)\n",
        "\n",
        "    elif method == 'new_category':\n",
        "      # Add a new category 'None' in the dataset (for categorical values)\n",
        "      new_category_val = 'None'\n",
        "      imputer[col] = new_category_val\n",
        "      df[col] = df[col].fillna(new_category_val)\n",
        "      if test_df is not None:\n",
        "        test_df[col] = test_df[col].fillna(new_category_val)\n",
        "\n",
        "\n",
        "    elif method == 'new_category_numeric':\n",
        "      # Add an anomalous data in the dataset (for numerical values)\n",
        "      new_category_val = 0\n",
        "      # If there's not 0, replace nan with 0\n",
        "      if not((df[col] == 0).any()):\n",
        "        new_category_val = 0\n",
        "      # If there's not positive numbers, replace nan with 1\n",
        "      elif not((df[col] >= 0).any()):\n",
        "        new_category_val = 1\n",
        "      # If there's not negative numbers, replace nan with 1\n",
        "      elif not((df[col] <= 0).any()):\n",
        "        new_category_val = -1\n",
        "      else:\n",
        "        # Replace with min value - 100\n",
        "        new_category_val = df[col].min() - 100\n",
        "\n",
        "      imputer[col] = new_category_val\n",
        "      df[col] = df[col].fillna(new_category_val)\n",
        "      if test_df is not None:\n",
        "        test_df[col] = test_df[col].fillna(new_category_val)\n",
        "\n",
        "\n",
        "    elif method == 'model':\n",
        "      # If the variable is categorical, convert to numeric:\n",
        "      if df[col].dtype == 'object':\n",
        "        encoder = LabelEncoder()\n",
        "        df[col] = encoder.fit_transform(df[col].dropna())\n",
        "\n",
        "      # Create a linear regression model to impute missing values\n",
        "      model = imputation_model\n",
        "\n",
        "      # Get the data with complete column\n",
        "      complete_data = df.dropna(subset=[col])\n",
        "\n",
        "      # Encode categorical columns if needed in x data\n",
        "      le = {}\n",
        "      for col_group in groupby_cols:\n",
        "        if df[col_group].dtype == 'object':\n",
        "          le[col_group] = LabelEncoder()\n",
        "          complete_data[col_group] = le[col_group].fit_transform(complete_data[col_group])\n",
        "\n",
        "\n",
        "      # Fit the model on the complete data\n",
        "      X = complete_data[groupby_cols]\n",
        "      # Replace any remaining NaNs with the column mean\n",
        "      X = X.fillna(X.mean())\n",
        "      y = complete_data[col]\n",
        "      model.fit(X, y)\n",
        "\n",
        "      if df[col].dtype == 'object':\n",
        "        imputer[col] = (model, encoder, le)\n",
        "      else:\n",
        "        imputer[col] = (model, le)\n",
        "\n",
        "      ### Impute column using the model to predict the value:\n",
        "      missing_data = df[df[col].isna()].copy()\n",
        "      if test_df is not None:\n",
        "        missing_data_test = test_df[test_df[col].isna()].copy()\n",
        "\n",
        "      # Encode categorical columns if needed in x data\n",
        "      for col_group in groupby_cols:\n",
        "        if missing_data[col_group].dtype == 'object':\n",
        "          missing_data[col_group] = le[col_group].transform(missing_data[col_group].dropna())\n",
        "          if test_df is not None:\n",
        "            missing_data_test[col_group] = le[col_group].transform(missing_data_test[col_group].dropna())\n",
        "\n",
        "      for index, row in missing_data.iterrows():\n",
        "        values = row[groupby_cols].values.reshape(1, -1)\n",
        "        imputed_value = model.predict(values)\n",
        "        df.at[index, col] = imputed_value[0]\n",
        "\n",
        "      if test_df is not None:\n",
        "        for index, row in missing_data_test.iterrows():\n",
        "          values = row[groupby_cols].values.reshape(1, -1)\n",
        "          imputed_value = model.predict(values)\n",
        "          test_df.at[index, col] = imputed_value[0]\n",
        "\n",
        "      if method in ['median', 'mean', 'mode']:\n",
        "        imputer[col] = imputation_values\n",
        "        # Fill missing values with the method of the corresponding group\n",
        "        df[col] = df.apply(lambda x: imputation_values[tuple(x[groupby_cols])] if pd.isna(x[col]) else x[col], axis=1)\n",
        "        if test_df is not None:\n",
        "          test_df[col] = test_df.apply(lambda x: imputation_values[tuple(x[groupby_cols])] if pd.isna(x[col]) else x[col], axis=1)\n",
        "\n",
        "  if test_df is not None:\n",
        "    return df, test_df, imputer\n",
        "  else:\n",
        "    return df, imputer\n"
      ],
      "metadata": {
        "id": "dqZXomYXPa3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Apply data imputation to categorical variables\n",
        "\n",
        "1. Get from the categorical variables the list of categorical columns with missing data\n",
        "2. Use the function `apply_data_imputation` to impute the variables of train and test data. You can also use other methods of pandas or sklearn. But think about the possible bias that those methods could be adding to the dataset."
      ],
      "metadata": {
        "id": "nTFDHd8RKbqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã«æ¬ æå€¤è£œå®Œã‚’é©ç”¨ã™ã‚‹\n",
        "\n",
        "1. æ¬ æå€¤ã®ã‚ã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ•°ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€æ¬ æå€¤è£œå®Œã‚’è¡Œã†åˆ—ã‚’å–å¾—ã™ã‚‹ã€‚\n",
        "2. `apply_data_imputation`é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤è£œå®Œã‚’è¡Œã†ã€‚pandasã‚„sklearnã®ä»–ã®æ–¹æ³•ã‚’ä½¿ã£ã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ãŒã€ãã‚Œã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ç”Ÿã˜ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã«ã¤ã„ã¦ã‚ˆãè€ƒãˆã¦ã‹ã‚‰é©ç”¨ã—ã¾ã—ã‚‡ã†ã€‚"
      ],
      "metadata": {
        "id": "KlMiqfK8eRMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get from columns with missing data the categorical columns\n",
        "categorical_cols_missing ="
      ],
      "metadata": {
        "id": "NCNfYwpNKXRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importantâ—â—â—** If you apply other technique than `new_category` on categorical variables. You should do it after train-test split otherwise you could be introducing a data leakage in the test set"
      ],
      "metadata": {
        "id": "90vB5xhDzJvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æ³¨æ„äº‹é …â—â—â—** ã‚‚ã—`new_category`é–¢æ•°ä»¥å¤–ã®æ–¹æ³•ã§æ¬ æå€¤è£œå®Œã‚’è¡Œã†å ´åˆã€å¿…ãš**è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã¨ã®åˆ†å‰²å¾Œã«**è¡Œã†ã“ã¨ã€‚ã“ã†ã—ãªã„ã¨ãƒ‡ãƒ¼ã‚¿æ¼ã‚Œã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "Ax_jWtgAg95P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method =  # 'mode', 'new_category'\n",
        "groupby_cols =  # E.g. ['race_group', 'gender'] or None if is new_category\n",
        "\n",
        "train, test, imputer = apply_data_imputation(train, impute_cols=categorical_cols_missing, method=method, groupby_cols=groupby_cols, test_df=test)"
      ],
      "metadata": {
        "id": "KSws7IRAScbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Apply data imputation to Numerical variables\n",
        "\n",
        "1. Get from the numerical variables the list of numerical columns with missing data\n",
        "2. Use the function `apply_data_imputation` to impute the variables of train and test data. You can also use other methods of pandas or sklearn. But think about the possible bias that those methods could be adding to the dataset."
      ],
      "metadata": {
        "id": "OHwYtppJLNfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Apply data imputation to Numerical variables\n",
        "\n",
        "1. æ¬ æå€¤ã®ã‚ã‚‹æ•°å€¤å¤‰æ•°ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€æ¬ æå€¤è£œå®Œã‚’è¡Œã†åˆ—ã‚’å–å¾—ã™ã‚‹ã€‚\n",
        "\n",
        "2. `apply_data_imputation`é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤è£œå®Œã‚’è¡Œã†ã€‚pandasã‚„sklearnã®ä»–ã®æ–¹æ³•ã‚’ä½¿ã£ã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ãŒã€ãã‚Œã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ç”Ÿã˜ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã«ã¤ã„ã¦ã‚ˆãè€ƒãˆã¦ã‹ã‚‰é©ç”¨ã—ã¾ã—ã‚‡ã†ã€‚"
      ],
      "metadata": {
        "id": "Y0C-1C9AjhLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get from columns with missing data the numerical columns\n",
        "numerical_cols_missing =\n"
      ],
      "metadata": {
        "id": "7TavY8buLUIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important â—â—â—** If you apply other technique than `new_category_numeric` on numerical variables. You should do it after train-test split otherwise you could be introducing a data leakage in the test set"
      ],
      "metadata": {
        "id": "ZQHc5wKdzsb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æ³¨æ„äº‹é …â—â—â—** ã‚‚ã—`new_category_numeric`é–¢æ•°ä»¥å¤–ã®æ–¹æ³•ã§ä»¥å¤–ã®æ–¹æ³•ã§è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã¨ã®åˆ†å‰²å¾Œã«è¡Œã†ã“ã¨ã€‚ã“ã†ã—ãªã„ã¨ãƒ‡ãƒ¼ã‚¿æ¼ã‚Œã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "eRk6ef7koi5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method = # 'median', 'mean', 'new_category_numeric', 'model'\n",
        "groupby_cols =  # E.g. ['race_group', 'gender'] or None if is new_category_numeric\n",
        "\n",
        "\n",
        "train, test, imputer = apply_data_imputation(train, impute_cols=numerical_cols_missing, method=method, groupby_cols=groupby_cols, test_df=test)"
      ],
      "metadata": {
        "id": "tuTNR8qtLV48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Let's check null values again\n",
        "\n",
        "Print the coluns with missing values (if any) in train and test set.\n",
        "\n",
        "There should be no columns with missing data, if so check the previous tasks"
      ],
      "metadata": {
        "id": "Xtkuor2lfY-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ nullå€¤ãŒãªã„ã‹ã®å†ç¢ºèª\n",
        "\n",
        "æ¬ æå€¤è£œå®Œã‚’è¡Œã£ãŸåˆ—(ã‚ã‚Œã°)ã‚’è¡¨ç¤ºã—ã¦ç¢ºèªã—ã¾ã™ã€‚\n",
        "\n",
        "ã“ã®æ™‚ç‚¹ã§æ¬ æå€¤ã®ã‚ã‚‹åˆ—ã¯ãªã„ã¯ãšã§ã™ã€‚ã‚‚ã—æ¬ æãŒã‚ã‚‹å ´åˆã¯ã“ã“ã¾ã§ã®å‡¦ç†ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚\n"
      ],
      "metadata": {
        "id": "O8EL_b69osZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### List of columns with mussing values ###\n",
        "print('#'*20, ' Null train: ', '#'*20)\n",
        "null_values(train)\n",
        "### List of columns with mussing values ###\n",
        "print('#'*20, ' Null test: ', '#'*20)\n",
        "null_values(test)"
      ],
      "metadata": {
        "id": "J1C2rHdzDVIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the file!\n",
        "Finally let's save the dataset as a csv file! ğŸ˜€"
      ],
      "metadata": {
        "id": "hKqDpYhNpLcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜\n",
        "æœ€å¾Œã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’csvãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ã‚‡ã†ï¼ ğŸ˜€"
      ],
      "metadata": {
        "id": "m3Zn1DxBpiaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ Save your train and test dataset as a csv file"
      ],
      "metadata": {
        "id": "GzWHbaIbXx6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### âœï¸ è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’csvãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ã‚‡ã†"
      ],
      "metadata": {
        "id": "sgZtNmUppyjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save_train = \"path/train.csv\"\n",
        "path_to_save_test = \"path/test.csv\"\n"
      ],
      "metadata": {
        "id": "BD4fUTkT5dUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to create the directory if it doesn't exist\n",
        "def create_directory_if_not_exists(path):\n",
        "    directory = os.path.dirname(path)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Save function to be used for both train and test datasets\n",
        "def save_dataset(dataset, path_to_save):\n",
        "    create_directory_if_not_exists(path_to_save)\n",
        "    dataset.to_csv(path_to_save, index=False)"
      ],
      "metadata": {
        "id": "y8ElZGp-7sDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export files:\n",
        "save_dataset(train, path_to_save_train)\n",
        "save_dataset(test, path_to_save_test)"
      ],
      "metadata": {
        "id": "zXD403Pto7WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Acknowledgement\n",
        "Original Notebook by David Restrepo, Adrien Carrel, and Jack Gallifant<br>\n",
        "Translated into Japanese by é«˜ç ‚èŒ‰è‰èŠ±ï¼ˆMarika Takasagoï¼‰"
      ],
      "metadata": {
        "id": "JKKAqBMzwAIC"
      }
    }
  ]
}