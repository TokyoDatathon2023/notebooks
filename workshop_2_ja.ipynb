{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop 2: Clinical Variables Selection & Feature Engineering / 臨床変数の選択と特徴量エンジニアリング"
      ],
      "metadata": {
        "id": "M65WOuF6u62h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯 Workshop Goals\n",
        "The goal of this workshop is to provide participants with a deep understanding of data preprocessing in the data science workflow. Specifically, by the end of this workshop, participants should be able to:\n",
        "\n",
        "1. **Understand the importance of Data Preprocessing Techniques**: Understanding thes significance of data preprocessing in the data science workflow. This includes to be able to apply common techniques such as cleaning, normalization, transformation, and reduction of data. This also includes handling missing data, outliers, skewed data, and data with different scales.\n",
        "\n",
        "3. **Familiarize with Data Pre-processing Techniques**: such as feature scaling, dimensionality reduction, and feature engineering.\n",
        "\n",
        "4. **Apply data pre-processing techniques**: This involved the practical application of data preprocessing techniques to real-world datasets and to be able to evaluate the impact of different preprocessing techniques on machine learning model performance.\n",
        "\n",
        "Be aware of the potential biases that can be introduced in data preprocessing, and how to identify and mitigate them.\n",
        "Throughout the workshop, participants will engage in hands-on activities, case studies, and real-world examples. They will work in groups to apply the concepts learned to real datasets, and engage in discussions to share their experiences and insights. By the end of the workshop, participants should have gained a solid understanding of data preprocessing techniques and their importance in the data science workflow, and be able to apply these techniques to improve the performance of machine learning models."
      ],
      "metadata": {
        "id": "p-DviQlnd7vT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯このWorkshopの目標\n",
        "本ワークショップは、データの前処理についての理解を深めてもらうことを目的としています。\n",
        "本ワークショップを通じて学ぶこと：\n",
        "\n",
        "1. **データ前処理技術の重要性の理解**: データの前処理についての知識を会得します。一般的な技法であるデータ整理、正規化、変換、削減などができるようになることに加え、欠損データや外れ値、歪んだデータ、異なるスケールを持つデータへの対処も含みます。\n",
        "\n",
        "3. **データの前処理技術の習得**: 特徴量スケーリング、次元削減、特徴量エンジニアリングなど\n",
        "\n",
        "4. **データ前処理技術の適用**: 実際のデータにデータの前処理を施し、それぞれの処理が機械学習モデルの挙動に与える影響を評価します。\n",
        "\n",
        "データの前処理によって導入されうる潜在的なバイアスとそれを特定して軽減する方法を認識します。\n",
        "ワークショップを通じて、参加者の皆様には実戦的な活動やケーススタディ、実際の事例に取り組んでいただきます。グループに分かれて学んだ概要を実際のデータセットに適用し、ディスカッションを行って経験と洞察を共有します。ワークショップ終了時にはデータの前処理技術とその重要性についてしっかりと理解し、機械学習モデルの性能向上に活用できるようになります。"
      ],
      "metadata": {
        "id": "knt1O68zdxWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✏️ Expected Deliverables\n",
        "\n",
        "1. A report or dashboard summarizing the results of EDA, including visualizations and statistical summaries of the data distribution and correlations.\n",
        "\n",
        "2. A set of code scripts or pipelines that automate the data preprocessing process, making it easier and more efficient to apply these techniques to future datasets.\n",
        "\n",
        "3. A cleaned dataset that has undergone preprocessing techniques such as removal of duplicates, handling missing data, and dealing with outliers. The cleaned dataset should be ready to be fed into machine learning models."
      ],
      "metadata": {
        "id": "lHt_KgdnGilV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✏️ 最終目標\n",
        "\n",
        "1. EDA* の結果をデータの分布と相関について統計的にまとめたレポートやダッシュボードを作成し、データを可視化する\n",
        "\n",
        "2. データの前処理を自動化するためのコードかパイプラインのセットを作成し、将来的に使うデータセットに簡単かつ効率的に適用できるようにする\n",
        "\n",
        "3. 重複の除去、欠損データの処理、外れ値処理などを施してデータセットを整備し、機械学習モデルに入れる準備を整える\n",
        "\n",
        "\\* Exploratory Data Analysis: 探索的データ分析"
      ],
      "metadata": {
        "id": "Zqoq81Iaps7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❗ Highlighted Pitfall(s)\n",
        "1. Make sure to apply preprocessing steps only to the training data and avoid using information from the validation set to prevent data leakage.\n",
        "\n",
        "2. Check for potential biases that could be introduced or amplified by preprocessing techniques, and evaluate the impact of these techniques on different subgroups of the data.\n",
        "\n",
        "3. Carefully evaluate the appropriateness of different preprocessing techniques for a given dataset and ensure that the techniques are applied correctly to avoid incorrect preprocessing that could lead to poor model performance or incorrect conclusions about the data."
      ],
      "metadata": {
        "id": "EnR2K_EKGrEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❗ 注意事項\n",
        "1. データの漏洩を防ぐため、前処理に利用するデータは学習データのみで、検証データの情報を用いないようにすること。\n",
        "\n",
        "2. 前処理によって導入・強調されうる潜在的なバイアスを確認し、データの異なるサブグループにおけるこれらの処理技術の影響を評価する。\n",
        "\n",
        "3. 与えられたデータセットに対する様々な前処理技術の適切性を慎重に評価する。モデルの性能低下やデータに関する誤った結論につながる誤った前処理を避けるために、前処理が正しく適用されるようにする。"
      ],
      "metadata": {
        "id": "ZlnuhKzvp1fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add visual aid here (figures without preprocessing)\n",
        "前処理なしの図の追加"
      ],
      "metadata": {
        "id": "G3qRzUX_2qke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.  Setup environment / 環境構築**\n",
        "---"
      ],
      "metadata": {
        "id": "EffSajZhq7lo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otcVcnAOQEUH"
      },
      "outputs": [],
      "source": [
        "# Data reading in Dataframe format and data preprocessing\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For missing values\n",
        "import missingno as msno\n",
        "\n",
        "# Linear algebra operations\n",
        "import numpy as np\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif, mutual_info_regression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount user's Google Drive to Google Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Frc9RcmKR3Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive の中身が全部見られたくない人のためのアンマウント用\n",
        "# この場合データセットはランタイムにアップロードされるが、時間制限によって削除されるため注意すること\n",
        "\n",
        "# drive.flush_and_unmount('/content/drive')"
      ],
      "metadata": {
        "id": "03evg_cYdkC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **2. Data Analysis / データ分析**\n",
        "---"
      ],
      "metadata": {
        "id": "91xUTkd2rOUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we understand the problem, we have formed a multidisciplinary team, formulated the research question,  hypothesis, we have to start working with the data. What data do we have? How do they look? What distributions do they have?\n",
        "\n",
        "Let's remember that in data science, data is the most important thing, and whether or not we can solve a problem depends on the data quality. At the same time, understanding the data also helps us to have a clearer vision of what we are facing and in case the data is not very good, we can at least fix it.\n",
        "\n",
        "Specifically, understanding the problem and the data are the essential phases in a data science project. An error in this phase is much more critical than an error in the modeling and evaluation phases. We must bear in mind that machine learning is not a magical tool that solves any type of problem, but rather a mathematical/statistical tool that learns from what we teach it, therefore if the data has biases, the model will also have them."
      ],
      "metadata": {
        "id": "wsZkeFBG08pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "問題を理解したら、チームで研究課題を決めて仮説を立ててデータ処理に取り組みましょう。どんなデータがありますか？どのように見えますか？どんな分布を持っていますか？\n",
        "\n",
        "データサイエンスにおいてはデータが最も重要であり、課題解決はデータの質の良し悪しにかかっています。\n",
        "また、データを理解することは直面している問題を明確に捉えることに繋がります。さらに、データを理解することでデータの良くない部分を見つけ、対処をすることができます。\n",
        "\n",
        "問題とデータを理解することはデータサイエンスにおいては不可欠な要素です。\n",
        "この過程での失敗は後に出てくるモデリングや評価といった過程での失敗よりも重大なものとなります。\n",
        "機械学習とは我々が教えたことを学習する数学的/統計的な道具です。\n",
        "したがって、データにバイアスがあればモデルにもバイアスが生じます。\n",
        "**機械学習はどんな問題でも解決してくれる魔法の道具ではない**ということを頭に置いておきましょう。\n"
      ],
      "metadata": {
        "id": "gBRUjzAOdzIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the dataset\n",
        "\n",
        "Let's asume that you already have the dataset of workshop 1:"
      ],
      "metadata": {
        "id": "Hzzgjobkzz1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### データセットの読み込み\n",
        "\n",
        "workshop 1のデータセットを既に持っているものとします。"
      ],
      "metadata": {
        "id": "ZjRd4w2OfJxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Read the dataset and use the function null_values to see which columns has missing data."
      ],
      "metadata": {
        "id": "ewfSG4dLHFxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ データセットを読み込み、`null_values`関数を用いて欠損値のある列を探します。"
      ],
      "metadata": {
        "id": "CuHjFiIntohu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset and use the function null_values to see which columns has missing data.\n",
        "\n",
        "PATH = \"\"\n",
        "\n",
        "# Read file here:\n",
        "df = pd.read_csv(PATH)\n",
        "print(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns')"
      ],
      "metadata": {
        "id": "8F5j91Ii3WpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def null_values(df: pd.DataFrame) -> None:\n",
        "  \"\"\"\n",
        "  データ内に欠損値があるかどうかをチェックする関数。\n",
        "  欠損値がない場合: There aren't null values in the dataframe\n",
        "  欠損値がある場合: 該当箇所の列名と欠損値の数を出力する\n",
        "  \"\"\"\n",
        "  nulls = df.isnull().sum()\n",
        "  if nulls.sum() == 0:\n",
        "      print(\"There aren't null values in the dataframe\")\n",
        "  else:\n",
        "      print('Null values:')\n",
        "      print(nulls[nulls > 0])"
      ],
      "metadata": {
        "id": "T5NLdL3K1cib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there are null values here:\n",
        "null_values(df)"
      ],
      "metadata": {
        "id": "vwAy1XAHR3Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **3. Understand your Data / データを理解する**\n",
        "---\n"
      ],
      "metadata": {
        "id": "sKiLCcTe_1s2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the variables:\n",
        "\n",
        "The first step is to understand what variables your dataset has and how these variables are distributed. The columns and data types are described in [Variable Dictionary (GitHub Link)](https://github.com/TokyoDatathon2023/notebooks/blob/main/variable_dictionary.md).\n",
        "\n",
        "Discuss with your team about the problem and what variables are needed."
      ],
      "metadata": {
        "id": "zqwtcefTVxdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the variables:\n",
        "\n",
        "まずは、データセットがどのような変数を持ち、それらの変数がどのように分布しているのかを理解しましょう。\n",
        "\n",
        "データの変数一覧：[Variable Dictionary (GitHub Link)](https://github.com/TokyoDatathon2023/notebooks/blob/main/variable_dictionary.md).\n",
        "\n",
        "どの変数が必要かをチームで話し合いましょう。\n"
      ],
      "metadata": {
        "id": "pQsQaYbBqH3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select your variables\n",
        "**Task:** To simplify the task we will use a reduced group of variables for the following analysis. Ideally you should decide with your groups which variables you consider to be really important.\n",
        "\n",
        "The variables should be selected:\n",
        "1. Always thinking about the question to solve and the hypothesis.\n",
        "2. Using the clinical experience of the members of the group and if possible supported by literature.\n",
        "3. Using mathematical and statistical methods."
      ],
      "metadata": {
        "id": "ZEzfCAqiVRx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 変数の選択\n",
        "**タスク：**\n",
        "作業を単純化するために、これ以降の分析で使用する変数を減らして作業を進めます。\n",
        "重要な変数が何か、分析に必要な変数は何か、をグループで話し合いましょう。\n",
        "\n",
        "気を付けること\n",
        "1. 解決すべき課題と仮説を常に意識すること。\n",
        "2. グループのメンバーの臨床経験を基にすること。可能であれば文献による裏付けもとること。\n",
        "3. 数学的・統計的な手法を用いること。\n"
      ],
      "metadata": {
        "id": "_K4UPphxJxbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Select demographic variables:\n",
        "Due to the challenge we need some demographic information select the relevant columns:"
      ],
      "metadata": {
        "id": "npybroq6Ju2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 患者背景に関する人口統計変数の選択:\n",
        "課題解決のためにはいくつかの患者背景に関する人口統計学的情報が必要です。"
      ],
      "metadata": {
        "id": "4xcftuvbMeHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patient main information (include identifiers)\n",
        "patient_info = ['variable1', 'variable2', ... ]\n",
        "\n",
        "# Demographic variables\n",
        "demographic_variables = ['variable1', 'variable2', ... ]"
      ],
      "metadata": {
        "id": "ZIUc7QwLJuDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clinical variables selected were selected based on the variables that are most related with SaO2 and Spo2 and a correlation method was used to filter those variables.\n",
        "You can use other methods and criteria!"
      ],
      "metadata": {
        "id": "gZxXW_09KCOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SaO2とSpO2と関連性の高い変数を選択し、それらの変数をフィルタリングするために相関法を用います。もちろん、他の方法を採用することも可能です！"
      ],
      "metadata": {
        "id": "RMnIyfbVDOup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Select clinical variables\n",
        "\n",
        "1. Manual variable selection: Manually select with you team the variables that are clinically important. Don't worry if there are many, later we will use other methods to filter the variables"
      ],
      "metadata": {
        "id": "Ucr8SD4lW0Eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 臨床変数の選択\n",
        "\n",
        "1. 手動での変数選択: チームで話し合って、臨床的に重要な変数を手動で選択します。\n",
        "後で他の方法でフィルタリングを行うので、数が多くなっても気にせず選択してください。"
      ],
      "metadata": {
        "id": "_MLRB7YbOcWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Static clinical variables\n",
        "static_clinical_variables = ['variable1', 'variable2', ... ]\n",
        "\n",
        "# Temporal clinical variables (E.g. Sofa related variables)\n",
        "sofa_variables = ['variable1', 'variable2', ... ]\n",
        "temporal_clinical_variables = ['variable1', 'variable2', ... 'variable3'] + sofa_variables\n",
        "\n",
        "outcomes = ['variable1', 'variable2', ... ]\n",
        "\n",
        "treatment = ['variable1', 'variable2', ... ]"
      ],
      "metadata": {
        "id": "GLYKUYWtKjNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Variable filtering\n",
        "Let's now employ a few techniques to filter the variables. You should choose a technique to do it.\n",
        "\n",
        "Hint: To choose the variables most connected with the columns SpO2 and SaO2, we utilised the correlation as an example. You are not required to employ the identical column(s). Select the columns in accordance with your hypothesis."
      ],
      "metadata": {
        "id": "lhKfPsmRW8TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 変数のフィルタリング\n",
        "変数をフィルタリングする手法をいくつか使ってみましょう。\n",
        "どのフィルタリング手法を使うのかはチームで決定してください。\n",
        "\n",
        "ヒント: 先ほどは例として、SpO2列とSaO2列と最も関係のある変数を選択するために、相関を用いていましたが、チームで立てた仮説に基づいて適宜必要な列を選択して下さい。"
      ],
      "metadata": {
        "id": "cvgNoOalTMH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Union\n",
        "\n",
        "def get_most_correlated(df: pd.DataFrame, variable: str, n: int) -> List[str]:\n",
        "  \"\"\"\n",
        "  特定の変数に最も相関(正・負ともに)のある変数を出力する関数\n",
        "  引数(型名):\n",
        "    df(pandas dataframe): 計算対象の変数を含むデータ\n",
        "    variable(string): 相関を計算したい変数名(Eg. 'SaO2')\n",
        "    n(integer): 最も相関のある変数として取得したい変数の数を表す整数\n",
        "  戻り値:\n",
        "    最も相関(正・負ともに)の強い上位n個の変数名のリスト\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the correlation matrix\n",
        "  corr_matrix = df.corr()\n",
        "\n",
        "  # Print the correlation matrix\n",
        "  correlation = corr_matrix[variable].sort_values(ascending=False)\n",
        "  positive_correlated = correlation[:n]\n",
        "  inversely_correlated = correlation[-n:]\n",
        "  print('#'*40, f' {variable} ' , '#'*40)\n",
        "  print(f'The {n} most correlated variables to variable {variable} are: ')\n",
        "  print(positive_correlated)\n",
        "  print(f'The {n} most inversely correlated variables to variable {variable} are: ')\n",
        "  print(inversely_correlated)\n",
        "\n",
        "  return list(positive_correlated.index) + list(inversely_correlated.index)"
      ],
      "metadata": {
        "id": "bqsjG9cFIVTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: add another feature selection process to improve the model at a later stage\n",
        "array_like = Union[pd.DataFrame, np.ndarray, List, Any]\n",
        "\n",
        "outcome_var = \"mortality_in\"\n",
        "K = 7\n",
        "method = f_classif  # f_classif, chi2, mutual_info_classif, mutual_info_regression, etc\n",
        "\n",
        "def feat_select(features: List[str], df: pd.DataFrame, outcome_var: str, K: int, method: Any) -> array_like:\n",
        "  X = df[features]\n",
        "  y = df[outcome_var]\n",
        "  return SelectKBest(method, k=K).fit(X, y).get_feature_names_out()\n",
        "\n",
        "# resulting_temporal_clinical_variables = feat_select(resulting_temporal_clinical_variables, df, outcome_var, K, method)"
      ],
      "metadata": {
        "id": "Nlys9bNkJb2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The number of initial temporal clinical variables is {len(temporal_clinical_variables)}')\n",
        "\n",
        "# Select the number of related columns:\n",
        "n1 =  # Eg. 8\n",
        "n2 =  # Eg. 10\n",
        "ref_col1 = # Eg. 'SaO2'\n",
        "ref_col2 = # Eg. 'SpO2'\n",
        "\n",
        "# Get variables correlated to 'ref_col'\n",
        "columns_1 = get_most_correlated(df[temporal_clinical_variables], variable=ref_col1, n=n1)\n",
        "\n",
        "# Get variables correlated to 'ref_col_2'\n",
        "columns_2 = get_most_correlated(df[temporal_clinical_variables], variable=ref_col2, n=n2)\n",
        "\n",
        "# Merge the resulting columns in a single list:\n",
        "resulting_temporal_clinical_variables = list(set(columns_1+columns_2))\n",
        "\n",
        "print(f'The number of resulting variables is {len(resulting_temporal_clinical_variables)}')"
      ],
      "metadata": {
        "id": "rxZJnP7bIVWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the resulting dataframe\n",
        "variables = list(set(patient_info + demographic_variables + static_clinical_variables + resulting_temporal_clinical_variables + outcomes + treatment))\n",
        "\n",
        "df = df [variables]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NMQJzYj-DPk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Outcome Leakage\n",
        "Outcome leakage occurs when the model is inadvertently given access to data it shouldn't have until the time of prediction. For instance, when predicting hidden hypoxemia, including the SOFA respiratory score (sofa_resp), which includes PaO2 in its definition, in our model could lead to leakage. This is because PaO2 is directly related to the outcome we're trying to predict. It's important to be mindful of potential outcome leakages like this when selecting variables for model training."
      ],
      "metadata": {
        "id": "3ovYuX8y46H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Outcome Leakage（結果の漏洩）\n",
        "Outcome leakage とは、予測時までにモデルがアクセスすべきでないデータに誤ってアクセスする場合を指します。例えば、潜在性低酸素血症を予測する際に、それ自身の定義に PaO2 を含む SOFA 呼吸器スコア（sofa_resp）をモデルに含めてしまうと、漏洩が生じる可能性があります。これは、 PaO2 が我々が予測しようとしている結果と直接関連しているからです。モデルの訓練のために変数を選択する際には、このような潜在的な outcome leakage に注意することが重要です。"
      ],
      "metadata": {
        "id": "8JcYd5MZ5KKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's discuss in the team and eliminate features that could potentially cause outcome leakage!"
      ],
      "metadata": {
        "id": "BMrqdre8_vXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand how variables are distributed:"
      ],
      "metadata": {
        "id": "-MHV8nXH_pZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データの分布を理解する:"
      ],
      "metadata": {
        "id": "oXzM0DSpUm4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see a description of some statistics of each numeric variable, you can use `df.describe()`:\n"
      ],
      "metadata": {
        "id": "36D4dSeJK2zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "各数値変数の統計情報を見るには`describe()`関数を用います。"
      ],
      "metadata": {
        "id": "PJvJy3brU7lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "gyYJzvip-4-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with `data.describe()` is that it groups using all the data of each variable, this means that if in our group there are populations that are underestimated or overestimated, the description will not take that into account. An alternative to solve that problem is using the function `groupby()`. With group by we can generate groups of populations using variables. An exmple is:"
      ],
      "metadata": {
        "id": "q8hv2yMShisk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`data.describe()`関数の問題は、各変数の全データをまとめ評価してしまうことです。そのため過小評価もしくは過大評価されているサブグループがいたとしても、考慮されません。この問題に対処するために、`groupby()`関数を使うことで、サブグループ毎に評価ができます。"
      ],
      "metadata": {
        "id": "m_sTYda7C1_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the mean of oxygen saturation of arterial blood (SaO2) by gender\n",
        "avg_sao2_by_gender = df.groupby('gender')['SaO2'].mean()\n",
        "avg_sao2_by_gender"
      ],
      "metadata": {
        "id": "KmRNIklehdSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also generate groups using more than one variable and more than one grouping methods"
      ],
      "metadata": {
        "id": "cUtoT_Z2yoN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "複数の変数と複数のグループ化メソッドを用いてグループを作成することもできます。"
      ],
      "metadata": {
        "id": "cie41YCPYABp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain count, mean, standard deviation, min value and max value for (SaO2) and (SpO2) grouping by gender and race group:\n",
        "avg_sao2_by_ethnicity = df.groupby(['gender', 'race_group']).agg({\n",
        "    'SaO2': ['count', 'mean', 'std', 'min', 'max'],\n",
        "    'SpO2': ['count', 'mean', 'std', 'min', 'max']\n",
        "})\n",
        "avg_sao2_by_ethnicity"
      ],
      "metadata": {
        "id": "ZmqCwMY6hdVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, there are also libraries that allow us to do this process automatically. Below is an example using tableone library.\n",
        "\n",
        "Install using the command `!pip install tableone`"
      ],
      "metadata": {
        "id": "O-tm_ARoz-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "他にも、同様の処理を自動で行ってくれるライブラリもあります。\n",
        "以下に`tableone`ライブラリを使用した例を示します。\n",
        "\n",
        "インストール方法：`!pip install tableone`"
      ],
      "metadata": {
        "id": "s321miP3YRF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As a first step let's do a preprocessing to some variables like:\n",
        "df['language'] = df['language'].replace({'ENGLISH': 'Proficient', '?': 'Limited Proficiency'})"
      ],
      "metadata": {
        "id": "pPzYMKLP9LJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tableone\n",
        "from tableone import TableOne\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "bw0nGk2YJwJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groupby = ['race_group']\n",
        "\n",
        "categorical_variables = ['mortality_in','gender', 'language', 'invasive_vent']\n",
        "numerical_variables = ['anchor_age', 'los_icu', 'los_hospital', 'CCI', 'SOFA_admission']\n",
        "columns = categorical_variables + numerical_variables\n",
        "\n",
        "labels ={'anchor_age': 'age',\n",
        "         'SOFA_admission': 'SOFA'}\n",
        "\n",
        "\n",
        "mytable = TableOne(df, columns=columns, categorical=categorical_variables, groupby=groupby, nonnormal=numerical_variables, rename=labels, pval=False)\n",
        "mytable"
      ],
      "metadata": {
        "id": "foYkDJqQocnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Use tableone\n",
        "\n",
        "Now let's create a new tableone using all the variables to to measure differences between ethnic groups!\n",
        "\n",
        "Hint: Use the function `get_categorical_numerical_variables` to get all the numeric and categorical variables. Then use those variables to generate a tableone of the whole dataset\n"
      ],
      "metadata": {
        "id": "MWKI47ieLIIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ tableone を使う\n",
        "\n",
        "では、人種間の差を測定するために、全ての変数を用いて新しいtableoneを作成しましょう。\n",
        "\n",
        "ヒント:\n",
        "最初に、`get_categorical_numerical_variables` 関数を使ってすべての数値変数とカテゴリー変数を取得します。次に、それらの変数を用いてデータセット全体のtableoneを作成します。\n"
      ],
      "metadata": {
        "id": "4OB1wKepYy9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def get_categorical_numerical_variables(df: pd.DataFrame, limit: int, ignore: List[str], verbose: bool = True) -> Tuple[List[str], List[str]]:\n",
        "  \"\"\"\n",
        "  数値変数とカテゴリー変数を取得するための関数\n",
        "  引数:\n",
        "    df: 全ての変数を含む pandas のデータフレーム\n",
        "    limit: 変数がカテゴリカルであると判定するための最大の要素数を表す整数\n",
        "    ignore: 無視する列名のリスト\n",
        "\n",
        "  戻り値:\n",
        "    categorical_columns, numerical_columns: カテゴリー変数と数値変数の名前を含むリスト\n",
        "  \"\"\"\n",
        "\n",
        "  categorical_columns = []\n",
        "  numerical_columns = []\n",
        "\n",
        "  for column in df.columns:\n",
        "    if ignore:\n",
        "      if column in ignore:\n",
        "        continue\n",
        "    unique_values = len(pd.unique(df[column]))\n",
        "    if unique_values <= limit:\n",
        "      if verbose:\n",
        "        print(f'The column {column} has {unique_values}, so is categorical')\n",
        "      categorical_columns.append(column)\n",
        "    else:\n",
        "      if verbose:\n",
        "        print(f'The column {column} has {unique_values}, so is numerical')\n",
        "      numerical_columns.append(column)\n",
        "\n",
        "  return categorical_columns, numerical_columns"
      ],
      "metadata": {
        "id": "ouIrWL4AiEM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns for the tableone\n",
        "groupby = #\n",
        "ignore = ['column1', 'column2', ...] + groupby\n",
        "limit = # Max. Number of unique values to be categorical (E.g. 15)\n",
        "\n",
        "categorical_columns, numerical_columns = get_categorical_numerical_variables(df, limit=limit, ignore=ignore)"
      ],
      "metadata": {
        "id": "ilDdtS_tex3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total_columns =\n",
        "Total_columns = categorical_columns + numerical_columns\n",
        "\n",
        "TableOne(df, columns=Total_columns, categorical=categorical_columns, nonnormal=numerical_columns, groupby=groupby)"
      ],
      "metadata": {
        "id": "JXtGKC6jfcct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize your data\n",
        "\n",
        "While statistics are important, it is also very important and key to visualize the data. This will help to share the information and results in a visual and intuitive way, and identify anomalous and data patterns that could be useful when choosing the model."
      ],
      "metadata": {
        "id": "y-8mdEVQLSKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データを可視化する\n",
        "\n",
        "統計情報が大切であると同時にデータの可視化も大切な鍵となってきます。\n",
        "データの可視化は視覚的・直感的に情報と結果を共有するので、モデルを選ぶときに有用な、異常やデータパターンを特定することに役立ちます。"
      ],
      "metadata": {
        "id": "gkdLoH9aAwgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bar charts and histograms\n",
        "Bar charts and histograms can be used to see how our data is distributed and how many instances there are of each class, this way we can identify imbalances in the classes and avoid overfitting in the models\n",
        "\n",
        "To create a histogram with seaborn, you can use the code:\n",
        "\n",
        "\n",
        "```\n",
        "sns.histplot(data=df, x=\"column_name\", kde=False) #optional kwarg: hue=\"column_2\"\n",
        "plt.title('Title')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0L5teBSObmED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 棒グラフとヒストグラム\n",
        "\n",
        "棒グラフやヒストグラムを使うことでデータの分布や各クラスの件数を見ることができます。\n",
        "このようにしてクラスの不均衡を見つけ、過学習を防ぐことができます。\n",
        "\n",
        "seabornでヒストグラムを作成するには以下のコードを使用します。\n",
        "```\n",
        "sns.histplot(data=df, x=\"column_name\", kde=False) #optional kwarg: hue=\"column_2\"\n",
        "plt.title('Title')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hDVbBnSkT6In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Plot the distribution by race group"
      ],
      "metadata": {
        "id": "Op0QYpYxMvp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 人種分布のプロット"
      ],
      "metadata": {
        "id": "9NdYcQ9RXpzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "nIarH7T8bmbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Plot the distribution by gender"
      ],
      "metadata": {
        "id": "WP4GUREQM_0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 性別分布のプロット"
      ],
      "metadata": {
        "id": "n-lruYd8X1of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "21beiKNxcJGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Plot the distribution by age and race group"
      ],
      "metadata": {
        "id": "47abe3cjNCiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 年齢と人種の分布のプロット"
      ],
      "metadata": {
        "id": "VWvLPN3UX_lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "gKDp62Uldi6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Box plots and violin plots\n",
        "**Outilers ❗❗❗**: While in some cases there are no null values, could be outliers values. These outliers are sometimes indicators of biases in data collection, or social/cultural biases. However, in some cases the outliers are erroneous values during data collection and storage. Identifying values such as very high SaO2 (99999) or negative is an indicator in many cases of missing values and should be taken as such.\n",
        "\n",
        "Bar and violin charts are one of the best ways to identify outliers, because they show us around which values most of our data is clustered."
      ],
      "metadata": {
        "id": "oRt_xYq87hLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 箱ひげ図とバイオリンプロット\n",
        "**外れ値 ❗❗❗**: 欠損値はなくても、外れ値がある場合があります。これらの外れ値は、データ収集の偏りや社会的・社会的偏りの指標となることもありますが、時にはデータ収集時、保存時の誤った値である場合もあります。\n",
        "非常に高いSaO2(99999)や負の値などの識別は、多くの場合欠損値の指標であり、そのように扱うべきです。\n",
        "\n",
        "棒グラフやバイオリン・チャートは異常値を識別する最良の方法の一つです。なぜなら、データのがどの値の周りに集まっているかを示してくれるからです。\n"
      ],
      "metadata": {
        "id": "-yJZbTinYhzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_groupby(df: pd.DataFrame, group1: str, group2: str, variable: str, barplot=True, boxplot=True, violinplot=False) -> None:\n",
        "  \"\"\"\n",
        "  この関数は、data frame内の2つのグループの変数の平均値を比較する2つのプロットを作成します。\n",
        "  この関数では、各グループについて棒グラフ、箱ひげ図、バイオリンプロットのいずれかを選択することができます。\n",
        "  最初のプロットは最初のグループの平均を比較し、2番目のプロットは2番目のグループの平均を比較します。\n",
        "\n",
        "  引数 (型名)：\n",
        "    df (pd.DataFrame): 扱うデータセット\n",
        "    group1 (string): グループ化する最初のカテゴリ変数の名前\n",
        "    group2 (string): グループ化する2番目のカテゴリ変数の名前\n",
        "    variable (string): プロットする数値変数の名前\n",
        "    barplot (boolean): 棒グラフをプロットするかどうか(デフォルト値はTrue) *\n",
        "    boxplot (boolean): 箱ひげ図をプロットするかどうか(デフォルト値はTrue) *\n",
        "    violinplot (boolean): バイオリンプロットをプロットするかどうか(デフォルト値はFalse)*\n",
        "    *：省略可能\n",
        "\n",
        "  戻り値：\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  avg_by_group1 = df.groupby(group1)[variable].mean()\n",
        "  avg_by_group2 = df.groupby(group2)[variable].mean()\n",
        "\n",
        "  if barplot:\n",
        "    # Bar plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    print('Bar Plot: ')\n",
        "    ax1.bar(avg_by_group1.index, avg_by_group1.values)\n",
        "    ax1.set_xlabel(group1)\n",
        "    ax1.set_ylabel(f'Average {variable}')\n",
        "    ax2.bar(avg_by_group2.index, avg_by_group2.values)\n",
        "    ax2.set_xlabel(group2)\n",
        "    ax2.set_ylabel(f'Average {variable}')\n",
        "    plt.show()\n",
        "\n",
        "  if boxplot:\n",
        "    # Create box plots to visualize the distribution of SaO2 values by gender and ethnicity\n",
        "    print('Box Plot: ')\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.boxplot(x=group1, y=variable, hue=group2, data=df, ax=ax)\n",
        "    plt.show()\n",
        "\n",
        "  if violinplot:\n",
        "    print('Violin Plot: ')\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.violinplot(x=group1, y=variable, hue=group2, data=df, ax=ax)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IOWP5b6IZSyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Plot the distribution of SaO2 by gender and race group"
      ],
      "metadata": {
        "id": "Mhi4eVrrhzJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 性別・人種別の**SaO2**のプロット"
      ],
      "metadata": {
        "id": "Qe3DDEinj7KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot_groupby()"
      ],
      "metadata": {
        "id": "kd8Y6R_P9qag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Plot the distribution of SpO2 by gender and race group"
      ],
      "metadata": {
        "id": "5VbxyRjYh6UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 性別・人種別の**SpO2**のプロット"
      ],
      "metadata": {
        "id": "TfbJjrjqkF7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot_groupby()\n"
      ],
      "metadata": {
        "id": "mcbHq8-0jIMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Plot the distribution of any Time Offset by gender and race group (Use violin plot)\n",
        "\n",
        "Hint: You can use the next line of code to get all the variables related to a time offset"
      ],
      "metadata": {
        "id": "_IApC0RKfjcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 性別・人種のグループ別にタイムオフセット(計測された時刻の差)の分布のプロット(ヴァイオリンプロットを使う)\n",
        "\n",
        "ヒント: 以下のコードを用いてタイムオフセットに関連する変数を取得することができます。"
      ],
      "metadata": {
        "id": "HQ9l9QROkTBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df.columns:\n",
        "  if 'delta' in column:\n",
        "    print(column)"
      ],
      "metadata": {
        "id": "OSwa5fEdhQT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot_groupby()"
      ],
      "metadata": {
        "id": "QCGZGTHfeuiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing / 前処理\n",
        "---"
      ],
      "metadata": {
        "id": "keW_Fk-gruDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that there are some outliers so let's assume that those values are missing values and then analyze the amount of missing values for preprocessing"
      ],
      "metadata": {
        "id": "OQxBAUV7SCcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前章のプロットによりいくつかの外れ値があることが分かっているので、それらの値は欠損値として仮定して、前処理のために欠損値の数を分析しましょう。\n"
      ],
      "metadata": {
        "id": "XrvY8MdwpLUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing values\n",
        "\n",
        "Let's see which columns have variables with missing data"
      ],
      "metadata": {
        "id": "Th51UTXwSRYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 欠損値を見つける\n",
        "\n",
        "どの列に欠損値があるのかを探します。"
      ],
      "metadata": {
        "id": "13yPhjmvrBmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "cW7ysOZGeunN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ detect columns with null values\n",
        "Use the function `null_values` to see the columns with missing data and the number of rows missing.\n",
        "\n",
        "if the number of values with missing data is very high compared to the size of the dataset (E.g. 85%), in some cases it is better just to remove them"
      ],
      "metadata": {
        "id": "OcfF2HSqOM3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ nullを含む列を特定する\n",
        "`null_values` 関数を用いて欠損値のある列とその行番号を求めます。\n",
        "\n",
        "if the number of values with missing data is very high compared to the size of the dataset (E.g. 85%), in some cases it is better just to remove them\n",
        "\n",
        "もしデータセットの大半(E.g. 85%)が欠損していた場合、その列は省いてしまったほうがいいかもしれません。"
      ],
      "metadata": {
        "id": "0nrgrXKurTpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### List of columns with missing values ###\n",
        "### Code here ###\n",
        "null_values(df)"
      ],
      "metadata": {
        "id": "DNVsJ8FRSJ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split\n",
        "Dividing your dataset into training and test is important, since what we want is for the model to learn, not memorize.\n",
        "We want to test the behavior of the model on unknown data. For that reason, the dataset should always be divided into training and test.\n",
        "\n",
        "The way to divide it may vary... You can use 60% (training) / 40% (testing), or 70% (training) / 30% (testing), or even 50/50. The most important part is that you **make sure that in the test set you have a well-distributed data set that covers all the possibilities.**"
      ],
      "metadata": {
        "id": "LwRUZnXdsMB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練用とテスト用にデータセットを分割する\n",
        "データセットを訓練用とテスト用に分割することは結果に関わる重要な事です。(なぜなら、モデルに求めていることは\"暗記\"ではなく\"学習\"だからです)\n",
        "\n",
        "生成されたモデルを評価するにあたり、未知のデータにおけるモデルの挙動をテストするために、今あるデータセットを訓練用とテスト用に分割します。\n",
        "\n",
        "分割の比率は自由です。訓練用:テスト用を6:4、7:3、5:5など決まりはありません。この時に気を付けて頂きたいことは**テスト用のデータに偏りがなく、全てのパターンを網羅できるようになっていること**です。"
      ],
      "metadata": {
        "id": "T3dPEzpMsglU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "def train_test_split_with_id(df: pd.DataFrame, test_size: float, id_column: str, stratify: array_like=None, random_state: Optional[int]=42) -> Tuple[pd.DataFrame]:\n",
        "  \"\"\"\n",
        "  データセットを訓練用とテスト用に分割し、両方のデータセットに同じIDを持つ行が現れないようにする関数。\n",
        "\n",
        "  引数 (型名):\n",
        "    df (pandas.DataFrame): 分割したいデータセット\n",
        "    test_size (float): テスト用に使用するデータの割合(ex 0.3)\n",
        "    id_column (string): 患者の識別IDを含む列の名前 (訓練用とテスト用に同じ患者のデータを用いることはできません)\n",
        "    stratify (array-like): 層別化(各データの割合を揃えるため)に使う関数。特に指定しない(Noneの)場合、層別化は行われません。(オプション)\n",
        "    random_state (int): 分割に使用する乱数を生成するための整数(オプション).\n",
        "\n",
        "  戻り値 (型名):\n",
        "    tuple (pandas.DataFrames): 訓練用とテスト用の2つのタプル\n",
        "  \"\"\"\n",
        "  if stratify:\n",
        "    id_groups = df.groupby(id_column)\n",
        "    ids = []\n",
        "    ys = []\n",
        "    for _, group in id_groups:\n",
        "        ids.append(group[id_column].iloc[0])\n",
        "        ys.append(group[stratify].iloc[0])\n",
        "    ids = np.array(ids)\n",
        "    ys = np.array(ys)\n",
        "\n",
        "    train_ids, test_ids, train_ys, test_ys = train_test_split(ids, ys, test_size=test_size, stratify=ys, random_state=random_state)\n",
        "\n",
        "  else:\n",
        "    unique_ids = df[id_column].unique()\n",
        "    train_ids, test_ids = train_test_split(unique_ids, test_size=test_size, random_state=random_state)\n",
        "\n",
        "  train = df[df[id_column].isin(train_ids)]\n",
        "  test = df[df[id_column].isin(test_ids)]\n",
        "\n",
        "  print(f'The train shape is {train.shape}')\n",
        "  print(f'The test shape is {test.shape}')\n",
        "\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "BVzkNkfFsOhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Split your data\n",
        "Use the provided function to split your data into train and test. We have to split the dataset now so we can avoid data leakage during data imputation and data normalization"
      ],
      "metadata": {
        "id": "RcbYiAi0PFP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ データを分割する\n",
        "先ほど定義した`train_test_split_with_id`関数を用いてデータセットを分割します。\n",
        "データの分割により、欠損値補完や正規化の際のデータの漏れを防ぐことができます。"
      ],
      "metadata": {
        "id": "cchPehBrzZi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here!\n",
        "test_size =\n",
        "id_column = # Avoid same patient in train and test!!!\n",
        "stratify =\n",
        "\n",
        "#train, test = train_test_split_with_id('...')"
      ],
      "metadata": {
        "id": "L60Ly3JNxAZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Imputation\n",
        "\n",
        "There are various imputation methods of different complexity, ranging from imputing using the most common value (mode) (usually used for categorical variables) or the average or median value depending on the distribution of the data (usually used for numerical variables), to imputation methods using machine learning algorithms to predict the missing value using the other variables as a reference. While any method of data imputation can be valid, you have to take care to avoid data leakage or adding bias when doing this process."
      ],
      "metadata": {
        "id": "TGfy5tf4SndZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have the columns with missing data, let's divide the columns into two sub groups:\n",
        "1. Categorical columns with missing\n",
        "2. Numerical columns with missing"
      ],
      "metadata": {
        "id": "Vsi0qe3cQzmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 欠損値補完\n",
        "\n",
        "欠損値補完の方法には単純なものから複雑なものまで、様々な方法があります。\n",
        "最頻値(通常、カテゴリ変数に使用)を用いる方法や、データの分布に応じた平均値または中央値(通常、数値変数に使用)を用いる方法の他にも、他の変数を参照して値を予測しながら補完する、機械学習を用いた方法などがあります。\n",
        "\n",
        "どのような方法を用いても有用な補完が可能ですが、データの漏れやバイアスの付与を考慮して行う必要があります。"
      ],
      "metadata": {
        "id": "MaHRF6uX1oKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "欠損値のある列を2つのサブグループに分割します:\n",
        "1. カテゴリー変数列の欠損\n",
        "2. 数値変数列の欠損"
      ],
      "metadata": {
        "id": "gTZVVJYd6KeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Get numerical, categorical columns and columns with missing data\n",
        "\n",
        "Hint: You can use the function `get_categorical_numerical_variables` to get the list of categorical and mumerical columns.\n",
        "\n",
        "Hint 2: You can use the function `null_values` as part of the logic to get the columns with missing data"
      ],
      "metadata": {
        "id": "CPJYmaBJQUIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 数値変数を持つ列、カテゴリー変数を持つ列、欠損値を持つ列を取得する\n",
        "\n",
        "ヒント1: `get_categorical_numerical_variables` 関数を用いて数値変数とカテゴリー変数を持つ列のリストが取得できます。\n",
        "\n",
        "ヒント 2: `null_values` 関数は欠損値のある列を取得する方法の一部として使うことができます。"
      ],
      "metadata": {
        "id": "C6tf68HE6prx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numerical and categorical columns:\n",
        "categorical_cols, numerical_cols ="
      ],
      "metadata": {
        "id": "qExWrf1JxPoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the columns with missing data\n",
        "# columns_missing =\n",
        "columns_missing = train.isnull().sum()[train.isnull().sum() > 0].index\n",
        "columns_missing"
      ],
      "metadata": {
        "id": "0RLwYEoKynP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_data_imputation(df: pd.DataFrame, impute_cols: List[str], test_df: Optional[pd.DataFrame]=None, groupby_cols: List[str]=[], method: str='new_category_numeric', imputation_model=LinearRegression()):\n",
        "  \"\"\"\n",
        "  必要に応じて与えらえた列(impute_cols)に groupby を適用した後に、列毎の欠損値補完(Data Imputation)を行う.\n",
        "  !!! 注意事項 !!!\n",
        "  カテゴリー変数に対してnew_category関数を、数値変数に対してnew_category_numeric関数以外を適用する場合以外は、必ずデータの分割後に行うこと。そうしないとデータ漏れを招く恐れがあります。\n",
        "\n",
        "  引数(型名):\n",
        "    df(pd.DataFrame): 欠損値補完を行うデータ\n",
        "    impute_cols(list): 欠損値補完を適用する列名のリスト\n",
        "    test_df(pd.DataFrame): 訓練用データを参照して欠損値補完を行うテスト用データ(オプション)\n",
        "    groupby_cols(list): method='median', 'mean', 'mode'の場合はデータをグループ化するためのカラム名、 method='model'の場合はモデルの特徴を表すカラム名のリスト (E.g.: groupby_cols=['race_group', 'gender']).\n",
        "    method(string): 欠損値補完の方法。 'median'、'mean'、'mode'、'new_category'、'new_category_numeric'、'model'の6つが選択可能\n",
        "    model: methodが'model'の場合に欠損値の予測に用いるsklearnモデル\n",
        "\n",
        "  戻り値:\n",
        "    欠損値のある列のデータを補完後のデータ\n",
        "  \"\"\"\n",
        "  imputer = {}\n",
        "\n",
        "  for col in impute_cols:\n",
        "\n",
        "    # Group the data and calculate the method (E.g. Median) for each group\n",
        "    if method == 'median':\n",
        "      # Impute using median (for numerical values)\n",
        "      imputation_values = df.groupby(groupby_cols)[col].median()\n",
        "\n",
        "    elif method == 'mean':\n",
        "      # Impute using mean (for numerical values)\n",
        "      imputation_values = df.groupby(groupby_cols)[col].mean()\n",
        "\n",
        "    elif method == 'mode':\n",
        "      # Impute using mode (for categorical values)\n",
        "      imputation_values = df.groupby(groupby_cols)[col].agg(pd.Series.mode)\n",
        "\n",
        "    elif method == 'new_category':\n",
        "      # Add a new category 'None' in the dataset (for categorical values)\n",
        "      new_category_val = 'None'\n",
        "      imputer[col] = new_category_val\n",
        "      df[col] = df[col].fillna(new_category_val)\n",
        "      if test_df is not None:\n",
        "        test_df[col] = test_df[col].fillna(new_category_val)\n",
        "\n",
        "\n",
        "    elif method == 'new_category_numeric':\n",
        "      # Add an anomalous data in the dataset (for numerical values)\n",
        "      new_category_val = 0\n",
        "      # If there's not 0, replace nan with 0\n",
        "      if not((df[col] == 0).any()):\n",
        "        new_category_val = 0\n",
        "      # If there's not positive numbers, replace nan with 1\n",
        "      elif not((df[col] >= 0).any()):\n",
        "        new_category_val = 1\n",
        "      # If there's not negative numbers, replace nan with 1\n",
        "      elif not((df[col] <= 0).any()):\n",
        "        new_category_val = -1\n",
        "      else:\n",
        "        # Replace with min value - 100\n",
        "        new_category_val = df[col].min() - 100\n",
        "\n",
        "      imputer[col] = new_category_val\n",
        "      df[col] = df[col].fillna(new_category_val)\n",
        "      if test_df is not None:\n",
        "        test_df[col] = test_df[col].fillna(new_category_val)\n",
        "\n",
        "\n",
        "    elif method == 'model':\n",
        "      # If the variable is categorical, convert to numeric:\n",
        "      if df[col].dtype == 'object':\n",
        "        encoder = LabelEncoder()\n",
        "        df[col] = encoder.fit_transform(df[col].dropna())\n",
        "\n",
        "      # Create a linear regression model to impute missing values\n",
        "      model = imputation_model\n",
        "\n",
        "      # Get the data with complete column\n",
        "      complete_data = df.dropna(subset=[col])\n",
        "\n",
        "      # Encode categorical columns if needed in x data\n",
        "      le = {}\n",
        "      for col_group in groupby_cols:\n",
        "        if df[col_group].dtype == 'object':\n",
        "          le[col_group] = LabelEncoder()\n",
        "          complete_data[col_group] = le[col_group].fit_transform(complete_data[col_group])\n",
        "\n",
        "\n",
        "      # Fit the model on the complete data\n",
        "      X = complete_data[groupby_cols]\n",
        "      # Replace any remaining NaNs with the column mean\n",
        "      X = X.fillna(X.mean())\n",
        "      y = complete_data[col]\n",
        "      model.fit(X, y)\n",
        "\n",
        "      if df[col].dtype == 'object':\n",
        "        imputer[col] = (model, encoder, le)\n",
        "      else:\n",
        "        imputer[col] = (model, le)\n",
        "\n",
        "      ### Impute column using the model to predict the value:\n",
        "      missing_data = df[df[col].isna()].copy()\n",
        "      if test_df is not None:\n",
        "        missing_data_test = test_df[test_df[col].isna()].copy()\n",
        "\n",
        "      # Encode categorical columns if needed in x data\n",
        "      for col_group in groupby_cols:\n",
        "        if missing_data[col_group].dtype == 'object':\n",
        "          missing_data[col_group] = le[col_group].transform(missing_data[col_group].dropna())\n",
        "          if test_df is not None:\n",
        "            missing_data_test[col_group] = le[col_group].transform(missing_data_test[col_group].dropna())\n",
        "\n",
        "      for index, row in missing_data.iterrows():\n",
        "        values = row[groupby_cols].values.reshape(1, -1)\n",
        "        imputed_value = model.predict(values)\n",
        "        df.at[index, col] = imputed_value[0]\n",
        "\n",
        "      if test_df is not None:\n",
        "        for index, row in missing_data_test.iterrows():\n",
        "          values = row[groupby_cols].values.reshape(1, -1)\n",
        "          imputed_value = model.predict(values)\n",
        "          test_df.at[index, col] = imputed_value[0]\n",
        "\n",
        "      if method in ['median', 'mean', 'mode']:\n",
        "        imputer[col] = imputation_values\n",
        "        # Fill missing values with the method of the corresponding group\n",
        "        df[col] = df.apply(lambda x: imputation_values[tuple(x[groupby_cols])] if pd.isna(x[col]) else x[col], axis=1)\n",
        "        if test_df is not None:\n",
        "          test_df[col] = test_df.apply(lambda x: imputation_values[tuple(x[groupby_cols])] if pd.isna(x[col]) else x[col], axis=1)\n",
        "\n",
        "  if test_df is not None:\n",
        "    return df, test_df, imputer\n",
        "  else:\n",
        "    return df, imputer\n"
      ],
      "metadata": {
        "id": "dqZXomYXPa3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Apply data imputation to categorical variables\n",
        "\n",
        "1. Get from the categorical variables the list of categorical columns with missing data\n",
        "2. Use the function `apply_data_imputation` to impute the variables of train and test data. You can also use other methods of pandas or sklearn. But think about the possible bias that those methods could be adding to the dataset."
      ],
      "metadata": {
        "id": "nTFDHd8RKbqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ カテゴリー変数に欠損値補完を適用する\n",
        "\n",
        "1. 欠損値のあるカテゴリー変数のリストから、欠損値補完を行う列を取得する。\n",
        "2. `apply_data_imputation`関数を使用して訓練用とテスト用のデータの欠損値補完を行う。pandasやsklearnの他の方法を使っても問題ありませんが、それを適用することによって生じる可能性のあるデータの偏りについてよく考えてから適用しましょう。"
      ],
      "metadata": {
        "id": "KlMiqfK8eRMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get from columns with missing data the categorical columns\n",
        "categorical_cols_missing ="
      ],
      "metadata": {
        "id": "NCNfYwpNKXRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important❗❗❗** If you apply other technique than `new_category` on categorical variables. You should do it after train-test split otherwise you could be introducing a data leakage in the test set"
      ],
      "metadata": {
        "id": "90vB5xhDzJvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**注意事項❗❗❗** もし`new_category`関数以外の方法で欠損値補完を行う場合、必ず**訓練用データとテスト用データとの分割後に**行うこと。こうしないとデータ漏れを引き起こす可能性があります。"
      ],
      "metadata": {
        "id": "Ax_jWtgAg95P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method =  # 'mode', 'new_category'\n",
        "groupby_cols =  # E.g. ['race_group', 'gender'] or None if is new_category\n",
        "\n",
        "train, test, imputer = apply_data_imputation(train, impute_cols=categorical_cols_missing, method=method, groupby_cols=groupby_cols, test_df=test)"
      ],
      "metadata": {
        "id": "KSws7IRAScbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Apply data imputation to Numerical variables\n",
        "\n",
        "1. Get from the numerical variables the list of numerical columns with missing data\n",
        "2. Use the function `apply_data_imputation` to impute the variables of train and test data. You can also use other methods of pandas or sklearn. But think about the possible bias that those methods could be adding to the dataset."
      ],
      "metadata": {
        "id": "OHwYtppJLNfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Apply data imputation to Numerical variables\n",
        "\n",
        "1. 欠損値のある数値変数のリストから、欠損値補完を行う列を取得する。\n",
        "\n",
        "2. `apply_data_imputation`関数を使用して訓練用とテスト用のデータの欠損値補完を行う。pandasやsklearnの他の方法を使っても問題ありませんが、それを適用することによって生じる可能性のあるデータの偏りについてよく考えてから適用しましょう。"
      ],
      "metadata": {
        "id": "Y0C-1C9AjhLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get from columns with missing data the numerical columns\n",
        "numerical_cols_missing =\n"
      ],
      "metadata": {
        "id": "7TavY8buLUIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important ❗❗❗** If you apply other technique than `new_category_numeric` on numerical variables. You should do it after train-test split otherwise you could be introducing a data leakage in the test set"
      ],
      "metadata": {
        "id": "ZQHc5wKdzsb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**注意事項❗❗❗** もし`new_category_numeric`関数以外の方法で以外の方法で訓練用データとテスト用データとの分割後に行うこと。こうしないとデータ漏れを引き起こす可能性があります。"
      ],
      "metadata": {
        "id": "eRk6ef7koi5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method = # 'median', 'mean', 'new_category_numeric', 'model'\n",
        "groupby_cols =  # E.g. ['race_group', 'gender'] or None if is new_category_numeric\n",
        "\n",
        "\n",
        "train, test, imputer = apply_data_imputation(train, impute_cols=numerical_cols_missing, method=method, groupby_cols=groupby_cols, test_df=test)"
      ],
      "metadata": {
        "id": "tuTNR8qtLV48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Let's check null values again\n",
        "\n",
        "Print the coluns with missing values (if any) in train and test set.\n",
        "\n",
        "There should be no columns with missing data, if so check the previous tasks"
      ],
      "metadata": {
        "id": "Xtkuor2lfY-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ null値がないかの再確認\n",
        "\n",
        "欠損値補完を行った列(あれば)を表示して確認します。\n",
        "\n",
        "この時点で欠損値のある列はないはずです。もし欠損がある場合はここまでの処理を確認しましょう。\n"
      ],
      "metadata": {
        "id": "O8EL_b69osZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### List of columns with mussing values ###\n",
        "print('#'*20, ' Null train: ', '#'*20)\n",
        "null_values(train)\n",
        "### List of columns with mussing values ###\n",
        "print('#'*20, ' Null test: ', '#'*20)\n",
        "null_values(test)"
      ],
      "metadata": {
        "id": "J1C2rHdzDVIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the file!\n",
        "Finally let's save the dataset as a csv file! 😀"
      ],
      "metadata": {
        "id": "hKqDpYhNpLcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ファイルの保存\n",
        "最後に、データセットをcsvファイルに保存しましょう！ 😀"
      ],
      "metadata": {
        "id": "m3Zn1DxBpiaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ Save your train and test dataset as a csv file"
      ],
      "metadata": {
        "id": "GzWHbaIbXx6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ✏️ 訓練用データとテスト用データをcsvファイルとして保存しましょう"
      ],
      "metadata": {
        "id": "sgZtNmUppyjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save_train = \"path/train.csv\"\n",
        "path_to_save_test = \"path/test.csv\"\n"
      ],
      "metadata": {
        "id": "BD4fUTkT5dUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to create the directory if it doesn't exist\n",
        "def create_directory_if_not_exists(path):\n",
        "    directory = os.path.dirname(path)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Save function to be used for both train and test datasets\n",
        "def save_dataset(dataset, path_to_save):\n",
        "    create_directory_if_not_exists(path_to_save)\n",
        "    dataset.to_csv(path_to_save, index=False)"
      ],
      "metadata": {
        "id": "y8ElZGp-7sDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export files:\n",
        "save_dataset(train, path_to_save_train)\n",
        "save_dataset(test, path_to_save_test)"
      ],
      "metadata": {
        "id": "zXD403Pto7WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Acknowledgement\n",
        "Original Notebook by David Restrepo, Adrien Carrel, and Jack Gallifant<br>\n",
        "Translated into Japanese by 高砂茉莉花（Marika Takasago）"
      ],
      "metadata": {
        "id": "JKKAqBMzwAIC"
      }
    }
  ]
}